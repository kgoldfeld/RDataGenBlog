---
comments: false
---



<div id="ordinal-categorical-data" class="section level2">
<h2>Ordinal Categorical Data</h2>
<p>Using the <code>defData</code> and <code>genData</code> functions, it is is relatively easy to specify multinomial distributions that characterize categorical data. Order becomes relevant when the categories take on meanings related strength of opinion or agreement (as in a Likert-type response) or frequency. A motivating example could be when a response variable takes on five possible values: (1) strongly disagree, (2) disagree, (3) neutral, (4) agree, (5) strongly agree. There is a natural order to the response possibilities.</p>
<p>It is common to summarize the data by looking at <em>cumulative</em> probabilities, odds, or log-odds. Comparisons of different exposures or individual characteristics typically look at how these cumulative measures vary across the different exposures or characteristics. So, if we were interested in cumulative odds, we would compare
<span class="math display">\[\small{\frac{P(response = 1|exposed)}{P(response &gt; 1|exposed)} \ \ vs. \ \frac{P(response = 1|unexposed)}{P(response &gt; 1|unexposed)}},\]</span></p>
<p><span class="math display">\[\small{\frac{P(response \le 2|exposed)}{P(response &gt; 2|exposed)} \ \ vs. \ \frac{P(response \le 2|unexposed)}{P(response &gt; 2|unexposed)}},\]</span></p>
<p>and continue until the last (in this case, fourth) comparison</p>
<p><span class="math display">\[\small{\frac{P(response \le 4|exposed)}{P(response &gt; 4|exposed)} \ \ vs. \ \frac{P(response \le 4|unexposed)}{P(response &gt; 4|unexposed)}},\]</span></p>
<p>We can use an underlying (continuous) latent process as the basis for data generation. If we assume that probabilities are determined by segments of a logistic distribution (see below), we can define the ordinal mechanism using thresholds along the support of the distribution. If there are <span class="math inline">\(k\)</span> possible responses (in the meat example, we have 5), then there will be <span class="math inline">\(k-1\)</span> thresholds. The area under the logistic density curve of each of the regions defined by those thresholds (there will be <span class="math inline">\(k\)</span> distinct regions) represents the probability of each possible response tied to that region.</p>
<p><img src="/page/ordinal_files/figure-html/threshold-1.png" width="504" /></p>
<div id="comparing-response-distributions-of-different-populations" class="section level3">
<h3>Comparing response distributions of different populations</h3>
<p>In the cumulative logit model, the underlying assumption is that the odds ratio of one population relative to another is constant across all the possible responses. This means that all of the cumulative odds ratios are equal:</p>
<p><span class="math display">\[\small{\frac{codds(P(Resp = 1 | exposed))}{codds(P(Resp = 1 | unexposed))} = \frac{codds(P(Resp \leq 2 | exposed))}{codds(P(Resp \leq 2 | unexposed))} = \ ... \ = \frac{codds(P(Resp \leq 4 | exposed))}{codds(P(Resp \leq 4 | unexposed))}}\]</span></p>
<p>In terms of the underlying process, this means that each of the thresholds shifts the same amount, as shown below, where we add 1.1 units to each threshold that was set for the exposed group. What this effectively does is create a greater probability of a lower outcome for the unexposed group.</p>
<p><img src="/page/ordinal_files/figure-html/plotB-1.png" width="504" /></p>
</div>
<div id="the-cumulative-proportional-odds-model" class="section level3">
<h3>The cumulative proportional odds model</h3>
<p>In the <code>R</code> package <code>ordinal</code>, the model is fit using function <code>clm</code>. The model that is being estimated has the form</p>
<p><span class="math display">\[log \left( \frac{P(Resp \leq  i)}{P(Resp &gt; i)} | Group \right) = \alpha_i - \beta*I(Group=exposed) \  \ , \ i \in \{1, 2, 3, 4\}\]</span></p>
<p>The model specifies that the cumulative log-odds for a particular category is a function of two parameters, <span class="math inline">\(\alpha_i\)</span> and <span class="math inline">\(\beta\)</span>. (Note that in this parameterization and the model fit, <span class="math inline">\(-\beta\)</span> is used.) <span class="math inline">\(\alpha_i\)</span> represents the cumulative log odds of being in category <span class="math inline">\(i\)</span> or lower for those in the reference exposure group, which in our example is Group A. <em><span class="math inline">\(\alpha_i\)</span> also represents the threshold of the latent continuous (logistic) data generating process.</em> <span class="math inline">\(\beta\)</span> is the cumulative log-odds ratio for the category <span class="math inline">\(i\)</span> comparing the unexposed to reference group, which is the exposed. <em><span class="math inline">\(\beta\)</span> also represents the shift of the threshold on the latent continuous process for the exposed relative to the unexposed</em>. The proportionality assumption implies that the shift of the threshold for each of the categories is identical.</p>
</div>
<div id="simulation" class="section level3">
<h3>Simulation</h3>
<p>To generate ordered categorical data using <code>simstudy</code>, there is a function <code>genOrdCat</code>.</p>
<pre class="r"><code>baseprobs &lt;- c(0.11, 0.33, 0.36, 0.17, 0.03)

defA &lt;- defDataAdd(varname = &quot;z&quot;, formula = &quot;-1.1*exposed&quot;, dist = &quot;nonrandom&quot;)

set.seed(130)

dT &lt;- genData(25000)
dT &lt;- trtAssign(dT, grpName = &quot;exposed&quot;)
dT &lt;- addColumns(defA, dT)

dT &lt;- genOrdCat(dT, adjVar = &quot;z&quot;, baseprobs, catVar = &quot;r&quot;)</code></pre>
<p>Estimating the parameters of the model using function <code>clm</code>, we can recover the original parameters quite well.</p>
<pre class="r"><code>library(ordinal)
clmFit &lt;- clm(r ~ exposed, data = dT)
summary(clmFit)</code></pre>
<pre><code>## formula: r ~ exposed
## data:    dT
## 
##  link  threshold nobs  logLik    AIC      niter max.grad cond.H 
##  logit flexible  25000 -33309.96 66629.91 6(0)  1.97e-10 2.3e+01
## 
## Coefficients:
##         Estimate Std. Error z value Pr(&gt;|z|)    
## exposed   -1.119      0.024   -46.5   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Threshold coefficients:
##     Estimate Std. Error z value
## 1|2  -2.1066     0.0222   -94.8
## 2|3  -0.2554     0.0173   -14.7
## 3|4   1.3448     0.0203    66.3
## 4|5   3.4619     0.0456    75.9</code></pre>
<p>In the model output, the <code>exposed</code> coefficient of -1.15 is the estimate of <span class="math inline">\(-\beta\)</span> (i.e. <span class="math inline">\(\hat{\beta} = 1.15\)</span>), which was set to -1.1 in the simulation. The threshold coefficients are the estimates of the <span class="math inline">\(\alpha_i\)</span>’s in the model - and match the thresholds for the unexposed group.</p>
<p>The log of the cumulative odds for groups 1 to 4 from the data without exposure are</p>
<pre class="r"><code>(logOdds.unexp &lt;- log(odds(cumsum(dT[exposed == 0, prop.table(table(r))])))[1:4])</code></pre>
<pre><code>##     1     2     3     4 
## -2.10 -0.25  1.34  3.46</code></pre>
<p>And under exposure:</p>
<pre class="r"><code>(logOdds.expos &lt;- log(odds(cumsum(dT[exposed == 1, prop.table(table(r))])))[1:4])</code></pre>
<pre><code>##     1     2     3     4 
## -0.99  0.86  2.49  4.60</code></pre>
<p>The log of the cumulative odds ratios for each of the four groups is</p>
<pre class="r"><code>logOdds.expos - logOdds.unexp</code></pre>
<pre><code>##   1   2   3   4 
## 1.1 1.1 1.2 1.1</code></pre>
</div>
<div id="correlated-multivariate-ordinal-data-available-on-github-version-only" class="section level3">
<h3>Correlated multivariate ordinal data (available on github version only)</h3>
<p>Function <code>genCorOrdCat</code> generates multiple categorical response variables that may be correlated. For example, a survey of multiple Likert-type questions could have many response variables. The function generates correlated latent variables (using a normal copula) to simulate correlated categorical outcomes. The user specifies a matrix of probabilities, with each row representing a single item or categorical variable. The across each row must be 1. Adjustment variables can be specified for each item, or a single adjustment variable can be specified for all items. The correlation is on the standard normal scale, and is specified with a value of <code>rho</code> and a correlation structure (<em>independence</em>, <em>compound symmetry</em>, or <em>AR-1</em>). Alternatively, a correlation matrix can be specified.</p>
<p>In this example, there are 5 questions, each of which has three possible responses: “none”, “some”, “a lot”. The probabilities of response are specified in a <span class="math inline">\(5 \times 3\)</span> matrix, and the rows sum to 1:</p>
<pre class="r"><code>baseprobs &lt;- matrix(c(0.2, 0.1, 0.7,
                      0.7, 0.2, 0.1,
                      0.5, 0.2, 0.3,
                      0.4, 0.2, 0.4,
                      0.6, 0.2, 0.2), 
                    nrow = 5, byrow = TRUE)

# generate the data

set.seed(333)                     
dT &lt;- genData(10000)

dX &lt;- genCorOrdCat(dT, adjVar = NULL, baseprobs = baseprobs, 
                   prefix = &quot;q&quot;, rho = 0.15, corstr = &quot;cs&quot;)</code></pre>
<p>The observed correlation of the items is slightly less than the specified correlations as expected:</p>
<pre class="r"><code>round(dX[, cor(cbind(q1, q2, q3, q4, q5))], 2)</code></pre>
<pre><code>##      q1   q2   q3   q4   q5
## q1 1.00 0.08 0.10 0.10 0.08
## q2 0.08 1.00 0.09 0.09 0.09
## q3 0.10 0.09 1.00 0.11 0.11
## q4 0.10 0.09 0.11 1.00 0.10
## q5 0.08 0.09 0.11 0.10 1.00</code></pre>
<p>However, the marginal probability distributions of each item match quite closely with the specified probabilities:</p>
<pre class="r"><code>dM &lt;- melt(dX, id.vars = &quot;id&quot;)
dProp &lt;- dM[ , prop.table(table(value)), by = variable]
dProp[, response := rep(seq(3), 5)]

# observed probabilities
dcast(dProp, variable ~ response, value.var = &quot;V1&quot;, fill = 0)</code></pre>
<pre><code>##    variable    1    2    3
## 1:       q1 0.20 0.10 0.70
## 2:       q2 0.69 0.21 0.10
## 3:       q3 0.50 0.20 0.30
## 4:       q4 0.40 0.20 0.40
## 5:       q5 0.60 0.20 0.21</code></pre>
<pre class="r"><code># specified probabilites
baseprobs</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,]  0.2  0.1  0.7
## [2,]  0.7  0.2  0.1
## [3,]  0.5  0.2  0.3
## [4,]  0.4  0.2  0.4
## [5,]  0.6  0.2  0.2</code></pre>
<p>In the next example, the structure of the correlation is changed to AR-1, so the correlation between questions closer to each other is higher than for questions farther apart. But the probability distributions are unaffected:</p>
<pre class="r"><code>dX &lt;- genCorOrdCat(dT, adjVar = NULL, baseprobs = baseprobs, 
                   prefix = &quot;q&quot;, rho = 0.40, corstr = &quot;ar1&quot;)

# correlation
round(dX[, cor(cbind(q1, q2, q3, q4, q5))], 2)</code></pre>
<pre><code>##      q1   q2   q3   q4   q5
## q1 1.00 0.22 0.10 0.05 0.02
## q2 0.22 1.00 0.29 0.10 0.03
## q3 0.10 0.29 1.00 0.31 0.11
## q4 0.05 0.10 0.31 1.00 0.29
## q5 0.02 0.03 0.11 0.29 1.00</code></pre>
<pre class="r"><code>dM &lt;- melt(dX, id.vars = &quot;id&quot;)
dProp &lt;- dM[ , prop.table(table(value)), by = variable]
dProp[, response := rep(seq(3), 5)]

# probabilities
dcast(dProp, variable ~ response, value.var = &quot;V1&quot;, fill = 0)</code></pre>
<pre><code>##    variable    1    2     3
## 1:       q1 0.20 0.10 0.692
## 2:       q2 0.70 0.20 0.099
## 3:       q3 0.50 0.20 0.300
## 4:       q4 0.39 0.21 0.397
## 5:       q5 0.60 0.19 0.204</code></pre>
</div>
</div>
