---
title: Estimating a risk difference (and confidence intervals) using logistic regression
author: ''
date: '2021-06-15'
slug: []
categories: []
tags:
  - R
type: ''
subtitle: ''
image: ''
output:
  blogdown::html_page:
    anchor_sections: no
---

<script src="{{< blogdown/postref >}}index.en_files/header-attrs/header-attrs.js"></script>


<p>The <em>odds ratio</em> (OR) – the effect size parameter estimated in logistic regression – is notoriously difficult to interpret. It is a ratio of two quantities (odds, under different conditions) that are themselves ratios of probabilities. I think it is pretty clear that a very large or small OR implies a strong treatment effect, but translating that effect into a clinical context can be challenging, particularly since ORs cannot be mapped to unique probabilities.</p>
<p>One alternative measure of effect is the <em>risk difference</em>, which is certainly much more intuitive. Although a difference is very easy to calculate when measured non-parametrically (you just calculate the proportion for each arm and take the difference), things get a little less obvious when there are covariates that need adjusting. (There is a method developed by <a href="https://amstat.tandfonline.com/doi/full/10.1080/01621459.2016.1192546?casa_token=EspaMRhG3OIAAAAA%3AHCGnpIqZnUoAroQuWUCwKv5ANjH5mapba9vCUMrY-pkEmOMVmUuKZjDL-pZu2gC_9eKirj8j7CBk" target="_blank">Richardson, Robins, &amp; Wang</a>, that allow analysts to model the risk difference, but I won’t get into that here.)</p>
<p>Currently, I’m working on an <a href="https://impactcollaboratory.org/" target="_blank">NIA IMPACT Collaboratory</a> study evaluating an intervention designed to increase COVID-19 vaccination rates for staff and long-term residents in nursing facilities. A collaborator suggested we report the difference in vaccination rates rather than the odds ratio, arguing in favor of the more intuitive measure. From my perspective, the only possible downside in using a risk difference instead of an OR is that risk difference estimates are <em>marginal</em>, whereas odds ratios are <em>conditional</em>. (I’ve written about this distinction <a href="https://www.rdatagen.net/post/marginal-v-conditional/" target="_blank">before</a>.) The marginal risk difference estimate is a function of the distribution of patient characteristics in the study that influence the outcome, so the reported estimate might not be generalizable to other populations. The odds ratio, on the other hand, is not dependent on the covariates. The ultimate consensus on our research team is that the benefits of improved communication outweigh the potential loss of generalizability.</p>
<p>My goal here is to demonstrate the relative simplicity of estimating the marginal risk difference described in these papers by <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/j.1475-6773.2008.00900.x" target="_blank">Kleinman &amp; Norton</a> and <a href="https://www.sciencedirect.com/science/article/pii/S0895435608003168" target="_blank">Peter Austin</a>. I won’t be using real data from the study that motivated this, but will generate simulated data so that I can illustrate the contrast between marginal and conditional estimates.</p>
<div id="quickly-defining-the-parameters-of-interest" class="section level3">
<h3>Quickly defining the parameters of interest</h3>
<p>In the study that motivated this, we had two study arms - an intervention arm which involved extensive outreach and vaccination promotion and the other a control arm where nothing special was done. So, there are two probabilities that we are interested in: <span class="math inline">\(p_1 \equiv P(\text{vaccinated} | \text{intervention})\)</span> and <span class="math inline">\(p_0 \equiv P(\text{vaccinated} | \text{control}).\)</span></p>
<p>The risk difference comparing the two groups is simply</p>
<p><span class="math display">\[\text{RD} = p_1 - p_0,\]</span>
the odds <span class="math inline">\(w_a\)</span> for each treatment group is</p>
<p><span class="math display">\[w_a = \frac{p_a}{1-p_a}, \ \ a \in \{0,1\},\]</span>
and the odds ratio comparing the intervention arm to the control arm is</p>
<p><span class="math display">\[\text{OR} = \frac{w_1}{w_0}.\]</span></p>
<p>The logistic regression model models the log odds as a linear function of the intervention status and any other covariates that are being adjusted. In the examples below, there is one continuous covariate <span class="math inline">\(x\)</span> that ranges from -0.5 to 0.5:</p>
<p><span class="math display">\[\text{log}(w_A) = \alpha + \beta A + \gamma X.\]</span>
<span class="math inline">\(\beta\)</span> represents the log(OR) conditional on a particular value of <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[\text{log}(w_1) = \alpha + \beta + \gamma X \\
\text{log}(w_0) = \alpha + \gamma X,
\]</span>
and</p>
<p><span class="math display">\[\text{log(OR)} = \text{log}\left(\frac{w_1}{w_0}\right) =\text{log}(w_1) - \text{log}(w_0) = \beta\]</span></p>
<p>More importantly, we can move between odds and probability relatively easily:</p>
<span class="math display">\[\begin{aligned}
\frac{p_a}{1-p_a} &amp;= w_a \\
p_a &amp;= w_a(1- p_a) \\
p_a + w_ap_a &amp;= w_a \\
p_a &amp;= \frac{w_a}{1 + w_a} \\
p_a &amp;= \frac{1}{1 + w_a^{-1}}
\end{aligned}\]</span>
</div>
<div id="estimating-the-marginal-probability-using-model-estimates" class="section level3">
<h3>Estimating the marginal probability using model estimates</h3>
<p>After fitting the model, we have estimates <span class="math inline">\(\hat{\alpha}\)</span>, <span class="math inline">\(\hat{\beta}\)</span>, and <span class="math inline">\(\hat{\gamma}\)</span>. We can generate a pair of odds for each individual <span class="math inline">\(i\)</span> (<span class="math inline">\(w_{i1}\)</span> and <span class="math inline">\(w_{i0}\)</span>) using their observed <span class="math inline">\(x_i\)</span> and the estimated parameters. All we need to do is set <span class="math inline">\(a=1\)</span> and <span class="math inline">\(a=0\)</span> to generate a predicted <span class="math inline">\(\hat{w}_{i1}\)</span> and <span class="math inline">\(\hat{w}_{i0}\)</span>, respectively, for each individual. Note we do not pay attention to the actual treatment arm that the individual was randomized to:</p>
<p><span class="math display">\[ \text{log}(\hat{w}_{i1}) = \hat{\alpha} + \hat{\beta} + \hat{\gamma}x_i, \]</span></p>
<p>or</p>
<p><span class="math display">\[ \hat{w}_{i1} = \text{exp}(\hat{\alpha} + \hat{\beta} + \hat{\gamma}x_i). \]</span></p>
<p>Likewise,</p>
<p><span class="math display">\[ \hat{w}_{i0} = \text{exp}(\hat{\alpha} + \hat{\gamma}x_i). \]</span>
We get <span class="math inline">\(\hat{p}_{ia}\)</span> for <span class="math inline">\(a \in \{0,1\}\)</span> as</p>
<p><span class="math display">\[ \hat{p}_{ia} = \frac{1}{1 + \hat{w}_{ia}^{-1}}\]</span></p>
<p>Finally, the marginal risk difference <span class="math inline">\(\widehat{\text{RD}}\)</span> can be estimated as</p>
<p><span class="math display">\[ \widehat{\text{RD}} = \frac{1}{n}\sum_{i=1}^n \hat{p}_{i1} - \frac{1}{n}\sum_{i=1}^n \hat{p}_{i0} \]</span></p>
<p>from all <span class="math inline">\(n\)</span> study participants regardless of actual treatment assignment.</p>
<p>Fortunately, in <code>R</code> we don’t need to do any of these calculations as predictions on the probability scale can be extracted from the model fit. Standard errors of this risk difference can be estimated using bootstrap methods.</p>
</div>
<div id="simulated-data-set" class="section level3">
<h3>Simulated data set</h3>
<p>Before getting into the simulations, here are the packages needed to run the code shown here:</p>
<pre class="r"><code>set.seed(287362)

library(simstudy)
library(data.table)
library(ggplot2)
library(ggthemes)
library(parallel)</code></pre>
<p>I am generating a binary outcome <span class="math inline">\(y\)</span> that is a function of a continuous covariate <span class="math inline">\(x\)</span> that ranges from -0.5 to 0.5. I use the <em>beta</em> distribution to generate <span class="math inline">\(x1\)</span> which is transformed into <span class="math inline">\(x\)</span>. The advantage of this distribution is the flexibility we have in defining the shape. The OR used to generate the outcome is 2.5:</p>
<pre class="r"><code>def &lt;- defDataAdd(varname = &quot;x1&quot;, formula = &quot;..mu_x&quot;, variance = 8, dist = &quot;beta&quot;)
def &lt;- defDataAdd(def, varname = &quot;x&quot;, formula = &quot;x1 - 0.5&quot;, dist = &quot;nonrandom&quot;)
def &lt;- defDataAdd(def, varname = &quot;y&quot;, 
  formula = &quot;-2 + log(2.5) * rx + 1.5 * x&quot;,
  dist = &quot;binary&quot;, link=&quot;logit&quot;)</code></pre>
<p>In the first scenario of 500 observations, the distribution of <span class="math inline">\(x\)</span> will be right-skewed. This is established by setting the mean of <span class="math inline">\(x1\)</span> close to 0:</p>
<pre class="r"><code>mu_x = 0.2
  
dd_2 &lt;- genData(500)
dd_2 &lt;- trtAssign(dd_2, grpName = &quot;rx&quot;)
dd_2 &lt;- addColumns(def, dd_2)</code></pre>
<pre class="r"><code>ggplot(data = dd_2, aes(x = x)) +
  geom_histogram(fill=&quot;#9ec785&quot;, binwidth = 0.05, boundary = 0) +
  scale_x_continuous(limits = c(-.55, .55), breaks = seq(-.5, .5, by = .25)) +
  theme(panel.grid = element_blank())</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-5-1.png" width="576" /></p>
<p>The first step in estimating the risk difference is to fit a logistic regression model:</p>
<pre class="r"><code>glmfit &lt;- glm(y ~ rx + x, data = dd_2, family = &quot;binomial&quot;)</code></pre>
<p>Next, we need to predict the probability for each individual based on the model fit under each treatment condition. This will give us <span class="math inline">\(\hat{p}_{i1}\)</span> and <span class="math inline">\(\hat{p}_{i0}\)</span>:</p>
<pre class="r"><code>newdata &lt;- dd_2[, .(rx=1, x)]
p1 &lt;- mean(predict(glmfit, newdata, type = &quot;response&quot;))

newdata &lt;- dd_2[, .(rx=0, x)]
p0 &lt;- mean(predict(glmfit, newdata, type = &quot;response&quot;))

c(p1, p0)</code></pre>
<pre><code>## [1] 0.152 0.068</code></pre>
<p>A simple calculation gives us the point estimate for the risk difference (and note that the estimated OR is close to 2.5, the value used to generate the data):</p>
<pre class="r"><code>risk_diff &lt;- p1 - p0
odds_ratio &lt;- exp(coef(glmfit)[&quot;rx&quot;])
  
c(rd = risk_diff, or = odds_ratio)</code></pre>
<pre><code>##    rd or.rx 
## 0.084 2.456</code></pre>
<p>We can use a bootstrap method to estimate a 95% confidence interval for risk difference. This involves sampling ids from each treatment group <em>with</em> replacement, fitting a new logistic regression model, predicting probabilities, and calculating a the risk difference. This is repeated 999 times to get a distribution of risk differences, from which we extract an estimated confidence interval:</p>
<pre class="r"><code>bootdif &lt;- function(dd) {
  
  db &lt;- dd[, .(id = sample(id, replace = TRUE)), keyby = rx]
  db &lt;- merge(db[, id, rx], dd, by = c(&quot;id&quot;, &quot;rx&quot;))
  
  glmfit &lt;- glm(y ~ rx + x, data = db, family = &quot;binomial&quot;)

  newdata &lt;- db[, .(rx=1, x)]
  p1 &lt;- mean(predict(glmfit, newdata, type = &quot;response&quot;))

  newdata &lt;- db[, .(rx=0, x)]
  p0 &lt;- mean(predict(glmfit, newdata, type = &quot;response&quot;))

  return(p1 - p0)
}

bootest &lt;- unlist(mclapply(1:999, function(x) bootdif(dd_2), mc.cores = 4))
quantile(bootest, c(0.025, 0.975))</code></pre>
<pre><code>##  2.5% 97.5% 
## 0.032 0.134</code></pre>
</div>
<div id="change-in-distribution-changes-risk-difference" class="section level3">
<h3>Change in distribution changes risk difference</h3>
<p>To illustrate how a shift in the distribution of <span class="math inline">\(x\)</span> can influence the marginal risk difference without changing the odds ratio, I just need to specify the mean of <span class="math inline">\(x1\)</span> to be closer to 1. This creates a left-skewed distribution that will increase the risk difference:</p>
<pre class="r"><code>mu_x = 0.8</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-11-1.png" width="576" /></p>
<p>The risk difference appears to increase, but the OR seems to be pretty close to the true value of 2.5:</p>
<pre><code>##    rd or.rx 
##  0.18  2.59</code></pre>
<p>And for completeness, here is the estimated confidence interval:</p>
<pre><code>##  2.5% 97.5% 
##  0.11  0.25</code></pre>
</div>
<div id="a-more-robust-comparison" class="section level3">
<h3>A more robust comparison</h3>
<p>It is hardly fair to evaluate this property using only two data sets. It is certainly possible that the estimated risk differences are inconsistent just by chance. I have written some functions (provided below in the<a href="#addendum">addendum</a>) that facilitate the replication of numerous data sets created under different distribution assumptions to a generate a distribution of estimated risk differences (as well as a distribution of estimated ORs). I have generated 5000 data sets of 500 observations each under four different assumptions of <code>mu_x</code> used in the data generation process defined above: {0.2, 0.4, 0.6, 0.8}.</p>
<p>It is pretty apparent the the risk difference increases as the distribution of <span class="math inline">\(x\)</span> shifts from right-skewed to left-skewed:</p>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-16-1.png" width="576" /></p>
<p>And it is equally apparent that shifting the distribution has no impact on the OR, which is consistent across different levels of <span class="math inline">\(x\)</span>:</p>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-17-1.png" width="576" /></p>
<p>
<p><small><font color="darkkhaki"></p>
<p>References:</p>
<p>Austin, Peter C. “Absolute risk reductions, relative risks, relative risk reductions, and numbers needed to treat can be obtained from a logistic regression model.” <em>Journal of clinical epidemiology</em> 63, no. 1 (2010): 2-6.</p>
<p>Kleinman, Lawrence C., and Edward C. Norton. “What’s the risk? A simple approach for estimating adjusted risk measures from nonlinear models including logistic regression.” <em>Health services research</em> 44, no. 1 (2009): 288-302.</p>
<p>Richardson, Thomas S., James M. Robins, and Linbo Wang. “On modeling and estimation for the relative risk and risk difference.” <em>Journal of the American Statistical Association</em> 112, no. 519 (2017): 1121-1130.</p>
<p>Support:</p>
<p>This work was supported in part by the National Institute on Aging (NIA) of the National Institutes of Health under Award Number U54AG063546, which funds the NIA IMbedded Pragmatic Alzheimer’s Disease and AD-Related Dementias Clinical Trials Collaboratory (<a href="https://impactcollaboratory.org/" targt="_blank">NIA IMPACT Collaboratory</a>). The author, a member of the Design and Statistics Core, was the sole writer of this blog post and has no conflicts. The content is solely the responsibility of the author and does not necessarily represent the official views of the National Institutes of Health.</p>
</font></small>
</p>
<p><a name="addendum"></a></p>
<p> </p>
</div>
<div id="addendum-replication-code" class="section level3">
<h3>Addendum: replication code</h3>
<pre class="r"><code>s_define &lt;- function() {
  
  def &lt;- defDataAdd(varname = &quot;x1&quot;, formula = &quot;..mu_x&quot;, variance = 8, dist = &quot;beta&quot;)
  def &lt;- defDataAdd(def, varname = &quot;x&quot;, formula = &quot;x1 - 0.5&quot;, dist = &quot;nonrandom&quot;)
  def &lt;- defDataAdd(def, varname = &quot;y&quot;, 
    formula = &quot;-2 + 1 * rx + 1.5 * x&quot;,
    dist = &quot;binary&quot;, link=&quot;logit&quot;)
  
  return(list(def = def)) # list_of_defs is a list of simstudy data definitions
}

s_generate &lt;- function(list_of_defs, argsvec) {
  
  list2env(list_of_defs, envir = environment())
  list2env(as.list(argsvec), envir = environment())
  
  dx &lt;- genData(n)
  dx &lt;- trtAssign(dx, grpName = &quot;rx&quot;)
  dx &lt;- addColumns(def, dx)
  
  return(dx) #  generated data is a data.table
}

s_model &lt;- function(dx) {
  
  glmfit &lt;- glm(y ~ rx + x, data = dx, family = &quot;binomial&quot;)
  
  newdata &lt;- dx[, .(rx=1, x)]
  p1 &lt;- mean(predict(glmfit, newdata, type = &quot;response&quot;))
  
  newdata &lt;- dx[, .(rx=0, x)]
  p0 &lt;- mean(predict(glmfit, newdata, type = &quot;response&quot;))
  
  risk_diff &lt;- p1 - p0
  odds_ratio &lt;- exp(coef(glmfit)[&quot;rx&quot;])
  
  model_results &lt;- data.table(risk_diff, odds_ratio)
  
  return(model_results) # model_results is a data.table
}

s_single_rep &lt;- function(list_of_defs, argsvec) {
  
  generated_data &lt;- s_generate(list_of_defs, argsvec)
  model_results &lt;- s_model(generated_data)
  
  return(model_results)
}


s_replicate &lt;- function(argsvec, nsim) {
  
  list_of_defs &lt;- s_define()
  
  model_results &lt;- rbindlist(
    parallel::mclapply(
      X = 1 : nsim, 
      FUN = function(x) s_single_rep(list_of_defs, argsvec), 
      mc.cores = 4)
  )
  
  model_results &lt;- cbind(t(argsvec), model_results)
  
  return(model_results) # summary_stats is a data.table
}

### Scenarios

scenario_list &lt;- function(...) {
  argmat &lt;- expand.grid(...)
  return(asplit(argmat, MARGIN = 1))
}

#----

n &lt;- 500
mu_x &lt;- c(0.2, 0.4, 0.6, 0.8)

scenarios &lt;- scenario_list(n = n, mu_x = mu_x)

summary_stats &lt;- rbindlist(lapply(scenarios, function(a) s_replicate(a, nsim = 5000)))

ggplot(data = summary_stats, aes(x = risk_diff, group = mu_x)) +
  geom_density(aes(fill = factor(mu_x)), alpha = .7) +
  scale_fill_canva(palette = &quot;Simple but bold&quot;, name = &quot;mu_x&quot;) +
  theme(panel.grid = element_blank()) +
  xlab(&quot;estimated risk difference&quot;)

ggplot(data = summary_stats, aes(x = odds_ratio, group = mu_x)) +
  geom_density(aes(fill = factor(mu_x)), alpha = .7) +
  scale_fill_canva(palette = &quot;Simple but bold&quot;, name = &quot;mu_x&quot;) +
  theme(panel.grid = element_blank()) +
  xlab(&quot;estimated odds ratio&quot;)</code></pre>
</div>
