---
title: Estimating a Bayesian proportional hazards model
author: Package Build
date: '2025-02-11'
slug: []
categories: []
tags:
  - R
  - survival analysis
type: ''
subtitle: ''
image: ''
draft: TRUE
---



<p>Like most of my posts, this one stems from a conversation with a colleague about a large cluster randomized trial that he’s innovated with, which uses a <a href="https://www.rdatagen.net/post/2022-12-13-modeling-the-secular-trend-in-a-stepped-wedge-design/" target="_blank">stepped-wedge design</a> (SW-CRT) he is involved with. The challenge here is that the primary outcome of interest is a time-to-event. I was intrigued by how to analyze this kind of data to estimate a hazard ratio while accounting for clustering and potential secular trends that might influence the time to the event. A quick dive into the literature suggested that time-to-event outcomes aren’t all that common in SW-CRTs-and that the best analytic approach isn’t entirely clear.</p>
<p>My first thought, naturally, was: <em>How would I simulate data to explore different modeling approaches?</em> My second thought: <em>Could a Bayesian approach be useful here?</em></p>
<p>I’m getting a little ahead of myself. Generating the complex data (with clustering and secular time trend) turned out to be fairly straightforward (I’ll share that in a future post). But here I’m focusing on the early stages of developing a Bayesian model that might eventually accommodate flexible secular trends using splines, something I explored a bit <a href="https://www.rdatagen.net/post/2024-10-08-can-chatgpt-help-construct-non-trivial-bayesian-models-with-cluster-specific-splines/" target="_blank">before</a>. I started by generating a simple data set of time-to-event outcomes (without any clustering or time trends) and fitting a proportional hazards model using <code>Stan</code>code drawn from the <a href="https://mc-stan.org/docs/stan-users-guide/survival.html#proportional-hazards-model" target="_blank">online guide</a>. That model works fine, but it has a serious limitation, which I addressed in subsequent iterations. Getting to the final model turned out to be a bit more challenging than I initially envisioned. In this post, I’ll walk through the steps I took along the way, but if you’re just here for the final model, feel free to jump to the end.</p>
<p>My hope is that the final model will be flexible enough to handle the extensions I need for the data structure that sparked this whole exploration.</p>
<div id="simulating-an-rct-with-time-to-event-outcomes" class="section level3">
<h3>Simulating an RCT with time-to-event outcomes</h3>
<p>Before we get to the code, here are the <code>R</code> packages that are used in this post:</p>
<pre class="r"><code>library(simstudy)
library(data.table)
library(survival)
library(cmdstanr)</code></pre>
<p>Here are the data definitions for a two-arm randomized controlled trial that is stratified by a variable <span class="math inline">\(M\)</span>. Both the treatment <span class="math inline">\(A\)</span> and covariate <span class="math inline">\(M\)</span> are associated with the time-to-event outcome, as defined in <code>defS</code>. On average, the treatment speeds up the time-to-event, and <span class="math inline">\(M\)</span> slows things down. (In <code>simstudy</code> survival times are <a href="https://www.rdatagen.net/post/2022-02-08-simulating-survival-outcomes-setting-the-parameters-for-the-desired-distribution/" target="_blank">generated</a> using a Weibull data generation process.)</p>
<pre class="r"><code>defI &lt;-
  defData(varname = &quot;M&quot;, formula = 0.3, dist = &quot;binary&quot;) |&gt;
  defData(varname = &quot;A&quot;, formula = &quot;1;1&quot;, variance = &quot;M&quot;, dist = &quot;trtAssign&quot;)

defS &lt;- 
  defSurv(
    varname = &quot;timeEvent&quot;, 
    formula = &quot;-11.6 + ..delta_f * A + ..beta_m * M&quot;,
    shape = 0.30)  |&gt;
  defSurv(varname = &quot;censorTime&quot;, formula = -11.3, shape = .35)

## Parameters

delta_f &lt;- 1.5
beta_m &lt;- -1.0</code></pre>
<p>We are generating 1000 independent observations:</p>
<pre class="r"><code>set.seed(123)

dd &lt;- genData(1000, defI)
dd &lt;- genSurv(dd, defS, timeName = &quot;tte&quot;, censorName = &quot;censorTime&quot;, eventName = &quot;event&quot;)
dd</code></pre>
<pre><code>## Key: &lt;id&gt;
## Index: &lt;type&gt;
##          id     M     A    tte event       type
##       &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;num&gt; &lt;num&gt;     &lt;char&gt;
##    1:     1     0     0 26.974     1  timeEvent
##    2:     2     1     0 38.353     1  timeEvent
##    3:     3     0     0 32.836     1  timeEvent
##    4:     4     1     0 28.768     0 censorTime
##    5:     5     1     0 54.366     1  timeEvent
##   ---                                          
##  996:   996     1     1 11.012     0 censorTime
##  997:   997     0     0 15.420     1  timeEvent
##  998:   998     0     0 21.212     1  timeEvent
##  999:   999     1     0 41.153     0 censorTime
## 1000:  1000     0     1 25.659     1  timeEvent</code></pre>
<p>Here is a Kaplan-Meier plot showing the “survival” times for each level of <span class="math inline">\(M\)</span> and each treatment arm:</p>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/plot1-1.png" width="672" /></p>
<p>Fitting a traditional Cox proportional hazards model, we can see that the log hazard ratio for treatment <span class="math inline">\(A\)</span> is greater than 0, suggesting that on average the time-to-event is shorter for those in the treatment arm. Likewise, those with <span class="math inline">\(M=1\)</span> have longer times-to-events and the log hazard ratio is less than zero:</p>
<pre class="r"><code>cox_model &lt;- coxph(Surv(tte, event) ~ A + M, data = dd)
summary(cox_model)</code></pre>
<pre><code>## Call:
## coxph(formula = Surv(tte, event) ~ A + M, data = dd)
## 
##   n= 1000, number of events= 821 
## 
##       coef exp(coef) se(coef)      z Pr(&gt;|z|)    
## A  1.44309   4.23374  0.08018  18.00   &lt;2e-16 ***
## M -0.92537   0.39638  0.08302 -11.15   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
##   exp(coef) exp(-coef) lower .95 upper .95
## A    4.2337     0.2362    3.6181    4.9542
## M    0.3964     2.5228    0.3369    0.4664
## 
## Concordance= 0.695  (se = 0.009 )
## Likelihood ratio test= 415.5  on 2 df,   p=&lt;2e-16
## Wald test            = 389  on 2 df,   p=&lt;2e-16
## Score (logrank) test = 423.4  on 2 df,   p=&lt;2e-16</code></pre>
</div>
<div id="first-bayes-model" class="section level3">
<h3>First Bayes model</h3>
<p>As I mentioned before, I turned to <code>Stan</code> <a href="https://mc-stan.org/docs/stan-users-guide/survival.html#proportional-hazards-model" target="_blank">documentation</a> for the code that follows. I won’t go into the detailed derivation of the partial likelihood here since that is covered very nicely in the document. However, it is useful to see the final likelihood that is then reflected in the code.</p>
<p>The likelihood is written as follow - and note that the <span class="math inline">\(n\)</span>’s represent only the cases with observed times:</p>
<p><span class="math display">\[
L(\beta)= \prod_{n=1}^{N^{obs}} \left( \frac{\text{exp}(x_n \cdot\beta)}{\sum_{n&#39;=n}^{N} \text{exp}(x_{n&#39;} \cdot \beta)}\right)
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(N^{obs}\)</span> is the number of observed times,</li>
<li><span class="math inline">\(x_n\)</span> is the covariate for the <span class="math inline">\(n\)</span>-th observed time,</li>
<li><span class="math inline">\(\beta\)</span> is the coefficient (and is interpreted as the log hazard ratio)</li>
</ul>
<p>We want the log likelihood, this gets transformed to sums of logs as</p>
<p><span class="math display">\[
\begin{aligned}
\text{log } Pr[\text{obs. fails ordered } 1, \dots N^{obs}|x, \beta] &amp;= \sum_{n=1}^{N^{obs}} \text{log} \left( \frac{\text{exp}(x_n \cdot\beta)}{\sum_{n&#39;=n}^{N} \text{exp}(x_{n&#39;} \cdot \beta)}\right) \\
\\
&amp;= \sum_{n=1}^{N^{obs}} \left( \text{exp}(x_n \cdot\beta) - \text{log}\sum_{n&#39;=n}^{N} \text{exp}(x_{n&#39;} \cdot \beta)\right)
\end{aligned}
\]</span>
In <code>Stan</code>, the function <code>logSumExp</code> can be used to efficiently calculate</p>
<p><span class="math display">\[
\text{log}\sum_{n&#39;=n}^{N} \text{exp}(x_{n&#39;} \cdot \beta).
\]</span></p>
<p>This partial likelihood is implemented below in <code>Stan</code>. One confusing aspect (at least to me) is the way censoring is handled. Essentially, all event times for censored cases are assumed to occur <em>after</em> the last observed time. In this case, all censored cases are part of the risk set for observed events, which is not an approach I had seen before. This is a pretty big assumption and has implications for data where the actual censoring times are before the last observed event time.</p>
<p>The code is a little confusing, because the data are delivered to <code>Stan</code> in reverse order. It is done this way to make calculation of the log likelihood more efficient. But if you are trying to follow along with the code to see how it lines up with the equations above, it is good to keep in mind.</p>
<pre class="r"><code>stan_code &lt;-
&quot;
data {
  int&lt;lower=0&gt; K;          // num covariates

  int&lt;lower=0&gt; N;          // num uncensored obs
  vector[N] t;             // event time (non-strict decreasing)
  matrix[N, K] x;          // covariates for uncensored obs

  int N_c;                 // num censored obs
  real &lt;lower=t[N]&gt; t_c;   // censoring time
  matrix[N_c, K] x_c;      // covariates for censored obs
}

parameters {
  vector[K] beta;          // slopes (no intercept)
}

transformed parameters {
  vector[N] log_theta = x * beta;
  vector[N_c] log_theta_c = x_c * beta;
}

model {
  beta ~ normal(0, 4);
  
  real log_denom = log_sum_exp(log_theta_c);
  
  for (n in 1:N) {
    log_denom = log_sum_exp(log_denom, log_theta[n]);
    target += log_theta[n] - log_denom;   // log likelihood
  }
  
}
&quot;</code></pre>
<p>This code prepares the <code>R</code> data for <code>Stan</code>:</p>
<pre class="r"><code>dd.o &lt;- dd[event == 1]
setorder(dd.o, -tte)
x.o &lt;- data.frame(dd.o[, .(A, M)])
N.o &lt;- dd.o[, .N]
t.o &lt;- dd.o[, tte]

dd.c &lt;- dd[event == 0]
setorder(dd.c, -tte)
x.c &lt;- data.frame(dd.c[, .(A, M)])
N.c &lt;- dd.c[, .N]
t.c &lt;- dd.c[, tte]

K &lt;- ncol(x.o)          # num covariates

stan_data &lt;- list(
  K = K,
  N = N.o,
  t = t.o,
  x = x.o,
  N_c = N.c,
  t_c = max(t.c),
  x_c = x.c
)</code></pre>
<p>I’m using <code>cmdstanr</code> to interface with <code>Stan</code>. First we compile the <code>Stan</code> code.</p>
<pre class="r"><code>stan_model &lt;- cmdstan_model(write_stan_file(stan_code))</code></pre>
<p>And then we fit the model. Even with 1,000 observations, the model estimates in just a couple of seconds on my laptop.</p>
<pre class="r"><code>fit &lt;- stan_model$sample(
  data = stan_data, 
  iter_warmup = 1000,
  iter_sampling = 4000,
  chains = 4,
  parallel_chains = 4,
  refresh = 0
)</code></pre>
<pre><code>## Running MCMC with 4 parallel chains...
## 
## Chain 3 finished in 1.6 seconds.
## Chain 1 finished in 1.8 seconds.
## Chain 4 finished in 1.7 seconds.
## Chain 2 finished in 1.8 seconds.
## 
## All 4 chains finished successfully.
## Mean chain execution time: 1.8 seconds.
## Total execution time: 2.0 seconds.</code></pre>
<p>Looking at the log hazard ratios, though, something seems awry. The Bayesian estimates are attenuated relative to the original Cox PH estimates, and this is not due to the prior distribution assumption. Rather, it is the result of assuming that all censored times are longer than the longest observed time-to-event. I’m not showing this here, but the attenuation does largely go away if there is no censoring.</p>
<pre class="r"><code>fit$summary(variables = &quot;beta&quot;)</code></pre>
<pre><code>## # A tibble: 2 × 10
##   variable   mean median     sd    mad     q5    q95  rhat ess_bulk ess_tail
##   &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
## 1 beta[1]   1.03   1.03  0.0723 0.0720  0.916  1.15   1.00   14541.   11205.
## 2 beta[2]  -0.588 -0.588 0.0802 0.0802 -0.721 -0.455  1.00   13062.   10867.</code></pre>
<p>Given this limitation, I decided to try to implement an algorithm that accommodates dynamic risk sets, effectively taking censored cases out of the analysis as soon as they are censored. This is what the model estimated using <code>coxph</code> above does.</p>
</div>
<div id="second-bayes-model" class="section level3">
<h3>Second Bayes model</h3>
<p>The partial likelihood for the Cox proportional hazards model is given by:</p>
<p><span class="math display">\[
L(\beta) = \prod_{i=1}^{n} \left( \frac{\exp x_i \beta}{\sum_{j \in R(t_i)} \exp(x_j\beta)} \right)^{\delta_i}
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(n\)</span> is the number of observations (censored or not),</li>
<li><span class="math inline">\(x_i\)</span> is the covariates for the <span class="math inline">\(i\)</span>-th observation,</li>
<li><span class="math inline">\(\beta\)</span> is the coefficient of interest,</li>
<li><span class="math inline">\(t_i\)</span> is the observed time for the <span class="math inline">\(i\)</span>-th observation,</li>
<li><span class="math inline">\(R(t_i)\)</span> is the risk set at time <span class="math inline">\(t_i\)</span> (the set of individuals still at risk just before time <span class="math inline">\(t_i\)</span>) - which only includes censored cases censored <em>after</em> <span class="math inline">\(t_i\)</span>,</li>
<li><span class="math inline">\(\delta_i\)</span> is the event indicator (<span class="math inline">\(\delta_i = 1\)</span> if the event occurred, <span class="math inline">\(\delta_i = 0\)</span> if censored).</li>
</ul>
<p>The <code>Stan</code> code below codes the log likelihood that follows from this. In contrast to the first version, the data is passed to <code>Stan</code> in ascending order. The one major complication is that I needed to create a search function in order to define the “risk” set. Actually, I asked DeepSeek to do this for me, and it obliged. The fundamental difference between this approach and the first approach is calculation of the denominator in the log likelihood.</p>
<pre class="r"><code>stan_code &lt;-
&quot;
functions {
  int binary_search(vector v, real tar_val) {
    int low = 1;
    int high = num_elements(v);
    int result = -1;

    while (low &lt;= high) {
      int mid = (low + high) %/% 2;
      if (v[mid] == tar_val) {
        result = mid; // Store the index
        high = mid - 1; // Look for earlier occurrences
      } else if (v[mid] &lt; tar_val) {
        low = mid + 1;
      } else {
        high = mid - 1;
      }
    }
    return result;
  }
}

data {
  int&lt;lower=0&gt; K;          // Number of covariates

  int&lt;lower=0&gt; N_o;        // Number of uncensored observations
  vector[N_o] t_o;         // Event times (sorted in decreasing order)
  matrix[N_o, K] x_o;      // Covariates for uncensored observations

  int&lt;lower=0&gt; N;          // Number of total observations
  vector[N] t;             // Individual times
  matrix[N, K] x;          // Covariates for all observations
}

parameters {
  vector[K] beta;          // Fixed effects for covariates
}

model {
  
  // Prior

  beta ~ normal(0, 4);
  
  // Model

  vector[N] log_theta = x * beta;

  for (n_o in 1:N_o) {
    int start_risk = binary_search(t, t_o[n_o]); // Use binary search
    real log_denom = log_sum_exp(log_theta[start_risk:N]);
    target += log_theta[start_risk] - log_denom;
  }

}
&quot;</code></pre>
<p>Preparing the data is a little different. This time, I am passing the observed data and the full data, both in ascending order:</p>
<pre class="r"><code>dx &lt;- copy(dd)
setorder(dx, tte)

dx.o &lt;- dx[event == 1]
x_o &lt;- data.frame(dx.o[, .(A, M)])
N_o &lt;- dx.o[, .N]
t_o &lt;- dx.o[, tte]

x_all &lt;- data.frame(dx[, .(A, M)])
N_all &lt;- dx[, .N]
t_all &lt;- dx[, tte]

K &lt;- ncol(x_o)          # num covariates

stan_data &lt;- list(
  K = K,
  N_o = N_o,
  t_o = t_o,
  x_o = x_o,
  N = N_all,
  t = t_all,
  x = x_all
)

stan_model &lt;- cmdstan_model(write_stan_file(stan_code))

fit &lt;- stan_model$sample(
  data = stan_data,
  iter_warmup = 1000,
  iter_sampling = 4000,
  chains = 4,
  parallel_chains = 4,
  refresh = 0
)</code></pre>
<pre><code>## Running MCMC with 4 parallel chains...
## 
## Chain 2 finished in 58.1 seconds.
## Chain 4 finished in 66.9 seconds.
## Chain 1 finished in 67.3 seconds.
## Chain 3 finished in 67.6 seconds.
## 
## All 4 chains finished successfully.
## Mean chain execution time: 65.0 seconds.
## Total execution time: 67.7 seconds.</code></pre>
<pre class="r"><code>fit$summary(variables = &quot;beta&quot;)</code></pre>
<pre><code>## # A tibble: 2 × 10
##   variable   mean median     sd    mad    q5    q95  rhat ess_bulk ess_tail
##   &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
## 1 beta[1]   1.44   1.43  0.0801 0.0801  1.31  1.57   1.00   12604.   10436.
## 2 beta[2]  -0.925 -0.924 0.0816 0.0817 -1.06 -0.791  1.00   12492.   10436.</code></pre>
<p>Two things to note about this model. First, it appears that it worked! The estimates mirror the estimates from the <code>coxph</code> model using the <code>survival</code> package. Second, the implementation is very inefficient, taking more than a minute to run! This does not bode well for a more complex model that incorporates random effects and splines.</p>
</div>
<div id="final-bayes-model" class="section level3">
<h3>Final Bayes model</h3>
<p>I asked ChatGPT this time to see if it could make my code more efficient. (I’ve been comparing ChatGPT and DeepSeek - both have been pretty impressive.) It recognized that my initial brute force approach was calculating each denominator anew for each observed evaluation. This is highly inefficient (on the order of <span class="math inline">\(O(N^2)\)</span>). The algorithm is reconfigured so that the denominators are pre-calculated - starting with the last time point (censored or observed), similar to the first approach. It turns out this is much more efficient, <span class="math inline">\(O(N)\)</span>.</p>
<pre class="r"><code>stan_code &lt;-
&quot;
...

model {
  
  // Prior
  
  beta ~ normal(0, 4);
  
    // Likelihood
  
  vector[N] theta = x * beta;
  vector[N] log_sum_exp_theta;
  
  // Compute cumulative sum of exp(theta) in log space
  
  log_sum_exp_theta[N] = theta[N]; // Initialize the last element
  
  for (i in tail(sort_indices_desc(t), N-1)) {
    log_sum_exp_theta[i] = log_sum_exp(theta[i], log_sum_exp_theta[i + 1]);
  }

  for (n_o in 1:N_o) {
    int start_risk = binary_search(t, t_o[n_o]); // Use binary search
    real log_denom = log_sum_exp_theta[start_risk];
    target += theta[start_risk] - log_denom;
  }
}
&quot;</code></pre>
<p>The data requirements for this are the as the second model, so no changes are needed.</p>
<pre class="r"><code>stan_model &lt;- cmdstan_model(write_stan_file(stan_code))

fit &lt;- stan_model$sample(
  data = stan_data,
  iter_warmup = 1000,
  iter_sampling = 4000,
  chains = 4,
  parallel_chains = 4,
  refresh = 0
)</code></pre>
<pre><code>## Running MCMC with 4 parallel chains...
## 
## Chain 3 finished in 2.2 seconds.
## Chain 2 finished in 2.4 seconds.
## Chain 4 finished in 2.4 seconds.
## Chain 1 finished in 2.8 seconds.
## 
## All 4 chains finished successfully.
## Mean chain execution time: 2.4 seconds.
## Total execution time: 2.8 seconds.</code></pre>
<pre class="r"><code>fit$summary(variables = &quot;beta&quot;)</code></pre>
<pre><code>## # A tibble: 2 × 10
##   variable   mean median     sd    mad    q5    q95  rhat ess_bulk ess_tail
##   &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
## 1 beta[1]   1.44   1.44  0.0800 0.0794  1.30  1.57   1.00   12036.   10499.
## 2 beta[2]  -0.925 -0.925 0.0831 0.0834 -1.06 -0.789  1.00   11887.   10386.</code></pre>
<p>The model still works, as the estimate is the same as the previous (correct) model. And the computation time is reduced considerably, to about 3 seconds. This gives me hope for a future, more complex model.</p>
</div>
