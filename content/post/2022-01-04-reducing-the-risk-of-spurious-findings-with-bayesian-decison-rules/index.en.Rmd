---
title: Minimize skepticism about subgroup analyses with skeptical Bayesian decison rules
author: Keith Goldfeld
date: '2022-01-04'
slug: []
categories: []
tags:
  - R
  - Bayesian model
type: ''
subtitle: ''
image: ''
draft: TRUE
---

Over the past couple of years, I have been working with an amazing group of investigators as part of the CONTAIN trial to study whether COVID-19 convalescent plasma (CCP) can improve the clinical status of patients hospitalized with COVID-19 and requiring noninvasive supplemental oxygen. This was a multi-site study in the US that randomized 941 patients to either CCP or a saline solution placebo. The overall [findings](https://jamanetwork.com/journals/jamainternalmedicine/article-abstract/2787090){target="_blank"} suggest that CCP did not benefit the patients who received it, but if you drill down a little deeper, the story may be more complicated than that.

Of course, it is the "drilling down" part that gets people (and biostatisticians) a little nervous. Once we get beyond the primary analysis, all bets are off. If we look hard enough at the data, we may eventually find something that is interesting to report on. But, just because we find something with the data in hand, it does not mean that we would find it again in another data set from another study. Any conclusions we draw may be unwarranted:

<br>

![Source: [XKCD](https://xkcd.com/882/){target="_blank"}](extra/significant.png){width=50%}

The CONTAIN trial was conducted under very difficult circumstances, where the context was rapidly changing over time. Particularly challenging was the fact that new therapies were introduced for hospitalized patients over the course of the trial. And when we looked at the data, we noticed that while all patients had poor clinical outcomes in the first several months of the study, the CCP arm appeared to offer some benefits. Later on in the study, when corticosteroids and remdesivir were standard of care, patient outcomes were dramatically improved, and CCP was no longer providing any benefits. This was a very interesting finding that we felt merited some discussion.

Not surprisingly, we have received some push back, suggesting that this finding is a classic case of [regression to the mean](https://www.rdatagen.net/post/regression-to-the-mean/){target="_blank"}. Normally, I would not have been comfortable presenting those findings, particularly not in a highly visible journal article. But we had used a Bayesian modelling framework with quite conservative decision criteria and quite skeptical prior distribution assumptions to evaluate the primary outcome and the exploratory outcomes, so we felt that while there is still a non-trivial chance that the findings are spurious, these were not green jelly bean findings. Add to this strong biological plausibility, and we felt quite strongly about presenting this to the growing body of literature about CCP.

In this post, I am exploring through a series of simulations how conservative our conservative approach really is to make an assessment of how to interpret our exploratory analyses. This is [another look](https://www.rdatagen.net/post/2021-12-21-controling-type-1-error-rates-in-rcts-with-interim-looks-a-bayesian-perspective/){target="_blank"} at assessing Type I error rates, a frequentist notion, in the context of a Bayesian study design.

## The general idea

Let's say I have a data generation process individual $i$ with a (binary) outcome $Y$ with some (binary) treatment or exposure $A$ that looks like

$$log\left(\frac{P(Y_i=1)}{P(Y_i=0)}\right ) = \alpha + \delta A_i.$$

The log-odds is dependent only on the level of exposure $A$. 

Let's say that we've also measured some characteristic $G$, which is a categorical variable with three levels. While the primary aim of the study is to estimate $\delta$, the log-odds ratio comparing treated with controls, we might also be interested in a subgroup analysis based on $G$. That is, is there a unique group-level treatment effect for any level of $G$, $G \in \{1,2,3\}$? In terms of the model, this would be

$$ log\left(\frac{P(Y_i=1|G_i=g)}{P(Y_i=0|G_i=g)}\right ) = \alpha_g + \delta_g A_i, \ \  g \in \{1,2,3\}.$$
 
In the case here, we know that $\alpha_g = \alpha$ and $\delta_g = \deltaa$ for all $g$ because the data generation process is independent of $G$. However, due to sampling error it is quite possible that we will observe some differences in the data.

We can start by simulating this. But first, here are all the libraries used to create the examples in this post:

```{r, message=FALSE, warning=FALSE}
library(simstudy)
library(data.table)
library(posterior)
library(bayesplot)
library(cmdstanr)
library(gtsummary)
library(paletteer)
library(ggplot2)
library(ggdist)
```

## Data generation

```{r}
def <- defData(varname = "g", formula = "1/3;1/3;1/3", dist = "categorical")
def <- defData(def, "a", formula = "1;1", dist = "trtAssign")
def <- defData(def, "y", formula = "0.10", dist = "binary")

RNGkind("L'Ecuyer-CMRG")
set.seed(67)

dd <- genData(1000, def)
dd
```

## Logistic regression

We can fit a simple logistic model to evaluate estimate $\delta$, the overall effect of the treatment. We see that the estimate of the OR is very close to 1, suggesting the odds of $Y=1$ is similar for both groups, so no apparent treatment effect. (Note that the exponentiated intercept is an estimate of the odds of $Y=1$ for the control arm. The data generation process assumed $P(Y=1) = 0.10$, so the odds are $0.11$.)

```{r}
fitglm <- glm(y ~ a, data = dd, family = "binomial")
tbl_regression(fitglm, intercept = TRUE, exponentiate = TRUE)
```

So, we could not infer that for the entire group treatment $A$ has any effect. But, maybe for some subgroups there is a treatment effect? We can fit a generalized linear model that allows the intercept and effect estimate vary by level of $G$, and assess whether this is the case for subgroups defined by $G$.

```{r}
fitglm <- glm(y ~ factor(g) + a:factor(g) - 1, data = dd, family = "binomial")
tbl_regression(fitglm, exponentiate = TRUE)
```

While there is more variability in the estimates, we still wouldn't conclude that there are treatment effects within each level of $G$.

## Bayesian model

The purpose of this post is to illustrate how a appropriately specified Bayesian model can provide slightly more reliable estimates, particularly in the case where there really are no underlying treatment effects. So, here is a Bayes model that estimates subgroup-level intercepts and treatment effects:

$$Y_{ig} \sim Bin(p_{ig})$$
$$log\left(\frac{p_{ig}}{1-p_{ig}}\right) = \alpha_g + \delta_gA_i, \ \ g \in \{1,2,3\}  $$

The prior distribution assumptions for the parameters $\alpha_g$ and $\delta_g$ are

\begin{aligned}
  \alpha_g &\sim N(\mu = 0, \sigma = 10), \ \ g \in \{1,2,3\} \\

  \delta_g &\sim N(\mu=\delta, \sigma = 0.3537), \ \ g \in \{1,2,3\} \\

  \delta &\sim N(\mu = 0, \sigma = 0.3537)
\end{aligned}

Note that the variance of the $\delta_g\text{'s}$ around $\delta$ has been specified, but it could be estimated. However, since there are very few levels of $G$,  estimation of the variance can be slow; to speed the simulations, I've chosen a quicker path by specifying a pretty informative prior.

#### Fitting the Bayes model

The `Stan` code for the model can be found in the appendix. To estimate the model, the the data needs to be passed as a list - and here is a function to convert the `R` data into the proper format:

```{r}
listdat <- function(dx, grpvar) {
  
  dx[, grp := factor(get(grpvar))]
  
  N <- dx[, .N]
  L <- dx[, nlevels(grp)]
  y <- dx[, y]
  a <- dx[, a]
  grp <-dx[, as.numeric(grp)]
  
  list(N = N, L = L, y = y, a = a, grp = grp)
}
```

After compiling the program, samples from the posterior are drawn using four chains. There will be a total of 12000 samples (not including the warm-up samples):

```{r}
mod <- cmdstan_model("extra/simulation.stan")

fitbayes <- mod$sample(
    data = listdat(dd, "g"),
    refresh = 0,
    chains = 4L,
    parallel_chains = 4L,
    iter_warmup = 1500,
    iter_sampling = 3000
)
```

The estimates are quite similar to the `glm` estimates, though the ORs are pulled slightly towards 1 as a result of the informative prior. This is going to be the trick that ultimately protects the Type I error rate from completely blowing up.

```{r}
fitbayes$summary(c("Odds_g","OR_g"))
```

And here is a plot of the posterior distributions for the treatment effect at each subgroup defined by the levels of $G$:

```{r, fig.height = 3.5, fig.width = 4}
OR_df <- data.frame(as_draws_rvars(fitbayes$draws(variables = "OR_g")))

p_data <- with(OR_df, data.frame(
  cat = c("c1", "c2", "c3"),
  OR_g = OR_g
))

ggplot(data = p_data, aes(dist = OR_g, y = cat)) +
  geom_vline(xintercept = 1, color = "grey80", size = .3) +
  stat_dist_halfeye(fill = palettes_d$awtools$a_palette[6], position="dodge") +
  theme(panel.grid = element_blank()) +
  ylab("category") +
  scale_x_continuous(limits = c(0, 2), name = "OR")
```

It is easy to see that the 95% credible intervals all include the value of 1, no treatment effect, so we wouldn't be tempted to conclude that there is any treatment effect. We can also check manually to see if at least one of the credible intervals excludes 1. The answer is still "no."

```{r}
with(OR_df, any((quantile(OR_g, .025) > 1) | (quantile(OR_g, .975) < 1)))
```

## Multiple characteristics that define many subgroups

As the jelly beans make clear, things can really start to go awry when we start to investigate many possible subgroups. This can either be a single characteristic (like color) that has many, many levels, each of which can be a subgroup, or this can be many different characteristics that each have a few different levels. In our trial, we had the latter, all based on baseline data collection (and all reported as categories). These included health status, age, blood type, time between COVID symptom onset, medications, quarter of enrollment, and others.

My interest here is to see how Type I error increases as the number of subgroups increases. I consider that a Type I error has occurred if *any* of the subgroups would be declared what I am calling "interesting". "Interesting" and by no means definitive, because while the results might suggest that treatment effects are stronger in a particular subgroup, we do need to be aware that these are exploratory analyses.

To explore Type I error rates, I will generate 20 categories, each of which has three levels. I can then use the model estimates from each of the subgroup analyses to evaluate how many times I would draw the wrong conclusion.

#### Generating multiple categories

To illustrate how I code all of this, I am starting with a case where there are four categories, each with three levels. In the data set, these categories are named *g1*, *g2*, *g3*, and *g4*:

```{r}
genRepeatDef <- function(nvars, prefix, formula, variance, dist, link = "identity") {
  varnames <- paste0(prefix, 1 : nvars)
  data.table(varname = varnames, 
             formula = formula, 
             variance = variance, 
             dist = dist, link = link)
}

def <- genRepeatDef(4, "g", "1/3;1/3;1/3", 0, "categorical")
def <- defData(def, "a", formula = "1;1", dist = "trtAssign")
def <- defData(def, "y", formula = "0.10", dist = "binary")

def
```

A single data set based on these definitions looks like this:

```{r}
RNGkind("L'Ecuyer-CMRG")
set.seed(67) #4386212 83861 7611291

dd <- genData(1000, def)
dd
```

The function `fitmods` estimates both the `glm` and `stan` models for a single category. The models provide subgroup estimates of treatment effects, just as the example above did:

```{r}
fitmods <-function(dx, grpvar) {
  
  # GLM
  
  dx[, grp := factor(get(grpvar))]
  fitglm <- glm(y ~ grp + a:grp - 1, data = dx, family = "binomial")
  
  pvals <- coef(summary(fitglm))[, "Pr(>|z|)"]

  lpval <- length(pvals)
  freq_res <- any(pvals[(lpval/2 + 1) : lpval] < 0.05)
  
  # Bayes
  
  dat <- listdat(dx, grpvar)
  
  fitbayes <- mod$sample(
    data = dat,
    refresh = 0,
    chains = 4L,
    parallel_chains = 4L,
    iter_warmup = 1500,
    iter_sampling = 3000
  )
  
  OR_df <- data.frame(as_draws_rvars(fitbayes$draws(variables = "OR_g")))
  bayes_res <- with(OR_df, 
    any((quantile(OR_g, .025) > 1) | (quantile(OR_g, .975) < 1)))
  
  # Return results

  list(type1_dt = data.table(var = grpvar, bayes_res, freq_res),
       OR_post = data.frame(var = grpvar, cat = paste0("c",1:3), OR_df)
  )
  
}
```

In this case, I am calling the function `fitmods` for each of the four categorical groupings *g1* through *g4*:

```{r}
res <- lapply(paste0("g", 1:4), function(a) fitmods(dd, a))
```

One of the arguments returned by `fitmods` is a data.table of summary results for each of the four categorical grouping. If any of the effect estimates for one or more of the three subgroup levels in a category was deemed "interesting" (based on a p-value < 0.05 for the `glm` model, and the value 1 falling outside the 95% credible interval for the `stan` model), then the function returned a value of 1 or TRUE. In this case, at least one of the subgroups within *g4* would have been declared interesting based on the `glm` and `stan` models; at least one of the subgroups within *g2* would have been deemed interesting, but only based on the `glm` model:

```{r}
type1_dt <- rbindlist(lapply(res, function(x) x$type1_dt))
type1_dt
```

We can see this visually for the `stan` models by looking at the density plots for each subgroup within each category:

```{r, fig.height = 5}
dc <- do.call("rbind", lapply(res, function(x) x$OR_post))

ggplot(data = dc, aes(dist = OR_g, y = cat)) +
  geom_vline(xintercept = 1, color = "grey80", size = .3) +
  stat_dist_halfeye(fill = palettes_d$awtools$a_palette[7], alpha = .8) +
  theme(panel.grid = element_blank()) +
  xlab("OR") +
  # scale_x_continuous(limits = c(0, 4)) +
  facet_wrap(~ var)
```

We calculate the first occurrence of an *interesting* subgroup by looking across *g1* through *g4*. This will be useful for figuring out the Type I error rates for different numbers of categories. In this case, the first interesting subgroup based on the `stan` model is found when there are four categories; for the `glm` model, the first interesting subgroup is with two categories.

```{r}
first_true <- sapply(type1_dt[, c(2,3)], function(x) match(TRUE, x))
first_true
```

## Simulation study results

Now, having provided you with all that background, here are the results based on 1050 simulated data sets (because I had access to 75 cores on a high performance computing cluster) with 20 categorical groups of three levels each. For each data set $r, \ r \in \{1,\dots,1050\}$, I determined the first occurrence of an "interesting" finding among the 20 categories for each model, and stored this in $F_{br}$ and $F_{fr}$ for the Bayes and frequentist models, respectively. $T_m(g)$ is the error rate for the model type $m$, $m \in \{b, f\}$ with $g$ number of categories, and

$$
T_m(g) = \frac{\sum_{r=1}^{1050} I(F_{mr} \le g)}{1050}
$$

Here is a plot of the Type I error rates calculated different numbers of categories. We can see for the frequentist model based on p-values, the error rates get quite large quite quickly, exceeding 50% by the time we reach 8 categories. In comparison, the error rates under the Bayes model with quite skeptical prior assumptions are held in check quite a bit better.

```{r, echo = FALSE, fig.height=3.5}
library(ggplot2)
library(paletteer)

load("extra/simulation.rda")
res <- data.table(res)

res[is.na(bayes_2), bayes_2 := 99]
res[is.na(bayes_ci), bayes_ci := 99]
res[is.na(freq), freq := 99]

error_rate <- function(dx, nvars) {
  dx[, .(
    bayes_2 = mean(bayes_2 <= nvars),
    bayes_ci = mean(bayes_ci <= nvars), 
    frequentist = mean(freq <= nvars))]
}

error_dt <- rbindlist(lapply(1:20, function(a) error_rate(res, a)))
error_dt[, nvars := .I]

error_dt <- melt(error_dt, 
     id.vars = "nvars",
     measure.vars = c("bayes_2","bayes_ci", "frequentist"), 
     variable.name = "method"
)

ggplot(data = error_dt[method != "bayes_2"], aes(x = nvars, y = value, group = method)) +
  geom_line(aes(color = method), size = .9) +
  scale_y_continuous(limits = c(0, 1), 
                     name = bquote( paste("type I error rate ", T[m](g) )),
                     labels = scales::percent) +
  xlab("number of categories (g)") +
  theme(panel.grid = element_blank()) +
  scale_color_manual(values = c("#2A363B", "#019875"),
                     guide = guide_legend(reverse = TRUE),
                     labels = c("bayes","frequentist")) 
  
                         
# ggplot(data = error_dt[method != "frequentist"], aes(x = nvars, y = value, group = method)) +
#   geom_line(aes(color = method), size = .9) +
#   scale_y_continuous(limits = c(0, .30), name = "type I error rate",
#                      labels = scales::percent) +
#   xlab("number of subgroups") +
#   theme(panel.grid = element_blank()) +
#   scale_color_manual(values = c("#2A363B", "#FECEA8"),
#                      labels = c("credible interval", "one-sided test"))
```


However, even for the Bayes approach, the error rate is close to 20% for 12 categories. So, we still need to be careful in drawing conclusions. In the case that we do find some potentially interesting results (which we did in the case of the CCP CONTAIN  trial), readers certainly have a right to be skeptical, but there is no reason to completely dismiss the findings out of hand. These sorts of findings suggest that more work needs to be done to better understand the nature of the treatment effects.