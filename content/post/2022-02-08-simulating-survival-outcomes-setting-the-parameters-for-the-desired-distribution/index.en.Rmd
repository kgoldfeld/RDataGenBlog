---
title: 'Simulating survival outcomes: setting the parameters for the desired distribution'
author: Package Build
date: '2022-02-08'
slug: []
categories: []
tags:
  - R
  - simulation
  - survival analysis
type: ''
subtitle: ''
image: ''
draft: TRUE
---

The package `simstudy` has some functions that facilitate generating of survival data using an underlying Weibull distribution. Originally, I added this, because I thought it would be interesting to try to do, and I figured it would be useful for me someday (and hopefully some others, as well). Well, now I am working on a project that involves evaluating at least two survival-type processes that are occurring simultaneously. To get a handle on the analytic models we might use, I've started to try to simulate a simplified version of the data that we have.

At some point, I'd like to describe the motivating simulations in more detail. But here, I want to focus more generally on the survival data generating process using `simstudy`, looking in particular at how to identify the parameters that allow the simulated data to match the desired characteristics.

## Weibull-Cox proprtional hazard data generation process

`simstudy` uses an underlying data generating process for survival outcomes that draws from a Weibull distribution and satisfies the requirements of a Cox proportional hazards model. The approach was drawn directly from this [*Bender, Augustin, and Blettner*](https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.2059){target="_blank"} paper, so head over there if you really want the details.

The times to survival $T$ are generated using three parameters, $\lambda$ (scale), $\nu$ (shape), and $f$, which is really $\mathbf{\beta^\prime x}$ from a Cox proportional hazard model. 

$$T = \left[ \frac{- \lambda \ \text{log}(u) }{\text{exp}(f)}  \right] ^ \nu$$

A single instance $T$ is "drawn" from the Weibull distribution by generating $u$ from the uniform $U(0,1)$ distribution. It will be the case that $(1-u) \%$ of the survival times $T$ will fall below the values of $T$ determined by $u$; this will be helpful later when we need to generate data with specific distributions in mind.

It turns out that we don't really need the scale parameter $\lambda$, because it can be absorbed into $f$, so in all the examples that follow, we'll set $\lambda = 1$, which leaves us with

$$T = \left[ \frac{- \text{log}(u) }{\text{exp}(f)}  \right] ^ \nu$$

Weibull distribution data generation is extremely flexible, and can provide an infinite number of distributions. Here are some examples, but first, to get things started, here are the packages needed to run all the code here:

```{r, warning=FALSE,error=FALSE, fig.height = 2, fig.width = 8}
library(simstudy)
library(data.table)
library(ggplot2)
```

The function `get_surv` generates data for the survival curve. It is deterministic in that it does not generate draws of $u$, but calculates a specific $T$ for each value of $u$ distribution evenly between 0 and 1.

```{r, warning=FALSE,error=FALSE, fig.height = 2, fig.width = 8}
get_surv <- function(f, shape, n = 100) {
  
  u <- seq(1, 0.001, length = n)
  
  dd <- data.table(
    f = f,
    shape = shape,
    T = (-(log(u)/exp(f)))^(shape),
    p = round(1 - cumsum(rep(1/length(u), length(u))), 3)
  )
  
  return(dd)

}

get_surv(-10, .3, n = 10)
```

Here are 16 arbitrary distributions using four different values of $f$ and $\nu$. Each panel represents a different value of $\nu$, ranging from 0.16 to 0.22.

```{r, warning=FALSE,error=FALSE, fig.height = 2, fig.width = 8}
f <- c(-26, -27, -28, -29)
shape <- c(0.16, .18, .20, .22)

eg <- expand.grid(f=f, shape=shape)
eg <- asplit(eg, MARGIN = 1)

l <- lapply(eg, function(x) get_surv(x[1], x[2]))
l <- rbindlist(l)

ggplot(data = l, aes(x = T, y = p)) +
  geom_line(aes(group = f, color = factor(f))) +
  ylim(0,1) +
  xlim(0, 800) +
  facet_grid ( ~ shape) +
  theme(panel.grid = element_blank(),
        axis.text = element_text(size = 7.5)) +
  scale_color_manual(
    values = c("#7D9D33", "#CED38C", "#DCC949", "#BCA888", "#CD8862", "#775B24"),
    name = "  f"
  )
 
```

## Generating a particular distribution

The interpretation of the parameters is a bit opaque. If we have covariates embedded in $f$ the coefficients do have pretty clear interpretations as hazard ratios. But the intercept term (remember, I have set $\lambda = 1$) really defines the scale. So how do we go about selecting values for $f$ and $\nu$ to get the desired distribution?

If we can reasonably characterize the desired distribution by two single points on the survival curve, the task actually becomes remarkably easy. By this, I mean we pick two time points and a probability of survival for each time point. For example, in the first scenario, we may want a distribution with 80% survival at day 180 and 20% survival at day 365. Or, in a second scenario, we may want to have 90% survival at day 180 and 40% survival at day 365. Here is an animation of how we might find the curves by adjusting $f$ and $\nu$ (see <a href="#addendum">addendum</a> for code to generate this):

```{r, echo = FALSE, warning=FALSE}
library(gganimate)

sdd <- list()

sdd[[1]] <- get_surv(-6,  1, n = 1000)
sdd[[2]] <- get_surv(-7.566, 0.783 , n = 1000)
sdd[[3]] <- get_surv(-9.981, 0.587, n = 1000)
sdd[[4]] <- get_surv(-11.137, 0.521, n = 1000)
sdd[[5]] <- get_surv(-13.814, 0.417, n = 1000)
sdd[[6]] <- get_surv(-16.014, 0.358, n = 1000)
sdd[[7]] <- get_surv(-6,  1, n = 1000)
sdd[[8]] <- get_surv(-8.125, 0.777 , n = 1000)
sdd[[9]] <- get_surv(-10.183, 0.619, n = 1000)
sdd[[10]] <- get_surv(-12.481, 0.490, n = 1000)
sdd[[11]] <- get_surv(-14.595, 0.414, n = 1000)
sdd[[12]] <- get_surv(-18.139, 0.327, n = 1000)

k <- length(sdd)

sdds <- lapply(1:k, function(x) sdd[[x]][ , c("iter", "color") := list(x, "black")])
sdds[[k/2]][, color := "green"]
sdds[[k]][, color := "green"]
sdd <- rbindlist(sdds)

targets <- data.table(iter = 1:k, days1 = rep(180, k), days2 = rep(365, k),
  p1 = rep(c(.8, .9), each = k/2), p2 = rep(c(.2, .4), each = k/2))

dt_anot <- sdd[, .SD[1,], keyby = iter]
dt_anot[iter <= (k/2), targets := 1]
dt_anot[iter > (k/2), targets := 2]
dt_anot[, targets := factor(targets, labels = c("Scenario one", "Scenario two"))]
dt_anot[, color := "black"]
dt_anot[iter == (k/2), color := "green"]
dt_anot[iter == k, color := "green"]

a <- ggplot() +
  geom_point(data = targets, aes(x = days1, y=p1), pch = 1, size = 2) +
  geom_point(data = targets, aes(x = days2, y=p2), pch = 1, size = 2) +
  geom_point(data = sdd, aes(x = T, y = p, group = p, color = color), size = .2) +
  geom_vline(xintercept = c(180, 365), lty = 1, size = .3, color = "grey70") +
  geom_text(x = 750, y = .68, size = 5.5, hjust = "left", fontface = "bold",
            aes(label = targets), data = dt_anot) +
  geom_text(x = 750, y = .6, size = 5.5, hjust = "left",
            aes(label = paste("f:", f), color = color), data = dt_anot) +
  geom_text(x = 750, y = .54, size = 5.5, hjust = "left",
            aes(label = paste("shape:", shape), color = color), data = dt_anot) +
  scale_x_continuous(limits = c(0, 1250), 
                     breaks = c(seq(0, 1250, by = 250), 180, 365), name = "time") +
  scale_color_manual(values = c("black","#7D9D33")) +
  scale_y_continuous(limits = c(0.05, 0.995), 
                     breaks = c(0.2, 0.4, 0.8, 0.9), name = "probability of survival") +
  theme(panel.grid = element_blank(),
        legend.position = "none") +
  transition_states(iter, state_length = 1, transition_length = 1) 

animate(a, duration = 24, fps = 10, height = 350, width = 550)

```

### Determining the parameter values

If we have two points in mind, there is an extremely simple analytic solution that can derive $f$ and $\nu$ so that we can identify a Weibull-Cox based survival curve that is guaranteed to pass through both points.

For any time point $T$, we have from the equation above

\begin{aligned}
log(T) &= \nu \ \text{log} \left[ \frac{-\text{log}(u)}{exp(f)}  \right] \\ 
\\ 
&= \nu \ \left ( \text{log}\left[-\text{log}(u)\right] - \text{log}\left[ \text{exp}(f) \right] \right ) \\
\\
&= \nu \ (\text{log}\left[-\text{log}(u)\right] - f)
\end{aligned}

If we have desired points $(T_1,u_1)$ and $(T_2,u_2)$, then we write can a simple system of two equations with two unknowns, $f^*$ and $\nu^*$, the target parameters:

$$ \text{log}(T_1) = \nu \ (\text{log} \left[ -\text{log}(u_1)\right] - f)$$

$$ \text{log}(T_2) = \nu \ (\text{log} \left[ -\text{log}(u_2)\right] - f)$$

Using simple algebra, we can rearrange terms to provide solutions for $f$ and $\nu$ (I'll spare you the extra steps):

$$\nu^* = \frac{\text{log}(T_2) - \text{log}(T_1)}{\text{log}(-\text{log}(u_2)) - \text{log}(-\text{log}(u_1))}$$

$$f^* = \text{log}(-\text{log}(u_1)) - \nu^{*^{-1}} \text{log}(T_1)$$

### Scenario three

If we want a curve that represents a new scenario, where there is 95% survival at day 180 and 40% survival at the end of 2 years, the desired parameters are 

$$\nu^* = \frac{\text{log}(365*2) - \text{log}(180)}{\text{log}(-\text{log}(0.40)) - \text{log}(-\text{log}(0.95))} = \frac{1.400}{2.883} = 0.486$$

$$f^* = \text{log}(-\text{log}(0.95)) - \frac{1}{0.486} \text{log}(180) = -2.970 - 10.685 = -13.655$$

<br>

We can generate all the points along these curve and then plot them:

```{r, warning=FALSE, error=FALSE, fig.height = 3.5, fig.width = 5.5}
dsurv <- get_surv(-13.655, 0.486, n = 1000)
dsurv
```

```{r, echo=FALSE, warning=FALSE,error=FALSE, fig.height = 3.5, fig.width = 5.5}
ggplot(data = dsurv, aes(x = T, y = p)) +
  geom_line(size = 0.8) +
  geom_point(x = 180, y = 0.95, pch = 21, fill = "#DCC949", size = 2.5) +
  geom_point(x = 365*2, y = 0.4, pch = 21, fill = "#DCC949", size = 2.5) + 
  scale_y_continuous(limits = c(0,1), breaks = c(0,0.4, 0.95),
                     name = "probability of survival") +
  scale_x_continuous(limits = c(0, 1250), 
                     breaks = c(0, 180, 500, 365*2, 1000, 1250), name = "time") +
  theme(panel.grid = element_blank(),
    axis.text = element_text(size = 7.5),
    axis.title = element_text(size = 8, face = "bold")
  )
```

Of course, what we really want to do is sample from a distribution defined by these parameters in order to conduct simulation experiments, not just generate and look at deterministic functions. This would include adding covariates and possibly censoring. But all of that remains for another day.

<p><small><font color="darkkhaki">
Reference:

Bender, Ralf, Thomas Augustin, and Maria Blettner. "Generating survival times to simulate Cox proportional hazards models." Statistics in medicine 24, no. 11 (2005): 1713-1723.

</font></small></p>


<a name="addendum"></a>  

\ 


## Addendum

Here is the code that is used to generate the animated plot, which done with the `gganimate` package: 

```{r, eval = FALSE, warning=FALSE}
library(gganimate)

sdd <- list()

sdd[[1]] <- get_surv(-6,  1, n = 1000)
sdd[[2]] <- get_surv(-7.566, 0.783 , n = 1000)
sdd[[3]] <- get_surv(-9.981, 0.587, n = 1000)
sdd[[4]] <- get_surv(-11.137, 0.521, n = 1000)
sdd[[5]] <- get_surv(-13.814, 0.417, n = 1000)
sdd[[6]] <- get_surv(-16.014, 0.358, n = 1000)
sdd[[7]] <- get_surv(-6,  1, n = 1000)
sdd[[8]] <- get_surv(-8.125, 0.777 , n = 1000)
sdd[[9]] <- get_surv(-10.183, 0.619, n = 1000)
sdd[[10]] <- get_surv(-12.481, 0.490, n = 1000)
sdd[[11]] <- get_surv(-14.595, 0.414, n = 1000)
sdd[[12]] <- get_surv(-18.139, 0.327, n = 1000)

k <- length(sdd)

sdds <- lapply(1:k, function(x) sdd[[x]][ , c("iter", "color") := list(x, "black")])
sdds[[k/2]][, color := "green"]
sdds[[k]][, color := "green"]
sdd <- rbindlist(sdds)

targets <- data.table(iter = 1:k, days1 = rep(180, k), days2 = rep(365, k),
  p1 = rep(c(.8, .9), each = k/2), p2 = rep(c(.2, .4), each = k/2))

dt_anot <- sdd[, .SD[1,], keyby = iter]
dt_anot[iter <= (k/2), targets := 1]
dt_anot[iter > (k/2), targets := 2]
dt_anot[, targets := factor(targets, labels = c("Scenario one", "Scenario two"))]
dt_anot[, color := "black"]
dt_anot[iter == (k/2), color := "green"]
dt_anot[iter == k, color := "green"]

a <- ggplot() +
  geom_point(data = targets, aes(x = days1, y=p1), pch = 1, size = 2) +
  geom_point(data = targets, aes(x = days2, y=p2), pch = 1, size = 2) +
  geom_point(data = sdd, aes(x = T, y = p, group = p, color = color), size = .2) +
  geom_vline(xintercept = c(180, 365), lty = 1, size = .3, color = "grey70") +
  geom_text(x = 750, y = .68, size = 5.5, hjust = "left", fontface = "bold",
            aes(label = targets), data = dt_anot) +
  geom_text(x = 750, y = .6, size = 5.5, hjust = "left",
            aes(label = paste("f:", f), color = color), data = dt_anot) +
  geom_text(x = 750, y = .54, size = 5.5, hjust = "left",
            aes(label = paste("shape:", shape), color = color), data = dt_anot) +
  scale_x_continuous(limits = c(0, 1250), 
                     breaks = c(seq(0, 1250, by = 250), 180, 365), name = "time") +
  scale_color_manual(values = c("black","#7D9D33")) +
  scale_y_continuous(limits = c(0.05, 0.995), 
                     breaks = c(0.2, 0.4, 0.6, 0.8), name = "probability of survival") +
  theme(panel.grid = element_blank(),
        legend.position = "none") +
  transition_states(iter, state_length = 1, transition_length = 1) 

animate(a, duration = 24, fps = 10, height = 350, width = 550)

```

***A note on where the parameters for the animation came from***

You may be wondering where the parameters used in the animation come from. I really wanted to generate a series of curves that started at a fair distance from the true value and converged to the right spot. My idea was to use a simple loss function that involved the unknown parameters $f$ and $\nu$, which would be optimized (in this case minimized) at the correct values (the same as those derived above). If I could recover the interim values of optimization algorithm, those would provide a sequence of parameters and curves the converge on the true values.

The loss function is a simple squared loss, which is the sum of the squared loss for both points that define the curve:

$$\left[ \hat{\nu} \ (\text{log} \left[ -\text{log}(u_1)\right] - \hat{f}) - \text{log}(T_1) \right]^2 + \left[ \hat{\nu} \ (\text{log} \left[ -\text{log}(u_2)\right] - \hat{f}) - \text{log}(T_2) \right]^2$$

This is implemented as function `fx`, which is to be optimized using function `optim`. I know this may not be the best optimization option in R, but given that this is quite a simple problem, it should suffice. In the function $x[1]$ represents $f$, and $x[2]$ represents $\nu$.

```{r}
fx <- function(x, p, times) {
  (x[2]*(log(-log(p[1])) - x[1]) - log(times[1])) ^ 2 + 
  (x[2]*(log(-log(p[2])) - x[1]) - log(times[2])) ^ 2 
}
```

The optimization provides starting values for $f$ and $\nu$. I chose values of $f$ and $\nu$ that would locate the initial curve between the two target points. $\nu$ is constrained to be non-negative (and $f$ is unconstrained). The key here is that the *trace* option is set so that interim values of $f$ and $\nu$ are reported. I am not showing the full output here, because it is quite lengthy. I only used four interim values (plus the starting and ending values) to create the animation. The final output includes the values of $f$ and $\nu$ that optimize the quadratic loss:

```{r, eval = FALSE}
optim(
  par = c(-(log((180+365)/2) - log(-log(.5))), 1), 
  fn = fx, 
  p = c(.8, .2), 
  times = c(180, 365),
  method = "L-BFGS-B", 
  lower = c(-Inf, 0),
  upper = c(Inf, Inf),
  control= list(trace = 5)
)
```

```{r, echo = FALSE}
optim(
  par = c(-(log((180+365)/2) - log(-log(.5))), 1), 
  fn = fx, 
  p = c(.8, .2), 
  times = c(180, 365),
  method = "L-BFGS-B", 
  lower = c(-Inf, 0),
  upper = c(Inf, Inf)
)
```

Incidentally, our analytic formulas give us 

$$\nu^* = \frac{\text{log}(365) - \text{log}(180)}{\text{log}(-\text{log}(0.2)) - \text{log}(-\text{log}(0.8))} = 0.3577951$$

$$f^* = \text{log}(-\text{log}(0.8)) - \frac{1}{0.3577951} \text{log}(180) =-16.01371$$