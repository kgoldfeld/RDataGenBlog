---
title: A Bayesian proportional hazards model with a penalized spline
author: Package Build
date: '2025-03-04'
slug: []
categories: []
tags:
  - R
  - survival analysis
  - Bayesian analysis
type: ''
subtitle: ''
image: ''
---

In my previous [post](https://www.rdatagen.net/post/2025-02-11-estimating-a-bayesian-proportional-hazards-model/){target="_blank"},  I outlined a Bayesian approach to proportional hazards modeling. This post serves as an addendum, providing code to incorporate a spline to model a time-varying hazard ratio non linearly. In a second addendum to come I will present a separate model with a site-specific random effect, essential for a cluster-randomized trial. These components lay the groundwork for analyzing a stepped-wedge cluster-randomized trial, where both splines and site-specific random effects will be integrated into a single model. I plan on describing this comprehensive model in a final post.

### Simulating data with a time-varying hazard ratio

Here are the `R` packages used in the post:

```{r, message=FALSE}
library(simstudy)
library(ggplot2)
library(data.table)
library(survival)
library(survminer)
library(splines)
library(splines2)
library(cmdstanr)
```

The dataset simulates a randomized controlled trial in which patients are assigned either to the treatment group ($A=1$) or control group ($A=0$) in a $1:1$ ratio. Patients enroll over nine quarters, with the enrollment quarter denoted by $M$, $M \in \{0, \dots, 8 \}$. The time-to-event outcome, $Y$, depends  on both treatment assignment and enrollment quarter. To introduce non-linearity, I define the relationship using a cubic function, with true parameters specified as follows:

```{r}
defI <- 
  defData(varname = "A", formula = "1;1", dist = "trtAssign") |>
  defData(varname = "M", formula = "0;8", dist = "uniformInt")

defS <-
  defSurv(
    varname = "eventTime",
    formula = "..int + ..beta * A + ..alpha_1 * M + ..alpha_2 * M^2 + ..alpha_3 * M^3",
    shape = 0.30)  |>
  defSurv(varname = "censorTime", formula = -11.3, shape = 0.40)

# parameters

int <- -11.6      
beta <-  0.70
alpha_1 <-  0.10   
alpha_2 <-  0.40    
alpha_3 <- -0.05
```

I've generated a single data set of $640$ study participants, $320$ in each arm. The plot below shows the Kaplan-Meier curves by arm for each enrollment period.

```{r}
set.seed(7368) # 7362

dd <- genData(640, defI)
dd <- genSurv(dd, defS, timeName = "Y", censorName = "censorTime",
  eventName = "event", typeName = "eventType", keepEvents = TRUE)
```

```{r surv_plots, echo=FALSE, fig.width = 3, fig.height = 7}

dd_surv <- survfit(Surv(Y, event) ~ A + M, data = dd)
dd_surv_tidy <- data.table(surv_summary(dd_surv, data = dd))
dd_censor <- dd_surv_tidy[n.censor > 0, ]

ggplot(dd_surv_tidy, aes(time, surv, color = A)) +
  geom_step() +  # Kaplan-Meier curves
  geom_point(data = dd_censor, aes(time, surv), shape = 3, size = 1, color = "black")  +
  labs(x = "Time to event", y = "Probability of no event") +
  theme(panel.grid = element_blank(), legend.position = "none") +
  facet_grid(M ~ .) +
  scale_y_continuous(limits = c(0, 1), breaks = c(.25, .75))
```

### Bayesian model

This Bayesian proportional hazards model builds directly on the approach from my previous [post](https://www.rdatagen.net/post/2025-02-11-estimating-a-bayesian-proportional-hazards-model/){target="_blank"}. Since the effect of $M$ on $Y$ follows a non-linear pattern, I model this relationship using a spline to account for temporal variation in the hazard. The partial likelihood is a function of the treatment effect and spline basis function coefficients, given by:

$$
L(\beta,\mathbf{\gamma}) = \prod_{i=1}^{N} \left( \frac{\exp \left(\beta A_i + \sum_{m=1} ^ M \gamma_m X_{m_i} \right)} {\sum_{j \in R(t_i)} \exp\left(\beta A_j + \sum_{m=1} ^ M \gamma_m X_{m_j}\right) } \right)^{\delta_i}
$$
where:

  * $M$: number of spline basis functions
  * $N$: number of observations (censored or not)
  * $A_i$: binary indicator for treatment
  * $X_{m_i}$: value of the $m^{\text{th}}$ spline basis function for the $i^{\text{th}}$ observation
  * $\delta_i$: event indicator ($\delta_i = 1$ if the event occurred, $\delta_i = 0$ if censored)
  * $\beta$: treatment coefficient
  * $\gamma_m$: spline coefficient for the $m^\text{th}$ spline basis function
  * $R(t_i)$: risk set at time $t_i$ (including only individuals censored *after* $t_i$)

The spline component of the model is adapted from a model I [described](https://www.rdatagen.net/post/2024-10-08-can-chatgpt-help-construct-non-trivial-bayesian-models-with-cluster-specific-splines/){target="_blank"} last year. In this formulation, time-to-event is modeled as a function of the vector $\mathbf{X_i}$ rather than the period itself. The number of basis functions is determined by the number of knots, with each segment of the curve estimated using B-spline basis functions. To minimize overfitting, we include a penalization term based on the second derivative of the B-spline basis functions. The strength of this penalization is controlled by a tuning parameter, $\lambda$, which is provided to the model.

The Stan code, provided in full here, was explained in earlier posts. The principal difference from the previous post is the addition of the spline-related data and parameters, as well as the penalization term in the model.:

```{r}
stan_code <-
"
functions {

  // Binary search optimized to return the last index with the target value

  int binary_search(vector v, real tar_val) {
    int low = 1;
    int high = num_elements(v);
    int result = -1;

    while (low <= high) {
      int mid = (low + high) %/% 2;
      if (v[mid] == tar_val) {
        result = mid; // Store the index
        high = mid - 1; // Look for earlier occurrences
      } else if (v[mid] < tar_val) {
        low = mid + 1;
      } else {
        high = mid - 1;
      }
    }
    return result;
  }
}

data {

  int<lower=0> K;          // Number of covariates
  int<lower=0> N_o;        // Number of uncensored observations
  vector[N_o] t_o;         // Event times (sorted in decreasing order)

  int<lower=0> N;          // Number of total observations
  vector[N] t;             // Individual times (sorted in decreasing order)
  matrix[N, K] x;          // Covariates for all observations

  // Spline-related data
  
  int<lower=1> Q;          // Number of basis functions
  matrix[N, Q] B;          // Spline basis matrix
  matrix[N, Q] D2_spline;  // 2nd derivative for penalization
  real lambda;             // penalization term
}

parameters {
  vector[K] beta;          // Fixed effects for covariates
  vector[Q] gamma;         // Spline coefficients
}

model {
  
  // Prior
  
  beta ~ normal(0, 4);
  
  // Spline coefficients prior
  
  gamma ~ normal(0, 4);
  
  // Penalization term for spline second derivative
  
  target += -lambda * sum(square(D2_spline * gamma));
  
  // Calculate theta for each observation to be used in likelihood
  
  vector[N] theta;
  vector[N] log_sum_exp_theta;
  
  for (i in 1:N) {
    theta[i] = dot_product(x[i], beta) + dot_product(B[i], gamma);  
  }
  
  // Compute cumulative sum of log(exp(theta)) from last to first observation
  
  log_sum_exp_theta[N] = theta[N];
  
  for (i in tail(sort_indices_desc(t), N-1)) {
    log_sum_exp_theta[i] = log_sum_exp(theta[i], log_sum_exp_theta[i + 1]);
  }

  // Likelihood for uncensored observations
  
  for (n_o in 1:N_o) {
    int start_risk = binary_search(t, t_o[n_o]); // Use binary search
    
    real log_denom = log_sum_exp_theta[start_risk];
    target += theta[start_risk] - log_denom;
  }
}
"
```

To estimate the model, we need to get the data ready to pass to `Stan`, compile the `Stan` code, and then sample from the model using `cmdstanr`:

```{r}
dx <- copy(dd)
setorder(dx, Y)

dx.obs <- dx[event == 1]
N_obs <- dx.obs[, .N]
t_obs <- dx.obs[, Y]

N_all <- dx[, .N]
t_all <- dx[, Y]
x_all <- data.frame(dx[, .(A)])

# Spline-related info

n_knots <- 5
spline_degree <- 3
knot_dist <- 1/(n_knots + 1)
probs <- seq(knot_dist, 1 - knot_dist, by = knot_dist)
knots <- quantile(dx$M, probs = probs)
spline_basis <- bs(dx$M, knots = knots, degree = spline_degree, intercept = TRUE)
B <- as.matrix(spline_basis)

D2 <- dbs(dx$M, knots = knots, degree = spline_degree, derivs = 2, intercept = TRUE)
D2_spline <- as.matrix(D2)

K <- ncol(x_all)             # num covariates - in this case just A

stan_data <- list(
  K = K,
  N_o = N_obs,
  t_o = t_obs,
  N = N_all,
  t = t_all,
  x = x_all,
  Q = ncol(B),
  B = B,
  D2_spline = D2_spline,
  lambda = 0.10
)

# compiling code

stan_model <- cmdstan_model(write_stan_file(stan_code))

# sampling from model

fit <- stan_model$sample(
  data = stan_data,
  iter_warmup = 1000,
  iter_sampling = 4000,
  chains = 4,
  parallel_chains = 4,
  max_treedepth = 15,
  refresh = 0
)
```

The posterior mean (and median) for $\beta$, the treatment effect, are quite close to the "true" value of 0.70:

```{r}
fit$summary(variables = c("beta", "gamma"))
```

The figure below shows the estimated spline and the 95% credible interval. The green line represents the posterior median log hazard ratio for each period (relative to the middle period, 4), with the shaded band indicating the corresponding credible interval. The purple points represent the log hazard ratios implied by the data generation process. For example, the log hazard ratio comparing period 1 to period 4 for both arms is:

$$
\begin{array}{c}
(-11.6 + 0.70A +0.10\times1 + 0.40 \times 1^2 -0.05\times1^3) - (-11.6 + 0.70A +0.10\times4 + 0.40 \times 4^2 -0.05\times4^3) = \\
(0.10 + 0.40 - 0.05) - (0.10 \times 4 + 0.40 \times 16 - 0.05 \times 64 ) = \\
0.45 - 3.60 = -3.15
\end{array}
$$

It appears that the median posterior aligns quite well with the values used in the data generation process:

```{r lHRplot, echo=FALSE, message=FALSE, fig.width = 7.5, fig.height = 4.5}
library(posterior)

M_values <- seq(min(dx$M), max(dx$M))

# Construct the same B-spline basis used in the model

B_pred <- bs(M_values, knots = knots, degree = spline_degree, intercept = TRUE)
B_pred <- as.matrix(B_pred)

# Extract posterior samples of gamma from Stan

posterior_samples <- as_draws_matrix(fit$draws("gamma"))
gamma_samples <- t(posterior_samples)  # Now Q x S

# Define reference value (e.g., median M)

M_ref <- median(dx$M)
B_ref <- B_pred[which(M_values == M_ref), , drop = FALSE]  # Extract matching row

# Extract posterior samples of gamma from the fitted model

posterior_samples <- as_draws_matrix(fit$draws("gamma"))  # S x Q matrix
gamma_samples <- t(posterior_samples)  # Q x S

# Compute posterior samples of HR for each M relative to M_ref

HR_samples <- exp(B_pred %*% gamma_samples - drop(B_ref %*% gamma_samples))
HR_samples <- B_pred %*% gamma_samples - drop(B_ref %*% gamma_samples)

# Summarize posterior distributions (median and 95% credible interval)

HR_summary <- apply(HR_samples, 1, function(x) {
  c(median = median(x),
    lower = quantile(x, 0.025),
    upper = quantile(x, 0.975))
})

# Convert to data frame for plotting

HR_df <- data.frame(
  M = M_values,
  HR = HR_summary["median", ],
  HR_lower = HR_summary["lower.2.5%", ],
  HR_upper = HR_summary["upper.97.5%", ]
)

d_truth <- data.table(
  M = M_values, 
  trueLHR = -3.6 + 0.10 * M_values + 0.40 * (M_values^2) - 0.05 * (M_values^3)
)

# Plot HR estimates

ggplot(HR_df, aes(x = M, y = HR)) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "grey70") +
  geom_line(color = "#8ab128", linewidth = 1) +
  geom_ribbon(aes(ymin = HR_lower, ymax = HR_upper), alpha = 0.2, fill = "#8ab128") +
  geom_point(data = d_truth, aes(y = trueLHR), shape = 19, size = 2, color="#4f28b1") +
  labs(x = "M", y = "Log HR (relative to M = 4)", 
       title = "Log Hazard Ratio Estimates with 95% Credible Intervals") +
  theme(panel.grid = element_blank(),
        plot.title = element_text(size = 10, face = "bold")) +  
  scale_x_continuous(breaks = c(0:8))
```

For the next post, I will present another scenario that includes random effects for a cluster randomized trial (but will not include splines).