---
title: Analysing an open cohort stepped-wedge clustered trial with repeated individual binary outcomes
author: ''
date: '2020-02-04'
slug: analyzing-the-open-cohort-stepped-wedge-trial-with-binary-outcomes
categories: []
tags:
  - R
  - Stan
  - Bayesian model
subtitle: ''
---



<p>I am currently wrestling with how to analyze data from a stepped-wedge designed cluster randomized trial. A few factors make this analysis particularly interesting. First, we want to allow for the possibility that between-period site-level correlation will decrease (or decay) over time. Second, there is possibly additional clustering at the patient level since individual outcomes will be measured repeatedly over time. And third, given that these outcomes are binary, there are no obvious software tools that can handle generalized linear models with this particular variance structure we want to model. (If I have missed something obvious with respect to modeling options, please let me know.)</p>
<p>I <a href="https://www.rdatagen.net/post/simulating-an-open-cohort-stepped-wedge-trial/">introduced</a> the project that is motivating all of this, the HAS-QOL study, when I provided simulations of the data generating process we are assuming for this analysis. And before that, I described (<a href="https://www.rdatagen.net/post/estimating-treatment-effects-and-iccs-for-stepped-wedge-designs/">here</a> and <a href="https://www.rdatagen.net/post/bayes-model-to-estimate-stepped-wedge-trial-with-non-trivial-icc-structure/">here</a>) Bayesian models that can be used to analyze data with more complicated variance patterns, though all of those examples were based on continuous outcomes. Here, I am extending and combining those ideas to present an approach to estimating treatment effects in the context of HAS-QOL.</p>
<p>This post walks through the data generation process and describes a Bayesian model that addresses the challenges posed by the open cohort stepped-wedge study design.</p>
<div id="the-model" class="section level3">
<h3>The model</h3>
<p>The process I use to simulate the data and then estimate to effects is based on a relatively straightforward logistic regression model with two random effects. To simplify things a bit, I intentionally make the assumption that there are no general time trends that affect that outcomes (though it would not be difficult to add in). In the logistic model, the log-odds (or logit) of a binary outcome is a linear function of predictors and random effects:</p>
<p><span class="math display">\[ \text{logit}(P(y_{ict}=1) = \beta_0 + \beta_1 I_{ct} + b_{ct} + b_i,\]</span>
where <span class="math inline">\(\text{logit}(P(y_{ict}=1))\)</span> is the log-odds for individual <span class="math inline">\(i\)</span> in cluster (or site) <span class="math inline">\(c\)</span> during time period <span class="math inline">\(t\)</span>, and <span class="math inline">\(I_{ct}\)</span> is a treatment indicator for cluster <span class="math inline">\(c\)</span> during period <span class="math inline">\(t\)</span>.</p>
<p>There are two random effects in this model. The first is a cluster-specific period random effect, <span class="math inline">\(b_{ct}\)</span> . For each cluster, there will actually be a vector of cluster effects <span class="math inline">\(\mathbf{b_c} = (b_{c0}, b_{c1},...,b_{c,T-1})\)</span>, where <span class="math inline">\(\mathbf{b_c}\sim MVN(\mathbf{0}, \sigma_{b_c}^2\mathbf{R})\)</span>, and <span class="math inline">\(\mathbf{R}\)</span> is</p>
<p><span class="math display">\[
\mathbf{R} =
\left(
\begin{matrix}
1    &amp; \rho  &amp; \rho^2  &amp; \cdots &amp; \rho^{T-1} \\
\rho &amp; 1     &amp; \rho   &amp; \cdots &amp;  \rho^{T-2} \\
\rho^2 &amp;  \rho     &amp; 1   &amp; \cdots &amp;  \rho^{T-3} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
\rho^{T-1} &amp; \rho^{T-2} &amp;  \rho^{T-3} &amp; \cdots &amp; 1
\end{matrix}
\right )
\]</span></p>
<p>The second random effect is the individual or patient-level random intercept <span class="math inline">\(b_i\)</span>, where <span class="math inline">\(b_i \sim N(0,\sigma_{b_i}^2)\)</span>. We could assume a more structured relationship for individual patients over time (such as a decaying correlation), but in this application, patients will not have sufficient measurements to properly estimate this.</p>
<p>In the model <span class="math inline">\(\beta_0\)</span> has the interpretation of the log-odds for the outcome when the the cluster is still in the control state and the cluster-period and individual effects are both 0. <span class="math inline">\(\beta_1\)</span> is the average treatment effect conditional on the random effects, and is reported as a log odds ratio.</p>
</div>
<div id="simulating-the-study-data" class="section level3">
<h3>Simulating the study data</h3>
<p>I am going to generate a single data set based on this model. If you want more explanation of the code, this <a href="https://www.rdatagen.net/post/simulating-an-open-cohort-stepped-wedge-trial/">earlier post</a> provides the details. The only real difference here is that I am generating an outcome that is a function of cluster-period effects, individual effects, and treatment status.</p>
<div id="site-level-data" class="section level4">
<h4>Site level data</h4>
<p>There will be 24 sites followed for 12 periods (<span class="math inline">\(t=0\)</span> through <span class="math inline">\(t=11\)</span>), and the stepped-wedge design includes 6 waves of 4 sites in each wave. The first wave will start at <span class="math inline">\(t=4\)</span>, and a new wave will be added each period, so that the last wave starts at <span class="math inline">\(t=9\)</span>.</p>
<pre class="r"><code>library(simstudy)

dsite &lt;- genData(24, id = &quot;site&quot;)

dper &lt;- addPeriods(dsite, nPeriods = 12, idvars = &quot;site&quot;, 
                   perName = &quot;period&quot;)
  
dsw &lt;- trtStepWedge(dper, &quot;site&quot;, nWaves = 6, lenWaves = 1, 
                    startPer = 4, perName = &quot;period&quot;,
                    grpName = &quot;Ict&quot;)</code></pre>
<p> </p>
</div>
<div id="correlated-site-level-effects" class="section level4">
<h4>Correlated site-level effects</h4>
<p>The average site-level effect is 0, the standard deviation of site averages is <span class="math inline">\(\sigma_{ct} = 0.3\)</span>, and the correlation coefficient that will determine between-period within site correlation is <span class="math inline">\(\rho = 0.5\)</span>. The correlation structure is “AR-1”, which means the between-period correlation decays over time (see definition of <span class="math inline">\(\mathbf{R}\)</span> above.)</p>
<pre class="r"><code>siteDef &lt;- defData(varname = &quot;eff.mu&quot;, formula = 0, 
                   dist = &quot;nonrandom&quot;, id = &quot;site&quot;)
siteDef &lt;- defData(siteDef, varname = &quot;eff.s2&quot;, formula = 0.3^2, 
                   dist = &quot;nonrandom&quot;)

dsw &lt;- addColumns(siteDef, dsw)

dsw &lt;- addCorGen(dsw, nvars = 12, idvar = &quot;site&quot;, rho = 0.5, 
                 corstr = &quot;ar1&quot;, dist = &quot;normal&quot;, 
                 param1 = &quot;eff.mu&quot;, param2 = &quot;eff.s2&quot;, 
                 cnames = &quot;eff.st&quot;)

dsw &lt;- dsw[, .(site, period, startTrt, Ict, eff.st)]</code></pre>
<p> </p>
</div>
<div id="patient-level-data" class="section level4">
<h4>Patient level data</h4>
<p>We are generating 20 patients per period for each site, so there will be a total of 5760 individuals (<span class="math inline">\(20\times24\times12\)</span>). The individual level effect standard deviation <span class="math inline">\(\sigma_{b_i} = 0.3\)</span>. Each of the patients will be followed until they die, which is a function of their health status over time, defined by the Markov process and its transition matrix defined below. (This was described in more detail in an <a href="https://www.rdatagen.net/post/simulating-an-open-cohort-stepped-wedge-trial/">earlier post</a>.</p>
<pre class="r"><code>dpat &lt;- genCluster(dper, cLevelVar = &quot;timeID&quot;, 
                   numIndsVar = 20, level1ID = &quot;id&quot;)

patDef &lt;- defDataAdd(varname = &quot;S0&quot;, formula = &quot;0.4;0.4;0.2&quot;,
                     dist = &quot;categorical&quot;)
patDef &lt;- defDataAdd(patDef, varname = &quot;eff.p&quot;, 
                     formula = 0, variance = 0.3^2)

dpat &lt;- addColumns(patDef, dpat)

P &lt;-t(matrix(c( 0.7, 0.2, 0.1, 0.0,
                0.1, 0.3, 0.5, 0.1,
                0.0, 0.1, 0.6, 0.3,
                0.0, 0.0, 0.0, 1.0),
             nrow = 4))

dpat &lt;- addMarkov(dpat, transMat = P, 
                  chainLen = 12, id = &quot;id&quot;, 
                  pername = &quot;seq&quot;, start0lab = &quot;S0&quot;,
                  trimvalue = 4)

dpat[, period := period + seq - 1]
dpat &lt;- dpat[period &lt; 12]</code></pre>
<p> </p>
</div>
<div id="individual-outcomes" class="section level4">
<h4>Individual outcomes</h4>
<p>In this last step, the binary outcome <span class="math inline">\(y_{ict}\)</span> is generated based on treatment status and random effects. In this case, the treatment lowers the probability of <span class="math inline">\(Y=1\)</span>.</p>
<pre class="r"><code>dx &lt;- merge(dpat, dsw, by = c(&quot;site&quot;,&quot;period&quot;))
setkey(dx, id, period)

outDef &lt;- defDataAdd(varname = &quot;y&quot;, 
                     formula = &quot;-0.5 - 0.8*Ict + eff.st + eff.p&quot;,
                     dist = &quot;binary&quot;, link = &quot;logit&quot;)

dx &lt;- addColumns(outDef, dx)
dx &lt;- dx[, .(site, period, id, Ict, y)]</code></pre>
<p>Here are the site-level averages over time. The light blue indicates periods in which a site is still in the control condition, and the dark blue shows the transition to the intervention condition. The lines, which are grouped by wave starting period, show the proportion of <span class="math inline">\(Y=1\)</span> for each period. You should be able to see the slight drop following entry into treatment.</p>
<p><img src="/img/post-opensw/siteplot.png" height="400" /></p>
</div>
</div>
<div id="estimating-the-treatment-effect-and-variance-components" class="section level3">
<h3>Estimating the treatment effect and variance components</h3>
<p>Because none of the maximum likelihood methods implemented in <code>R</code> or <code>SAS</code> could estimate this specific variance structure using a mixed effects logistic regression model, I am fitting a Bayesian model using <a href="http://mc-stan.org">RStan and Stan</a>, which requires a set of model definitions.</p>
<p>This model specification is actually quite similar to the model I estimated <a href="https://www.rdatagen.net/post/bayes-model-to-estimate-stepped-wedge-trial-with-non-trivial-icc-structure/">earlier</a>, except of course the outcome distribution is logistic rather than continuous. Another major change is the use of a <a href="https://mc-stan.org/docs/2_21/stan-users-guide/reparameterization-section.html">“non-centered” parameterization</a>, which actually reduced estimation times from hours to minutes (more precisely, about 12 hours to about 30 minutes). This reparameterization requires a Cholesky decomposition of the variance-covariance matrix <span class="math inline">\(\Sigma\)</span>. One additional limitation is that proper convergence of the MCMC chains seems to require a limited prior on <span class="math inline">\(\rho\)</span>, so that <span class="math inline">\(\rho \sim U(0,1)\)</span> rather than <span class="math inline">\(\rho \sim U(-1,1)\)</span>.</p>
<p>This particular code needs to be saved externally, and I have created a file named <code>binary sw - ar ind effect - non-central.stan</code>. This file is subsequently referenced in the call to <code>RStan</code>.</p>
<pre class="stan"><code>data {
  int&lt;lower=1&gt; I;              // number of unique individuals
  int&lt;lower=1&gt; N;              // number of records
  int&lt;lower=1&gt; K;              // number of predictors
  int&lt;lower=1&gt; J;              // number of sites
  int&lt;lower=0&gt; T;              // number of periods
  int&lt;lower=1,upper=I&gt; ii[N];  // id for individual
  int&lt;lower=1,upper=J&gt; jj[N];  // group for individual
  int&lt;lower=1,upper=T&gt; tt[N];  // period of indidvidual
  matrix[N, K] x;              // matrix of predictors
  int&lt;lower=0,upper=1&gt; y[N];   // vector of binary outcomes
}

parameters {
  vector[K] beta;              // model fixed effects
  real&lt;lower=0&gt; sigma_S;       // site variance (sd)
  real&lt;lower=0,upper=1&gt; rho;   // correlation
  real&lt;lower=0&gt; sigma_I;       // individual level varianc (sd)
  
  // non-centered paramerization
  
  vector[T] z_ran_S[J];   // site level random effects (by period)
  vector[I] z_ran_I;      // individual level random effects        
}

transformed parameters {

  cov_matrix[T] Sigma;
  matrix[T, T] L;         // for non-central parameterization
  vector[I] ran_I;        // individual level random effects
  vector[T] ran_S[J];     // site level random effects (by period)
  vector[N] yloghat;

  // Random effects with exchangeable correlation

  real sigma_S2 = sigma_S^2;

  for (j in 1:T)
    for (k in 1:T)
      Sigma[j,k] = sigma_S2 * pow(rho,abs(j-k));
  
  // for non-centered parameterization
  
  L = cholesky_decompose(Sigma);

  for(j in 1:J)
    ran_S[j] = L * z_ran_S[j];
    
  ran_I = sigma_I * z_ran_I;
  
  // defining mean on log-odds scale

  for (i in 1:N)
      yloghat[i] = x[i]*beta + ran_S[jj[i], tt[i]] + ran_I[ii[i]];
      
}

model {
  
  sigma_I ~ exponential(0.25);
  sigma_S ~ exponential(0.25);
  
  rho ~ uniform(0, 1);
  
  for(j in 1:J) {
    z_ran_S[j] ~ std_normal();
  }

  z_ran_I ~ std_normal();
  
  y ~ bernoulli_logit(yloghat);

}</code></pre>
</div>
<div id="set-up-the-data-and-call-stan-from-r" class="section level3">
<h3>Set up the data and call stan from R</h3>
<p>Just for completeness, I am providing the code that shows the interface between <code>R</code> and <code>Stan</code> using <code>RStan</code>. The data needs to be sent to Stan as a list of data elements, which here is called <code>testdat</code>. For the estimation of the posterior probabilities, I am specifying 4 chains of 4000 iterations each, which includes 1000 warm-up iterations. I specified “adapt_delta = 0.90” to reduce the step-size a bit (default is 0.80); this slows things down a bit, but improves stability.</p>
<p>As I mentioned earlier, with this data set (and rather large number of effects to estimate), the running time is between 30 and 45 minutes. One of the downsides of this particular Bayesian approach is that it wouldn’t really be practical to do any kind of sample size estimate.</p>
<pre class="r"><code>x &lt;- as.matrix(dx[ ,.(1, Ict)])
I &lt;- dx[, length(unique(id))]
N &lt;- nrow(x)
K &lt;- ncol(x)
J &lt;- dx[, length(unique(site))]
T &lt;- dx[, length(unique(period))]
ii &lt;- dx[, id]
jj &lt;- dx[, site]
tt &lt;- dx[, period] + 1
y &lt;- dx[, y]

testdat &lt;- list(I=I, N=N, K=K, J=J, T=T, ii=ii, jj=jj, tt=tt, x=x, y=y)

library(rstan)
options(mc.cores = parallel::detectCores())

rt &lt;- stanc(&quot;binary sw - ar ind effect - non-central.stan&quot;)
sm &lt;- stan_model(stanc_ret = rt, verbose=FALSE)

fit.ar1 &lt;- sampling(sm, data=testdat,
                    iter = 4000, warmup = 1000,
                    control=list(adapt_delta=0.90,
                        max_treedepth = 15),
                    chains = 4)</code></pre>
</div>
<div id="diagnostics" class="section level3">
<h3>Diagnostics</h3>
<p>After running the MCMC process to generate the probability distributions, the trace plots show that the mixing is quite adequate for the chains.</p>
<pre class="r"><code>plot(fit.ar1, plotfun = &quot;trace&quot;, pars = pars, 
  inc_warmup = FALSE, ncol = 1)</code></pre>
<p><img src="/img/post-opensw/diagplot.png" height="600" /></p>
</div>
<div id="extracting-results" class="section level3">
<h3>Extracting results</h3>
<p>If we take a look at the posterior probability distributions, we can see that they contain the original values used to generate the data - so at least in this case, the model seems to model the original data generation process quite well.</p>
<pre class="r"><code>pars &lt;- c(&quot;beta&quot;, &quot;sigma_S&quot;,&quot;sigma_I&quot;,&quot;rho&quot;)
summary(fit.ar1, pars = pars, probs = c(0.025, 0.975))$summary</code></pre>
<pre><code>##           mean  se_mean     sd   2.5%  97.5% n_eff Rhat
## beta[1] -0.519 0.000687 0.0459 -0.609 -0.428  4470    1
## beta[2] -0.751 0.000844 0.0573 -0.864 -0.638  4618    1
## sigma_S  0.307 0.000394 0.0256  0.260  0.362  4223    1
## sigma_I  0.254 0.001548 0.0476  0.148  0.337   945    1
## rho      0.544 0.001594 0.0812  0.376  0.698  2599    1</code></pre>
<p><img src="/img/post-opensw/postplot.png" height="500" /></p>
<p>One thing that is not working so well is my attempt to compare different models. For example, I might want to fit another model that does not assume between-period correlations decay and compare it to the current model. Previously, I used the <code>bridgesampling</code> package for the comparisons, but it does not seem to be able to accommodate these models. I will continue to explore the options more model comparison and will report back if I find something promising.</p>
<p>
<p><small><font color="darkkhaki"></p>
<p>This study is supported by the National Institutes of Health National Institute on Aging R61AG061904. The views expressed are those of the author and do not necessarily represent the official position of the funding organizations.</p>
</font></small>
</p>
</div>
