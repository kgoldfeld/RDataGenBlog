---
title: 'To impute or not: the case of an RCT with baseline and follow-up measurements'
author: Package Build
date: '2022-04-12'
slug: []
categories: []
tags:
  - R
  - Missing data
type: ''
subtitle: ''
image: ''
---



<p>Under normal conditions, conducting a randomized clinical trial is challenging. Throw in a pandemic and things like site selection, patient recruitment and patient follow-up can be particularly vexing. In any study, subjects need to be retained long enough so that outcomes can be measured; during a period when there are so many potential disruptions, this can become quite difficult. This issue of <em>loss to follow-up</em> recently came up during a conversation among a group of researchers who were troubleshooting challenges they are all experiencing in their ongoing trials. While everyone agreed that missing outcome data is a significant issue, there was less agreement on how to handle this analytically when estimating treatment effects.</p>
<p>For me, this discussion quickly brought to mind two posts I did on missing data, where I <a href="https://www.rdatagen.net/post/musings-on-missing-data/" target="_blank">reflected</a> on the different missing data mechanisms (MCAR, MAR, and NMAR) and <a href="https://www.rdatagen.net/post/2021-03-30-some-cases-where-imputing-missing-data-matters/" target="_blank">explored</a> when it might be imperative to use multiple imputation as part of the analysis.</p>
<p>In light of the recent conversation, I wanted to revisit this issue of loss to follow-up in the context of a clinical trial where the outcome measure is collected at baseline (about which I’ve written about before, <a href="https://www.rdatagen.net/post/thinking-about-the-run-of-the-mill-pre-post-analysis/" target="_blank">here</a> and <a href="https://www.rdatagen.net/post/2021-11-23-design-effects-with-baseline-measurements/" target="_blank">here</a>) and we can be fairly certain that this baseline measurement will be quite well balanced at baseline.</p>
<div id="the-data-generating-process" class="section level3">
<h3>The data generating process</h3>
<p>In my earlier posts on missing data, I described the observed and missing data processes using a directly acyclic graphs (DAGs), which allow us to visualize the assumed causal relationships in our model. Here is a DAG for a clinical trial that collects baseline measure <span class="math inline">\(Y\)</span> at baseline (<span class="math inline">\(Y_0\)</span>) and again one year later (<span class="math inline">\(Y_1\)</span>):</p>
<p><img src="img/MAR_3_DAG.png" style="width:40.0%" /></p>
<p><span class="math inline">\(A\)</span> is the treatment indicator, <span class="math inline">\(A \in \{0,1\}\)</span>, <span class="math inline">\(A=1\)</span> if the patient has been randomized to the treatment arm, and <span class="math inline">\(A=0\)</span> under the control arm. <span class="math inline">\(R_Y\)</span> is a missing data indicator set to 1 if there is loss to follow up (i.e., <span class="math inline">\(Y_1\)</span> is not collected), and 0 otherwise. <span class="math inline">\(Y_1^*\)</span> is the observed value of <span class="math inline">\(Y_1\)</span>. If <span class="math inline">\(R_Y = 1\)</span>, the value of <span class="math inline">\(Y_1^*\)</span> is <em>NA</em> (i.e. missing), otherwise <span class="math inline">\(Y_1 ^*= Y_1\)</span>.</p>
<p>In the scenario depicted in this DAG, both <span class="math inline">\(Y_0\)</span> and <span class="math inline">\(A\)</span> potentially influence the (possibly unobserved) outcome <span class="math inline">\(Y_1\)</span> <em>and</em> whether there is loss to follow-up <span class="math inline">\(R_Y\)</span>. (I have explicitly left out the possibility that <span class="math inline">\(Y_1\)</span> itself can impact missingness, because this is a much more challenging problem - not missing at random or NMAR.)</p>
<p>The strengths of the relationships between the variables are determined by the parameters <span class="math inline">\(\delta\)</span>, <span class="math inline">\(\alpha\)</span>, and <span class="math inline">\(\beta\)</span>. (I have fixed the direct relationship between <span class="math inline">\(Y_0\)</span> and <span class="math inline">\(Y_1\)</span> to a value of 1, but there is no reason that needs to be so.) The dashed line from <span class="math inline">\(Y_0\)</span> to the causal line connecting <span class="math inline">\(A\)</span> and <span class="math inline">\(Y_1\)</span> which has parameter <span class="math inline">\(\lambda\)</span> reflects the possibility that the treatment effect of <span class="math inline">\(A\)</span> will vary across different levels of the baseline measurement (i.e., there is an interaction between <span class="math inline">\(Y_0\)</span> and <span class="math inline">\(A\)</span>).</p>
<p>For the purposes of this simulation, I am assuming this linear relationship:</p>
<p><span class="math display">\[Y_{1i} = Y_{0i} + \delta A_i - \lambda A_i Y_{0i} + e_i, \ \ A_i \in \{0, 1\}\]</span></p>
<p>I am using <span class="math inline">\(-\lambda\)</span> in order to simulate a situation where patients with lower values of <span class="math inline">\(Y_0\)</span> actually have larger overall treatment effects than those with higher values.</p>
<p><span class="math inline">\(Y_0\)</span> and <span class="math inline">\(e\)</span> are both normally distributed:</p>
<p><span class="math display">\[ Y_{0i} \sim N(\mu =0, \sigma^2 = 1)\]</span>
<span class="math display">\[e_i \sim N(\mu =0, \sigma^2 = 0.5)\]</span></p>
<p>The missing data mechanism is also linear, but on the <em>logistic</em> scale. In this scenario, patients with lower baseline values <span class="math inline">\(Y_0\)</span> are more likely to be lost to follow-up than patients with higher values (assuming, of course, <span class="math inline">\(\alpha &gt; 0\)</span>):</p>
<p><span class="math display">\[\text{logit}(P(R_{Yi} = 1)) =-1.5 - \alpha Y_{0i} - \beta A_i\]</span></p>
<p>Under these assumptions, the probability that a patient with baseline measure <span class="math inline">\(Y_0 = 0\)</span> in the control arm is lost to follow-up is <span class="math inline">\(\frac{1}{1 + exp(1.5)} \approx 18\%\)</span>.</p>
</div>
<div id="data-simulation" class="section level3">
<h3>Data simulation</h3>
<p>I am using the <code>simstudy</code> package to simulate data from these models, which allows me to define the data generating process described above. First, let’s load the necessary libraries:</p>
<pre class="r"><code>library(simstudy)
library(data.table)
library(mice)</code></pre>
<p>The table <code>def</code> implements the definitions for the data generating process. I’ve created two versions of <span class="math inline">\(Y_1\)</span>. The first is the true underlying value of <span class="math inline">\(Y_1\)</span>, and the second <span class="math inline">\(Y_{1_{obs}}\)</span> is really <span class="math inline">\(Y_1^*\)</span> from the DAG. At the outset, there are no missing data, so initially <span class="math inline">\(Y_{1_{obs}}\)</span> is just a replicate of <span class="math inline">\(Y_1\)</span>:</p>
<pre class="r"><code>def &lt;- defData(varname = &quot;y0&quot;, formula = 0, variance = 1)
def &lt;- defData(def, &quot;a&quot;, formula = &quot;1;1&quot;, dist = &quot;trtAssign&quot;)
def &lt;- defData(def, &quot;y1&quot;, &quot;y0 + ..delta * a - ..lambda * y0 * a&quot;, 0.5)
def &lt;- defData(def, &quot;y1_obs&quot;, formula = &quot;y1&quot;, dist = &quot;nonrandom&quot;)</code></pre>
<p>The missing data generating process is defined in table <code>defM</code>:</p>
<pre class="r"><code>defM &lt;- defMiss(
    varname = &quot;y1_obs&quot;, 
    formula = &quot;-1.5  - ..alpha * y0 - ..beta * a&quot;, 
    logit.link = TRUE
)</code></pre>
<p>For this particular simulation, I am assuming <span class="math inline">\(\delta = 1\)</span>, <span class="math inline">\(\lambda = 0.8\)</span>, <span class="math inline">\(\alpha = 1\)</span>, and <span class="math inline">\(\beta = 0\)</span>:</p>
<pre class="r"><code>delta &lt;- 1
lambda &lt;- 0.8

alpha &lt;- 1
beta &lt;- 0</code></pre>
<p>With all the definitions and parameters set, we are ready to generate the data:</p>
<pre class="r"><code>RNGkind(kind = &quot;L&#39;Ecuyer-CMRG&quot;)
set.seed(1234)

dd &lt;- genData(1200, def)
dmiss &lt;- genMiss(dd, defM, idvars = &quot;id&quot;)
dobs &lt;- genObs(dd, dmiss, idvars = &quot;id&quot;)

dobs</code></pre>
<pre><code>##         id     y0 a     y1 y1_obs
##    1:    1 -0.068 1  0.365     NA
##    2:    2 -0.786 1  0.842  0.842
##    3:    3  0.154 0 -0.072 -0.072
##    4:    4  0.037 0 -1.593 -1.593
##    5:    5  0.926 0  1.915  1.915
##   ---                            
## 1196: 1196  0.442 1  1.333  1.333
## 1197: 1197  2.363 1  2.385  2.385
## 1198: 1198 -1.104 0 -2.115 -2.115
## 1199: 1199 -1.380 1  0.947  0.947
## 1200: 1200 -1.023 1  1.250     NA</code></pre>
</div>
<div id="estimating-the-treatment-effect" class="section level3">
<h3>Estimating the treatment effect</h3>
<p>Now, with the data in hand, we can estimate the treatment effect. In this case, I will fit three different models. The first assumes that there was no missing data at all (i.e., we had full access to <span class="math inline">\(Y_1\)</span> for all study participants). The second is an analysis using only cases with complete data, which ignores missing data entirely and assumes that the missing data process is MCAR (missing completely at random). The third analysis uses multiple imputation to generate values for the missing cases based on distributions of the observed data - and does this repeatedly to come up with a series of data sets (in this case 20). In this last analysis, a model is fit for each of the 20 data sets, and the results are pooled:</p>
<pre class="r"><code>fit_all &lt;- lm(y1 ~ y0 + a, data = dobs)
fit_comp &lt;- lm(y1_obs ~ y0 + a, data = dobs)

imp_dd &lt;- dobs[, -c(&quot;id&quot;, &quot;y1&quot;)]
imp &lt;- mice(imp_dd, m=20, maxit=5, print=FALSE)
fit_imp &lt;- pool(with(imp, lm(y1_obs ~ y0 + a)))</code></pre>
<p>Here is a figure that shows the estimated regression lines for each of the models (showed sequentially in animated form). In all three cases, we are adjusting for baseline measurement <span class="math inline">\(Y_0\)</span>, which is a good thing to do even when there is good balance across treatment arms; this tends to reduce standard errors. Also note that I am ignoring the possibility of heterogeneous treatment effects with respect to different levels of <span class="math inline">\(Y_0\)</span> (determined by <span class="math inline">\(\lambda\)</span> in the data generation process); I am effectively estimating the <em>average</em> treatment effect across all levels of <span class="math inline">\(Y_0\)</span>.</p>
<p>The analysis based on the full data set (A) recovers the treatment effect parameter quite well, but the complete data analysis (B) underestimates the treatment effect; the imputed analysis (C) does much better.</p>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-8-1.gif" /><!-- --></p>
</div>
<div id="estimating-the-bias-of-each-modeling-approach" class="section level3">
<h3>Estimating the bias of each modeling approach</h3>
<p>To conduct a more systematic assessment of the bias associated with each model <span class="math inline">\(m, \ m \in \{A, B, C\},\)</span> I repeatedly simulated data under a range of assumptions about <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> (I fixed <span class="math inline">\(\delta\)</span> since it has no impact on the bias). In total, I assessed 54 scenarios by setting <span class="math inline">\(\lambda = \{0, 0.2, \dots, 1\}\)</span>, <span class="math inline">\(\alpha = \{0, 0.5, 1\}\)</span>, and <span class="math inline">\(\beta = \{0, 1, 2\}\)</span>. (The code for this simulation can be found below in the addendum.)</p>
<p>For each set of assumptions <span class="math inline">\(s, \ s \in \{1, \dots, 54\}\)</span>, I generated 5000 data sets with 200 patients and estimated the parameters from all three models for each data set. I was particularly interested in the estimate of the average treatment effect <span class="math inline">\(\hat\delta_{smk}\)</span> (i.e. the average treatment effect for each model <span class="math inline">\(m\)</span> under assumptions <span class="math inline">\(s\)</span> and each iteration <span class="math inline">\(k, \ k\in \{1,\dots,5000\}).\)</span></p>
<p>Using the results from the iterations, I estimated the bias $_{sm} for each set of assumptions <span class="math inline">\(s\)</span> and model <span class="math inline">\(m\)</span> as:</p>
<p><span class="math display">\[\hat{\text{B}}_{sm} =\frac{1}{5000} \sum_{k=1}^{5000} (\hat\delta_{smk} - \delta)\]</span></p>
<p>The following figure shows <span class="math inline">\(\hat{\text{B}}_{sm}\)</span> for each of the three modeling approaches:</p>
<p><br></p>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>It is clear that if we have no missing data, all the estimates are unbiased. And in this case, it does not appear that missingness related specifically to treatment arm (determined by parameter <span class="math inline">\(\beta\)</span>) does not have much of an impact. However bias is impacted considerably by both heterogeneous treatment effect (parameter <span class="math inline">\(\lambda\)</span>) and missingness related to <span class="math inline">\(Y_0\)</span> (parameter <span class="math inline">\(\alpha\)</span>), and especially the combination of both <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\lambda\)</span>.</p>
<p>If missingness is independent of <span class="math inline">\(Y_0\)</span> (<span class="math inline">\(\alpha = 0\)</span>), there is no induced bias just using complete data (model B), even with substantial heterogeneity of treatment effect (<span class="math inline">\(\lambda = 1\)</span>). With moderate missingness due to <span class="math inline">\(Y_0\)</span> (<span class="math inline">\(\alpha = 0.5\)</span>), there is still no bias for the complete data analysis with low heterogeneity. However, bias is introduced here as heterogeneity becomes more pronounced. Using imputation reduces a good amount of the bias. Finally, when missingness is strongly related to <span class="math inline">\(Y_0\)</span>, both the complete data and imputed data analysis fare poorly, on average. Although multiple imputation worked well in our initial data set above with <span class="math inline">\(\alpha = 1\)</span>, the figure from the repeated simulations suggests that multiple imputation did not perform so well on average at that level. This is probably due to the fact that if there is <em>a lot</em> of missing data, imputation has much less information at its disposal and the imputed values are not so helpful.</p>
<p><br></p>
</div>
<div id="addendum" class="section level3">
<h3>Addendum</h3>
<p>Here is the code used to generate the iterative simulations:</p>
<pre class="r"><code>s_define &lt;- function() {
  
  def &lt;- defData(varname = &quot;y0&quot;, formula = 0, variance = 1)
  def &lt;- defData(def, &quot;a&quot;, formula = &quot;1;1&quot;, dist = &quot;trtAssign&quot;)
  def &lt;- defData(def, &quot;y1&quot;, 
           formula = &quot;y0 + ..delta * a - ..lambda * y0 * a&quot;, variance = 0.5)
  def &lt;- defData(def, &quot;y1_obs&quot;, formula = &quot;y1&quot;, dist = &quot;nonrandom&quot;)
  
  defM &lt;- defMiss(
    varname = &quot;y1_obs&quot;, formula = &quot;-1.5  - ..alpha * y0 - ..beta * a&quot;, 
    logit.link = TRUE
  )
  
  return(list(def = def, defM = defM))
}

s_generate &lt;- function(list_of_defs, argsvec) {
  
  list2env(list_of_defs, envir = environment())
  list2env(as.list(argsvec), envir = environment())
  
  dd &lt;- genData(200, def)
  dmiss &lt;- genMiss(dd, defM, idvars = &quot;id&quot;)
  dobs &lt;- genObs(dd, dmiss, idvars = &quot;id&quot;)

  return(dobs) #  generated_data is a data.table
}

s_model &lt;- function(generated_data) {
  
  imp_dd &lt;- generated_data[, -c(&quot;id&quot;, &quot;y1&quot;)]
  imp &lt;- mice(imp_dd, m=20, maxit=5, print=FALSE)
  
  a_all &lt;- coef(lm(y1 ~ y0 + a, data = generated_data))[&quot;a&quot;]
  a_missing &lt;- coef(lm(y1_obs ~ y0 + a, data = generated_data))[&quot;a&quot;]

  fit_imp &lt;- pool(with(imp, lm(y1_obs ~ y0 + a)))
  a_imp &lt;- summary(fit_imp)[3, &quot;estimate&quot;]

  return(data.table(a_all, a_missing, a_imp)) # model_results is a data.table
}

s_single_rep &lt;- function(list_of_defs, argsvec) {
  
  generated_data &lt;- s_generate(list_of_defs, argsvec)
  model_results &lt;- s_model(generated_data)
  
  return(model_results)
}

s_replicate &lt;- function(argsvec, nsim) {
  
  list_of_defs &lt;- s_define()
  
  model_results &lt;- rbindlist(
    parallel::mclapply(
      X = 1 : nsim, 
      FUN = function(x) s_single_rep(list_of_defs, argsvec), 
      mc.cores = 4)
  )
  
  #--- add summary statistics code ---#
  
  summary_stats &lt;- model_results[, .(
      mean_all = mean(a_all, na.rm = TRUE), 
      bias_all = mean(a_all - delta, na.rm = TRUE), 
      var_all = var(a_all, na.rm = TRUE), 
      
      mean_missing = mean(a_missing, na.rm = TRUE), 
      bias_missing = mean(a_missing - delta, na.rm = TRUE), 
      var_missing = var(a_missing, na.rm = TRUE),
      
      mean_imp = mean(a_imp, na.rm = TRUE), 
      bias_imp = mean(a_imp - delta, na.rm = TRUE), 
      var_imp = var(a_imp, na.rm = TRUE)
    )]
  
  summary_stats &lt;- data.table(t(argsvec), summary_stats)
  
  return(summary_stats) # summary_stats is a data.table
}

#---- specify varying power-related parameters ---#

scenario_list &lt;- function(...) {
  argmat &lt;- expand.grid(...)
  return(asplit(argmat, MARGIN = 1))
}

delta &lt;- 1
lambda &lt;- c(0, 0.2, .4, .6, .8, 1)
alpha &lt;- c(0, 0.5, 1)
beta &lt;- c(0, 1, 2)

scenarios &lt;- scenario_list(delta = delta, lambda = lambda, alpha = alpha, beta = beta)

summary_stats &lt;- rbindlist(lapply(scenarios, function(a) s_replicate(a, nsim = 5000)))</code></pre>
</div>
