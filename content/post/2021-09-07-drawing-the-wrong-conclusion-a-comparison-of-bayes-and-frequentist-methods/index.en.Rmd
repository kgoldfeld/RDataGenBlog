---
title: 
  'Drawing the wrong conclusion: a comparison of Bayes and frequentist methods'
author: Package Build
date: '2021-09-07'
slug: []
categories: []
tags:
  - Bayesian model
  - Stan
  - R
type: ''
subtitle: ''
image: ''
draft: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
options(digits = 2)
```

In the previous post [include a link], I simulated data from a hypothetical RCT that had heterogeneous treatment effects across subgroups. I presented two Bayesian models, a strongly *pooled* model and an *unpooled* version, that could be used estimate all the subgroup effects in a single model. I compared the estimates to a set of linear regression models that were estimated for each subgroup separately. My goal in doing these comparisons is to investigate how often we draw the wrong conclusion about subgroup effects when we conduct these types of analyses. In a typical frequentist framework, the probability of making a mistake is usually considerably greater than the 5\% error rate that we allow ourselves, because if we conduct multiple tests we have more chances to make a mistake. By using Bayesian hierarchical models that share information across subgroups, I wanted to see if we can reduce the chances of drawing the wrong conclusions. This follow-up shows the results this quick investigation.

### Simulation framework

The simulations used here are based on the same general process I used to generate a single data set the last time around (and feel free to take a look for more details). The key difference this time around is that I want to understand the operating characteristics of the models, and this requires us to generate many data sets and fit models for each of them. Since much of the modeling is similar to last time, I'm only showing the code that is different this time around. (If you want the detailed code, let me know.)

This is a pretty computing intensive exercise. While the models don't take too long to fit, especially with only 150 observations per data set, fitting 2500 sets of models can take some time. As I do for all the simulations that require repeated Bayesian estimation, I executed all of this on a high-performance computer. I used a framework similar to what I've described for conducting [power analyses](https://www.rdatagen.net/post/2021-03-16-framework-for-power-analysis-using-simulation/){target="_blank"} and [exploring](https://www.rdatagen.net/post/a-frequentist-bayesian-exploring-frequentist-properties-of-bayesian-models/){target="_blank"} the operating characteristics of Bayesian models.

#### Definitions

The definitions are the same, except I've made the generation of `theta` more flexible. Last time around, I fixed the coefficients at specific values. Here, the coefficients can vary from iteration to iteration. Even though I am generating data with no treatment effect, I am taking a Bayesian point of view on this - so that the treatment effect parameters will have a distribution that is centered around 0 with very low variance.

```{r}
library(cmdstanr)
library(simstudy)
library(posterior)
library(data.table)
library(slurmR)

setgrp <- function(a, b, c) {
  
  if (a==0 & b==0 & c==0) return(1)
  if (a==1 & b==0 & c==0) return(2)
  if (a==0 & b==1 & c==0) return(3)
  if (a==0 & b==0 & c==1) return(4)
  if (a==1 & b==1 & c==0) return(5)
  if (a==1 & b==0 & c==1) return(6)
  if (a==0 & b==1 & c==1) return(7)
  if (a==1 & b==1 & c==1) return(8)
  
}

s_define <- function() {
  
  d <- defData(varname = "a", formula = 0.6, dist="binary")
  d <- defData(d, varname = "b", formula = 0.4, dist="binary")
  d <- defData(d, varname = "c", formula = 0.3, dist="binary")
  d <- defData(d, varname = "theta",
    formula = "..tau[1] + ..tau[2]*a  + ..tau[3]*b + ..tau[4]*c +
               ..tau[5]*a*b + ..tau[6]*a*c + ..tau[7]*b*c + ..tau[8]*a*b*c",
    dist = "nonrandom"
  )
  
  drx <- defDataAdd(
    varname = "y", formula = "0 + theta*rx", 
    variance = 16, 
    dist = "normal"
  )

  return(list(d = d, drx = drx))
  
}
```

#### Data generation

Nothing has really changed here, except that I am now generating 8 values of `tau` for each iteration from a $N(\mu = 0, \sigma = 0.5)$ distribution.

```{r}
s_generate <- function(n, list_of_defs) {
  
  list2env(list_of_defs, envir = environment())
  
  tau <- rnorm(8, 0, .5)
  
  dd <- genData(n, d)
  dd <- trtAssign(dd, grpName = "rx")
  dd <- addColumns(drx, dd)
  
  dd[, grp := setgrp(a, b, c), keyby = id]
  
  dd[]
  
}
```

Here is a single data set - we can see that `theta` is close to 0 but not exactly 0 as we would typically do in simulation using a frequentist framework (where the parameters are presumed known).

```{r}
set.seed(298372)

defs <- s_define()
s_generate(10, defs)
```

#### Model fitting

The models here are precisely how I defined it in the last post. The code is a bit involved, so I opted to exclude it. For each data set, I fit a set of subgroup-specific linear regression models (as well as an overall model that ignored the subgroups), in addition to the two Bayesian models described in the previous post. Each replication defines the data, generates a new data set, and estimates the three different models before returning the results.

```{r, eval=FALSE}
s_model <- function(dd, mod_pool, mod_nopool) {
  ...
}

s_replicate <- function(x, n, mod_pool, mod_nopool) {
  
  set_cmdstan_path(path = "/.../cmdstan/2.25.0")
  
  defs <- s_define()
  generated_data <- s_generate(n, defs)
  estimates <- s_model(generated_data, mod_pool, mod_nopool)

  estimates[]
}
```

This is the code splits the task up so that 50 multi-core computing nodes each run 50 replications. There's actually parallelization in parallel, as each of the nodes has multiple processors so that each Bayes model can be esimated using parallel chains:

```{r, eval = FALSE, echo=TRUE}
set_cmdstan_path(path = "/gpfs/share/apps/cmdstan/2.25.0")

model_pool <- cmdstan_model("/.../subs_pool_hpc.stan")
model_nopool <- cmdstan_model("/.../subs_nopool_hpc.stan")

job <- Slurm_lapply(
  X = 1:2500, 
  FUN = s_replicate, 
  n = 150,
  mod_pool = model_pool,
  mod_nopool = model_nopool,
  njobs = 50, 
  mc.cores = 4L,
  job_name = "i_subs",
  tmp_path = "/.../scratch",
  plan = "wait",
  sbatch_opt = list(time = "12:00:00", partition = "cpu_short", `mem-per-cpu` = "5G"),
  export = c("s_define", "s_generate", "s_model"),
  overwrite = TRUE
)

job
res <- Slurm_collect(job)

save(res, file = "/.../sub_0.rda")
```

The figure shows the results from 80 models. Each column is a different subgroup (and the last is the overall treatment effect estimate). The intervals are the 95% credible intervals from the Bayes model, and the 95% confidence interval from the linear regression model. The intervals are color coded based on whether the interval includes 0 (grey) or not (red). The cases with red intervals are ones we might conclude incorrectly that there is indeed some sort of effect. It is apparent, that there are many more red lines for the linear regression estimates compared to either of the Bayesian models

```{r, echo = FALSE, fig.width=7}

library(simstudy)
library(data.table)
library(ggplot2)
library(cmdstanr)
library(posterior)

load("data/sub_0.rda")

res <- rbindlist(res)
res[, subgroup := variable]

res[, effect.pool := as.numeric(!between(0, p.025, p.975))]
res[, effect.nopool := as.numeric(!between(0, n.025, n.975))]
res[, effect.lm := as.numeric(!between(0, lm.025, lm.975))]

###

cis <- res[, .(index =x, grp = variable, p.025, p.975, n.025, n.975, lm.025, lm.975)]

dp <- melt(data = cis, 
     id.vars = c("index", "grp"),
     measure.vars = list(c("p.025", "n.025", "lm.025"), c("p.975","n.975", "lm.975")), 
     value.name = c("l", "u"),
     variable.factor = TRUE, 
     variable.name = "method"
)

dp[, method := factor(method, labels = c("pooled", "unpooled", "lm"))]

set.seed(8376251)
samp_index <- sample(2500, 80, replace = FALSE)
ds <- dp[index %in% samp_index]
ds[, includes_0 := between(0, l, u)]

ggplot(data= ds, aes(x = l, xend = u, y = factor(index), yend = factor(index))) +
  geom_vline(xintercept = 0, color = "grey85") +
  geom_segment(aes(size = includes_0, color = includes_0)) +
  facet_grid(method ~ grp) +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank(),
        panel.grid = element_blank(),
        legend.position = "none"
  ) +
  scale_color_manual(values = c("#b30003", "grey50")) +
  scale_size_manual(values = c(.25, .1)) +
  xlab("effect size")
```

For the full set of 2500 replications, here are the proportion of cases where at least one of the subgroup (or overall) intervals did not include 0. The *pooled* Bayes model, even without any kind of adjustment for multiple testing, was about 5\%:

```{r, echo=FALSE}
res[, .(sum(effect.pool), 
        sum(effect.nopool),
        sum(effect.lm)), keyby = x][, .(pooled = mean(V1 > 0), 
                                        unpooled = mean(V2 > 0), 
                                        lm = mean(V3 > 0))]

```

