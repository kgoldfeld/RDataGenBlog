---
title: 
  'Drawing the wrong conclusion: a comparison of Bayes and frequentist methods'
author: Package Build
date: '2021-09-07'
slug: []
categories: []
tags:
  - Bayesian model
  - Stan
  - R
type: ''
subtitle: ''
image: ''
draft: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
options(digits = 2)
```

In the previous [post](https://www.rdatagen.net/post/2021-08-31-subgroup-analysis-using-a-bayesian-hierarchical-model/){target="_blank"}, I simulated data from a hypothetical RCT that had heterogeneous treatment effects across subgroups defined by three covariates. I presented two Bayesian models, a strongly *pooled* model and an *unpooled* version, that could be used estimate all the subgroup effects in a single model. I compared the estimates to a set of linear regression models that were estimated for each subgroup separately. 

My goal in doing these comparisons is to see how often we might draw the wrong conclusion about subgroup effects when we conduct these types of analyses. In a typical frequentist framework, the probability of making a mistake is usually considerably greater than the 5\% error rate that we allow ourselves, because conducting multiple tests gives us more chances to make a mistake. By using Bayesian hierarchical models that share information across subgroups and more reasonably measure uncertainty, I wanted to see if we can reduce the chances of drawing the wrong conclusions.

### Simulation framework

The simulations used here are based on the same general process I used to generate a single data set the [last time around](https://www.rdatagen.net/post/2021-08-31-subgroup-analysis-using-a-bayesian-hierarchical-model/){target="_blank"}. The key difference that I now want to understand the operating characteristics of the models, and this requires many data sets (and their model fits). Since much of the modeling is similar to last time, I'm only showing the code here that is different. (If you want to see the detailed code, let me know.)

This is a pretty computing intensive exercise. While the models don't take too long to fit, especially with only 150 observations per data set, fitting 2500 sets of models can take some time. As I do for all the simulations that require repeated Bayesian estimation, I executed all of this on a high-performance computer. I used a framework similar to what I've described for conducting [power analyses](https://www.rdatagen.net/post/2021-03-16-framework-for-power-analysis-using-simulation/){target="_blank"} and [exploring the operating characteristics](https://www.rdatagen.net/post/a-frequentist-bayesian-exploring-frequentist-properties-of-bayesian-models/){target="_blank"} of Bayesian models.

#### Definitions

The definitions of the data generation process are the same as in the previous post, except I've made the generation of `theta` more flexible. Last time, I fixed the coefficients at specific values. Here, the coefficients can vary from iteration to iteration. Even though I am generating data with no treatment effect, I am taking a Bayesian point of view on this - so that the treatment effect parameters will have a distribution that is centered around 0 with very low variance.

```{r}
library(cmdstanr)
library(simstudy)
library(posterior)
library(data.table)
library(slurmR)

setgrp <- function(a, b, c) {
  
  if (a==0 & b==0 & c==0) return(1)
  if (a==1 & b==0 & c==0) return(2)
  if (a==0 & b==1 & c==0) return(3)
  if (a==0 & b==0 & c==1) return(4)
  if (a==1 & b==1 & c==0) return(5)
  if (a==1 & b==0 & c==1) return(6)
  if (a==0 & b==1 & c==1) return(7)
  if (a==1 & b==1 & c==1) return(8)
  
}

s_define <- function() {
  
  d <- defData(varname = "a", formula = 0.6, dist="binary")
  d <- defData(d, varname = "b", formula = 0.4, dist="binary")
  d <- defData(d, varname = "c", formula = 0.3, dist="binary")
  d <- defData(d, varname = "theta",
    formula = "..tau[1] + ..tau[2]*a  + ..tau[3]*b + ..tau[4]*c +
               ..tau[5]*a*b + ..tau[6]*a*c + ..tau[7]*b*c + ..tau[8]*a*b*c",
    dist = "nonrandom"
  )
  
  drx <- defDataAdd(
    varname = "y", formula = "0 + theta*rx", 
    variance = 16, 
    dist = "normal"
  )

  return(list(d = d, drx = drx))
  
}
```

#### Data generation

Nothing has really changed here, except that I am now generating 8 values of `tau` for each iteration from a $N(\mu = 0, \sigma = 0.5)$ distribution.

```{r}
s_generate <- function(n, list_of_defs) {
  
  list2env(list_of_defs, envir = environment())
  
  tau <- rnorm(8, 0, .5)
  
  dd <- genData(n, d)
  dd <- trtAssign(dd, grpName = "rx")
  dd <- addColumns(drx, dd)
  
  dd[, grp := setgrp(a, b, c), keyby = id]
  
  dd[]
  
}
```

Here is a single data set - we can see that `theta` is close to 0 but not exactly 0 as we would typically do in simulation using a frequentist framework (where the parameters are presumed known).

```{r}
set.seed(298372)

defs <- s_define()
s_generate(10, defs)
```

#### Model fitting

The models here are precisely how I defined it in the last post. The code is a bit involved, so I opted to exclude it. For each data set, I fit a set of subgroup-specific linear regression models (as well as an overall model that ignored the subgroups), in addition to the two Bayesian models described in the previous post. Each replication defines the data, generates a new data set, and estimates the three different models before returning the results.

```{r, eval=FALSE}
s_model <- function(dd, mod_pool, mod_nopool) {
  ...
}

s_replicate <- function(x, n, mod_pool, mod_nopool) {
  
  set_cmdstan_path(path = "/.../cmdstan/2.25.0")
  
  defs <- s_define()
  generated_data <- s_generate(n, defs)
  estimates <- s_model(generated_data, mod_pool, mod_nopool)

  estimates[]
}
```

This is the code splits the task up so that 50 multi-core computing nodes each runs 50 replications. There's actually parallelization in parallel, as each of the nodes has multiple processors so the Bayesian models can be estimated with parallel chains:

```{r, eval = FALSE, echo=TRUE}
set_cmdstan_path(path = "/gpfs/share/apps/cmdstan/2.25.0")

model_pool <- cmdstan_model("/.../subs_pool_hpc.stan")
model_nopool <- cmdstan_model("/.../subs_nopool_hpc.stan")

job <- Slurm_lapply(
  X = 1:2500, 
  FUN = s_replicate, 
  n = 150,
  mod_pool = model_pool,
  mod_nopool = model_nopool,
  njobs = 50, 
  mc.cores = 4L,
  job_name = "i_subs",
  tmp_path = "/.../scratch",
  plan = "wait",
  sbatch_opt = list(time = "12:00:00", partition = "cpu_short", `mem-per-cpu` = "5G"),
  export = c("s_define", "s_generate", "s_model"),
  overwrite = TRUE
)

job
res <- Slurm_collect(job)

save(res, file = "/.../sub_0.rda")
```

### Results

The figure shows the results from 80 models. Each column is a different subgroup (and the last is the overall treatment effect estimate). The intervals are the 95% credible intervals from the Bayesian models, and the 95% confidence interval from the linear regression model. The intervals are color coded based on whether the interval includes 0 (grey) or not (red). The red intervals are cases where we might incorrectly conclude that there is indeed some sort of effect. There are many more red lines for the linear regression estimates compared to either of the Bayesian models:

```{r, echo = FALSE, fig.width=7}

library(simstudy)
library(data.table)
library(ggplot2)
library(cmdstanr)
library(posterior)

load("data/sub_0.rda")

res <- rbindlist(res)
res[, subgroup := variable]

res[, effect.pool := as.numeric(!between(0, p.025, p.975))]
res[, effect.nopool := as.numeric(!between(0, n.025, n.975))]
res[, effect.lm := as.numeric(!between(0, lm.025, lm.975))]

###

cis <- res[, .(index =x, grp = variable, p.025, p.975, n.025, n.975, lm.025, lm.975)]

dp <- melt(data = cis, 
     id.vars = c("index", "grp"),
     measure.vars = list(c("p.025", "n.025", "lm.025"), c("p.975","n.975", "lm.975")), 
     value.name = c("l", "u"),
     variable.factor = TRUE, 
     variable.name = "method"
)

dp[, method := factor(method, labels = c("pooled", "unpooled", "lm"))]

set.seed(8376251)
samp_index <- sample(2500, 80, replace = FALSE)
ds <- dp[index %in% samp_index]
ds[, includes_0 := between(0, l, u)]

ggplot(data= ds, aes(x = l, xend = u, y = factor(index), yend = factor(index))) +
  geom_vline(xintercept = 0, color = "grey85") +
  geom_segment(aes(size = includes_0, color = includes_0)) +
  facet_grid(method ~ grp) +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank(),
        panel.grid = element_blank(),
        legend.position = "none"
  ) +
  scale_color_manual(values = c("#b30003", "grey50")) +
  scale_size_manual(values = c(.25, .1)) +
  xlab("effect size")
```

For the full set of 2500 replications, about 5\% of the intervals from the *pooled* Bayes did not include 0, lower than the *unpooled* model, and far below the approach using individual subgroup regression models:

```{r, echo=FALSE}
res[, .(sum(effect.pool), 
        sum(effect.nopool),
        sum(effect.lm)), keyby = x][, .(pooled = mean(V1 > 0), 
                                        unpooled = mean(V2 > 0), 
                                        lm = mean(V3 > 0))]

```

I'm not sure that the journal reviewers would buy this argument, but it seems that pooling estimates across subgroups provides a viable way to guard against making overly strong statements about effect sizes when they are not really justified.