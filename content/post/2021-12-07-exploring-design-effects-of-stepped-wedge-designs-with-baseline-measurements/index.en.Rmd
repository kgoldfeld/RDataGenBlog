---
title: Exploring design effects of stepped wedge designs with baseline measurements
author: Keith Goldfeld
date: '2021-12-07'
slug: []
categories: []
tags:
  - R
  - Cluster randomized trials
type: ''
subtitle: ''
image: ''
draft: TRUE
---

```{r setup, include=FALSE}
options(digits = 2)
```

In the [previous post](https://www.rdatagen.net/post/2021-11-23-design-effects-with-baseline-measurements/){target="_blank"}, I described an incipient effort that I am undertaking with two colleagues, Monica Taljaard and Fan Li, to better understand the implications for collecting baseline measurements on sample size requirements for stepped wedge cluster randomized trials. (The three of us are on the [Design and Statistics Core](https://impactcollaboratory.org/design-and-statistics-core/){target="_blank"} of the [NIA IMPACT Collaboratory](https://impactcollaboratory.org/){target="_blank"}.) In that post, I conducted a series of simulations that illustrated the design effects in parallel cluster randomized trial designs that had been derived analytically in a [paper](https://onlinelibrary.wiley.com/doi/full/10.1002/sim.5352){target="_blank"} by *Teerenstra et al*. In this post, I am extending those simulations to stepped wedge trials, where design effects have yet to be derived; the actual derivation is the ultimate goal of this project.

```{r, warning=FALSE, message=FALSE, echo=FALSE}
RNGkind("L'Ecuyer-CMRG")
set.seed(71918)

library(simstudy)
library(ggplot2)
library(lmerTest)
library(parallel)
library(data.table)
library(pwr)
library(paletteer)
library(gtsummary)
```

```{r, echo = FALSE}
base_sw <- function(effect, trend, nsites, nwaves, nperiods, n, s_c, s_cp, s_s, s_sp) {
  
  defC <- defData(varname = "c", formula = 0, variance = "..s_c")
  defCP <- defDataAdd(varname = "c.p", formula = 0, variance = "..s_cp")
  defS <- defDataAdd(varname = "s", formula = 0, variance = "..s_s")
  defSP <- defDataAdd(varname = "y",
    formula = "..trend * s_per + ..effect * Z * p_fu + c + c.p + s", 
    variance = "..s_sp"
  )
  
  dsite <- genData(nsites, defC, id = "site")
  
  dper <- addPeriods(dsite, nPeriods = nperiods, idvars = "site", 
                     timeid = "s_time", perName = "s_per")
  dper <- addColumns(defCP, dper)
  
  dsw <- trtStepWedge(dper, "site", nWaves = nwaves, lenWaves = 1, 
                      startPer = 1, perName = "s_per",
                      grpName = "Z")
  
  dpat <- genCluster(dper, cLevelVar = "s_time", 
                     numIndsVar = n, level1ID = "id")
  dpat <- addColumns(defS, dpat)
  dpat <- addPeriods(dpat, nPeriods = 2, idvars = "id", timeid = "p_time", perName = "p_fu")

  setkey(dpat, "s_time", "site", "s_per", "id")
  dsw[, c("c", "c.p") := NULL]
  setkey(dsw, "s_time", "site", "s_per")
  
  dpat <- merge(dpat, dsw)
  setkey(dpat, "id", "p_fu")
  dpat <- addColumns(defSP, dpat)
}
```

## Stepped wedge designs

At the end of the previous post, I provided links to a few earlier entries where I described stepped wedge designs in some detail. The first [post](https://www.rdatagen.net/post/alternatives-to-stepped-wedge-designs/){target="_blank"} I suggested provides an introductory overview.

In the stepped-wedge design, all clusters in a trial will receive the intervention at some point, but the start of the intervention at different sites will be staggered. The amount of time in each state (control or intervention) will differ for each cluster (or group of clusters if there are waves of more than one cluster starting up at the same time). In this design, time is divided into discrete data collection/phase-in periods. 

In the figure below there are four waves of clusters, with sets of five clusters in each wave. All waves are in the control condition in the first observation period, wave 1 starts the intervention in period 2, wave 2 starts in period 3, wave 3 in period 4, and the last wave starts in period 5. Data from individuals in each cluster are collected for all periods, regardless of intervention status. The points in the figure represent individual (follow-up) responses within each cluster and time period.

```{r, echo = FALSE, fig.height=4}
dd <- base_sw(1, 3, 20, 4, 5, 30, 6, 0, 44, 20)

dplot <- dd[p_fu == 1]
dplot[, s_per := s_per + 1]
dplot[, startTrt := startTrt + 1]

dplot[, sitef := factor(site,labels = c(1:5)), keyby = startTrt]

ggplot(data = dplot, aes(y = y, x = s_per)) +
  geom_jitter(aes(color = factor(Z)),
            size = .2, width = .2) +
  facet_grid((startTrt - 1) ~ sitef) +
  theme(panel.grid = element_blank(),
        legend.position = "none") +
  # scale_color_manual(values = c("#0078b3", "#b33b00")) +
  scale_x_continuous(limits = c(0.4, 5.4), breaks = c(1:5), name = "period") +
  scale_colour_paletteer_d(palette = "wesanderson::Moonrise2")
```

## Data generation process

In order to accommodate a range of scenarios, I've written a general function `base_sw` that will be used in the data generating process (see <a href="#addendum">addendum</a>). This function allows for time period effects (specified as a time trend) as well as all the possible cluster and individual random effects. Importantly, this function will generate two outcome measures for each subject within a specific cluster and time period.

This is the general data generation process that I've used here:

$$
Y_{ijkt} = \tau  (t-1) +  \gamma k Z_{jt}   + c_j  + cp_{jt} + s_{ijt} + sp_{ijkt},
$$

where $Y_{ijkt}$ is the outcome for subject $i$ in cluster $j$ measured in period $t,$ $t \in \{1,\dots, T\};$ $k \in \{0, 1\}, \ k=0$ if the measurement is taken at baseline  and $k=1$ if the measurement is the follow-up. $Z_{jt}$ is a treatment indicator for cluster $j$ during period $t$. (There is no reason that the there needs to be a linear time trend - it is just what I am using here to easily generate time period effects.)

The parameter $\tau$ is the general time trend. $\gamma$ is the treatment effect, which can only impact follow-up measurements. $c_j$ and $cp_{jt}$ are the cluster and cluster-period specific random effects with distributions $N(0, \sigma_c^2)$ and $N(0, \sigma_{cp}^2)$, respectively. $s_{ijt}$ is the subject-level effect (and is specific to period $t$ since the subject is only observed in that period), $s_{ijt} \sim N(0, \sigma_{s}^2)$. $sp_{ijkt}$ is the measurement noise, $sp_{ijkt} \sim N(0, \sigma_{sp}^2).$

The <a href="#addendum">code</a> itself is a little involved, because there are two longitudinal processes that need to be combined - clusters are repeated over time and subjects are repeated over time within each cluster period. In addition, there are two possible layers of cluster and subject random effects.

Below are three plots of data generated from three possible scenarios, using ten sites divided into two waves. The first scenario includes cluster and subject specific effects, but no fixed or random period effects. The second adds fixed period effects, and the third adds both fixed and random period effects:

```{r}
dd_1 <- base_sw(
  effect = 1, 
  trend = 0, 
  nsites = 10, 
  nwaves = 2, 
  nperiods = 5, 
  n = 30, 
  s_c = 6, 
  s_cp = 0, 
  s_s = 44, 
  s_sp = 20
)

dd_2 <- base_sw(1, 3, 10, 2, 5, 30, 6, 0, 44, 20)
dd_3 <- base_sw(1, 3, 10, 2, 5, 30, 4, 2, 44, 20) 
```

```{r, echo=FALSE}
# No period effects (random or fixed)

dplot <-dd_1[startTrt <= 2]

dplot[, s_per := s_per + 1]
dplot[, startTrt := startTrt + 1]

dplot[, sitef := factor(site,labels = c(1:5)), keyby = startTrt]
dplot[, plotper := s_per - 0.5 + p_fu]
  
p_no <- ggplot(data = dplot, aes(y = y, x = plotper)) +
    geom_line(aes(group = id, color = factor(Z)),
              position = position_dodge(width = 0.25),
              size = .2) +
    facet_grid((startTrt - 1) ~ sitef) +
    theme(panel.grid = element_blank(),
          legend.position = "none",
          plot.title = element_text(size = 10, face = "bold"),
          axis.text = element_text(size = 7)) +
    scale_x_continuous(limits = c(0.25, 5.75), breaks = c(1:5), name = "period") +
    scale_y_continuous(limits = c(-50, 50)) +
    scale_colour_paletteer_d(palette = "wesanderson::Moonrise2") +
    ggtitle("no period effects: base_sw(1, 0, 10, 2, 5, 30, 6, 0, 44, 20)")
```

```{r, echo=FALSE}
# No random time effects

dplot <- dd_2[startTrt <= 2]

dplot[, s_per := s_per + 1]
dplot[, startTrt := startTrt + 1]

dplot[, sitef := factor(site, labels = c(1:5)), keyby = startTrt]
dplot[, plotper := s_per - 0.5 + p_fu]
  
p_fixedtime <- ggplot(data = dplot, aes(y = y, x = plotper)) +
    geom_line(aes(group = id, color = factor(Z)),
              position = position_dodge(width = 0.25),
              size = .2) +
    facet_grid((startTrt - 1) ~ sitef) +
    theme(panel.grid = element_blank(),
          legend.position = "none",
          plot.title = element_text(size = 10, face = "bold"),
          axis.text = element_text(size = 7)) +
    scale_x_continuous(limits = c(0.25, 5.75), breaks = c(1:5), name = "period") +
    scale_y_continuous(limits = c(-50, 50)) +
    scale_colour_paletteer_d(palette = "wesanderson::Moonrise2") +
    ggtitle("no random period effects: base_sw(1, 3, 10, 2, 5, 30, 6, 0, 44, 20)")
```


```{r, echo=FALSE}
# All random effects

dplot <- dd_3[startTrt <= 2]
dplot[, s_per := s_per + 1]
dplot[, startTrt := startTrt + 1]
dplot[, sitef := factor(site,labels = c(1:5)), keyby = startTrt]
dplot[, plotper := s_per - 0.5 + p_fu]
  
p_all <- ggplot(data = dplot, aes(y = y, x = plotper)) +
    geom_line(aes(group = id, color = factor(Z)),
              position = position_dodge(width = 0.25),
              size = .2) +
    facet_grid((startTrt - 1) ~ sitef) +
    theme(panel.grid = element_blank(),
          legend.position = "none",
          plot.title = element_text(size = 10, face = "bold"),
          axis.text = element_text(size = 7)) +
    scale_x_continuous(limits = c(0.25, 5.75), breaks = c(1:5), name = "period") +
    scale_y_continuous(limits = c(-50, 50)) +
    scale_colour_paletteer_d(palette = "wesanderson::Moonrise2") +
    ggtitle("random cluster-period effects: base_sw(1, 3, 10, 2, 5, 30, 4, 2, 44, 20)")
```

```{r, fig.width=6, fig.height=7, echo=FALSE}
ggpubr::ggarrange(p_no, p_fixedtime, p_all, nrow = 3)
```

## Models for estimating treatment effects

Presented with a data set, there are many ways we can choose to analyze it. Which model we pick depends in large part on the structure of the data and the assumptions we are willing to make about the data generation process. Here, we are limiting ourselves to five models, all of which are linear mixed effects models. Ideally, we will choose the model that provides us with the most information given the data we have, and we hope these explorations might help clarify which of these five models is our best bet.

To start, we are focusing on a situation where there is no cluster-period effect (Scenario 2 from above), so that $cp_{jt} = 0$ for all $j$ and $t$. The true effect size $\gamma = 1$ and the time trend $\tau = 3$. There will be 20 clusters divided into 4 waves of 5 clusters, with each wave starting in period $t=1$, and subsequent waves starting in each consecutive period. There will be 30 subjects for each cluster, each period. $\sigma_c^2 = 6$, $\sigma_{cp}^2 = 0$, $\sigma_s^2 = 44$, and $\sigma_{sp}^2 = 20.$ (See the second part of the <a href="#addendum">addendum</a> for examples from Scenarios 1 and 3.)

Here is another, larger data set generated under these assumptions:

```{r, message=FALSE, warning=FALSE}
dd <- base_sw(1, 3, 20, 4, 5, 30, 6, 0, 44, 20)
dd[, period := factor(s_per, labels = c(1:5))]

dd
```

And here is the data set that we will actually be able to observe in the real world, which will be used to estimate the models:

```{r}
dd[, .(id, site, period, p_fu, Z, y)]
```

Here is a description of each of the five models, along with code to estimate the model:

**Model 1: Hussey & Hughes model with no baseline measurement**

If we do not have access to the baseline data (or for some reason, choose to ignore it in the modeling), we could fit the stepped wedge model described by [Hussey & Hughes](https://www.sciencedirect.com/science/article/pii/S1551714406000632){target="_blank"}:

$$
Y_{ijt} = \alpha_t + \gamma Z_{jt} + c_j + sp_{ijt},
$$

where $Y_{ijt}$ is the outcome for subject $i$ in cluster $j$ measured in period $t$. $Z_{jt}$ is a treatment indicator for cluster $j$ during period $t$. The $\alpha_t\text{`s}$ are the period-specific fixed effects and $\gamma$ is the treatment effect. $c_j$ is the cluster specific random effects with distribution $N(0, \sigma_c^2)$. $sp_{ijt}$ is the subject-level effect or noise, $sp_{ijt} \sim N(0, \sigma_{s}^2).$ Since there is only a single measurement for each subject, there is only a single subject-level effect. And because we have assumed no cluster-period random effects, $cp_{jt}$ is not included in the model (and will also be the case for the subsequent models described below).

This model can be estimated using the `lmer` function in package `lme4`. Note that we are using only the follow-up data in the model estimation:

```{r, message=FALSE, warning=FALSE}
d_std <- dd[p_fu == 1]
fit_1 <-  lmer( y ~  period + Z  - 1 + (1|site), data = d_std)
```

```{r}
tbl_regression(fit_1, tidy_fun = broom.mixed::tidy) %>%
   modify_footnote(ci ~ NA, abbreviation = TRUE)
```

The treatment and period effects are recovered quite well, as are the variance estimates of the random effects.

**Model 2: Hussey & Hughes extended to baseline measurement**

If we have access to both baseline and follow-up outcome measurements for each subject, we have a number of modeling options, four of which are described here. First, we can extend the Hussey & Hughes model to estimate the treatment effect. **The baseline and follow-up measurements are assumed to be collected in the same period $t$.** The baseline measurement is collected prior to randomization (so will by definition be under the control condition). The follow-up measurement will be under the control or treatment condition, depending on the cluster and time period.

$$
Y_{ijkt} = \alpha_t +  \gamma k Z_{jt}  + c_j + s_{ijt} + sp_{ijkt},
$$

where $Y_{ijkt}$ is outcome $k$ for subject $i$ in cluster $j$ measured in period $t$. $k = 0$ when the measurement is from baseline and $k=1$ when it is the follow-up. $Z_{jt}$ is a treatment indicator for cluster $j$ during period $t$. The parameters $\alpha_t$ and $\gamma$ are the same as above. The random effect $c_j$ is also unchanged from the previous model. The subject-level effects of this model *are* different, reflecting the fact that there are now two measurements per subject: $s_{ijt}$ is the overall subject-level effect for subject $i$ in cluster $j$ during period $t$, $s_{ijt} \sim N(0,\sigma_s^2)$ and  $sp_{ijkt}$ is the measurement noise (which assumes that any two measurements from the same individual will vary), $sp_{ijkt} \sim N(0,\sigma_{sp}^2)$.

Once again, the model estimates recover the true values pretty well:

```{r, message=FALSE, warning=FALSE}
fit_2 <- lmer(y ~ period + p_fu:Z  - 1 +  (1|id:site) + (1|site), data = dd)
```

```{r, echo=FALSE}
tbl_regression(fit_2, tidy_fun = broom.mixed::tidy) %>%
   modify_footnote(ci ~ NA, abbreviation = TRUE)
```

**Model 3: Teerenstra et al difference in change model**

In the previous [post](https://www.rdatagen.net/post/2021-11-23-design-effects-with-baseline-measurements/){target="_blank"}, I described a "difference in change" model proposed in a paper by [Teerenstra et al](https://onlinelibrary.wiley.com/doi/full/10.1002/sim.5352){target="_blank"} for parallel designs. This can be extended to the context of a stepped wedge design:

$$
Y_{ijkt} = \alpha_t + \gamma_0 k + \gamma_1 Z_{jt} + \gamma_2 k Z_{jt} + c_j + s_{ijt} + sp_{ijkt}
$$

This is essentially the same as *Model 2*, though this model makes slightly fewer assumptions. In particular, *Model 3* includes the additional parameters $\gamma_0$ and $\gamma_1$, where $\gamma_0$ is the change from baseline to follow-up in the control arm and $\gamma_{1}$ is the difference at baseline between control and treatment arms (we would expect this to be $0$ in a randomized trial). $\gamma_{2}$ is the difference in the change from baseline to follow-up between the two arms. In *Model 2*, we essentially make the assumption that both $\gamma_0 = 0$ and $\gamma_1=0$, which is probably true on average in a randomized trial with no secular trend between baseline and follow-up measurements (and is certainly true in our simulated data generation process). In this case, under the assumption of randomization and no within-period secular trend, $\gamma_2$ should be equivalent to $\gamma$ in *Model 2*.

```{r , message=FALSE, warning=FALSE}
fit_3 <- lmer(y ~  period + p_fu * Z - 1 + (1|id:site) + (1|site), data = dd)
```

```{r, echo=FALSE}
tbl_regression(fit_3, tidy_fun = broom.mixed::tidy) %>%
   modify_footnote(ci ~ NA, abbreviation = TRUE)
```

The notable difference between the the *Model 2* and *Model 3* estimates is that the width of the confidence interval for the treatment effect estimate is considerably larger in *Model 3*. We will see how this plays out in the power estimates down below.

**Model 4: ANCOVA**

The next model is an ANCOVA model that adjusts each follow-up outcome $Y_1$ with the observed baseline measurement $Y_0.$ Because we have only a single outcome per subject, the subject-specific effect $sp_{ijkt}$ disappears, and $s_{ijt} is now individual subject-level noise.

$$
Y_{ijt} = \alpha_t +  \beta Y_0 + \gamma Z_{jt}  + c_j + s_{ijt}
$$

```{r, message=FALSE, warning=FALSE}
dd_b <- dd[p_fu ==0, .(id, y0 = y)]
dd_f <- dd[p_fu ==1, ]

dd <- merge(dd_f, dd_b, by = "id")

fit_4 <- lmer(y ~  period +  y0 + Z  - 1 + (1|site), data = dd)
```

```{r, echo=FALSE}
tbl_regression(fit_4, tidy_fun = broom.mixed::tidy) %>%
   modify_footnote(ci ~ NA, abbreviation = TRUE)
```

**Model 5: Change as outcome**

The final model considers the change from baseline to follow-up ($D_{ijt} = Y_{ij1t} - Y_{ij0t}$) as the outcome of interest:

$$
D_{ijt} = \alpha_t + \gamma Z_{jt}  + c_j + s_{ijt}
$$

```{r, message=FALSE, warning=FALSE}
dd[, d := y - y0]

fit_5 <- lmer(d ~  period + Z - 1 + (1|site), data = dd)
```

```{r, echo=FALSE}
tbl_regression(fit_5, tidy_fun = broom.mixed::tidy) %>%
   modify_footnote(ci ~ NA, abbreviation = TRUE)
```

## Power estimates

The ultimate goal of simulating data under different scenarios is to compare how efficiently different modeling assumptions estimate the treatment effect. In *frequentist* terms, if we assume a specific effect size, which model will lead us to the "correct" inference most often? Or putting it more technically, which model has the most statistical power?

By repeatedly generate data sets, we can estimate the parameters using each of the models and compare the overall proportion of data sets where the observed *p-value* is less than 0.05 to evaluate the power of each modeling approach. We have generated data under a range of effect size and time trend assumptions, as well as under the different random effect assumptions to see if relative power changes across different sets of assumptions.

The curves in the figures show estimated power under a range of effect size (the columns of plots), time trends (the rows of plots) and sample size assumptions (the x-axes). First up is Scenario 1:

```{r, echo=FALSE, fig.height=2.5, fig.width=11, eval=TRUE}
load("data/res_np.rda")

res <- rbindlist(res)
res <- res[effect < 1.2]

dd <- melt(res, id.vars = c("effect", "trend", "sites", "npat","waves", "periods"), 
                measure.vars = c("pval_s", "pval_1", "pval_2", "pval_3", "pval_4"))

dd[, model := factor(variable, 
                     levels = c("pval_2", "pval_1", "pval_4", "pval_3","pval_s"),
                     labels = c("H&H (model 2)","Teerenstra (model 3)", 
                                "Change (model 5)", "ANCOVA (model 4)", 
                                "No baseline (model 1)")
              )
]

ggplot(data = dd, aes( y = value, x = sites)) +
  geom_hline(yintercept = 0.80, color = "grey99") +
  geom_line(aes(group = variable, color=model)) +
  scale_x_continuous(limits = c(10, 62), breaks = seq(12, 60, by = 8)) +
  scale_y_continuous(limits = c(0,1), breaks = seq(0.2, 0.8, by = 0.2)) +
  ylab("power") +
  theme(panel.grid = element_blank(),
        plot.title = element_text(size = 11, face = "bold"),
        legend.title = element_blank()) +
  facet_grid(trend ~ effect) +
  scale_colour_paletteer_d(palette = "wesanderson::Moonrise3") +
  ggtitle("Scenario 1: no period effects")

```

In the case without any fixed or random period effects, there really is no difference between *Models 3, 4*, and *5*; power appears to be consistent under the different scenarios. *Model 2*, the Hussey & Hughes model for baseline and follow-up measures, has the most power; this is not surprising, given that the model has an embedded set of simplifying assumptions relative to *Model 3* (see above). On the flip side, if those assumptions are violated, the *Model 3* estimates for the treatment effect may be biased, so it is not the obvious choice. On the other end of the spectrum, *Model 1* that ignores the baseline measures provides the least power; it seems pretty clear that if the two measurements are available, it would be unwise to throw out one of them.

The relative weakness of *Model 1* and relative strength (though potential bias) of *Model 2* persist in the remaining two scenarios (below), so the interesting question is how the remaining three models fare. In the second scenario, where there are fixed period effects but no random period effects, differences emerge. *Model 3*, the difference in change or Teerenstra model, appears to have more power than the ANCOVCA or change models across all effect sizes and secular trends.

```{r, echo=FALSE, fig.height = 4, fig.width=11}

load("data/res_p.rda")

res <- rbindlist(res)
res <- res[effect < 1.2]

dd <- melt(res, id.vars = c("effect", "trend", "sites", "npat","waves", "periods"),
                measure.vars = c("pval_s", "pval_1", "pval_2", "pval_3", "pval_4"))

dd[, model := factor(variable,
                     levels = c("pval_2", "pval_1", "pval_4", "pval_3","pval_s"),
                     labels = c("H&H (model 2)","Teerenstra (model 3)", 
                                "Change (model 5)", "ANCOVA (model 4)", 
                                "No baseline (model 1)")
              )
]

ggplot(data = dd, aes( y = value, x = sites)) +
  geom_hline(yintercept = 0.80, color = "grey99") +
  geom_line(aes(group = variable, color=model)) +
  scale_x_continuous(limits = c(10, 62), breaks = seq(12, 60, by = 8)) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0.2, 0.8, by = 0.2)) +
  ylab("power") +
  theme(panel.grid = element_blank(),
        plot.title = element_text(size = 11, face = "bold"),
        legend.title = element_blank()) +
  facet_grid(trend ~ effect) +
  scale_colour_paletteer_d(palette = "wesanderson::Moonrise3") +
  ggtitle("Scenario 2: fixed period effects only")

```

In Scenario 3, where we have both fixed and random period effects, the general ordering of the models appears similar to the ranking in Scenario 2. However, *Model 5*, the change model appears, surprisingly, slightly superior to *Model 4*, the ANCOVA model.

```{r, echo=FALSE, fig.height = 4, fig.width=11,eval=TRUE}
load("data/res_cp.rda")

res <- rbindlist(res)
res <- res[effect < 1.2]

dd <- melt(res, id.vars = c("effect", "trend", "sites", "npat","waves", "periods"), 
                measure.vars = c("pval_s", "pval_1", "pval_2", "pval_3", "pval_4"))

dd[, model := factor(variable, 
                     levels = c("pval_2", "pval_1", "pval_4", "pval_3","pval_s"),
                     labels = c("H&H (model 2)","Teerenstra (model 3)", 
                                "Change (model 5)", "ANCOVA (model 4)", 
                                "No baseline (model 1)")
              )
]

ggplot(data = dd, aes( y = value, x = sites)) +
  geom_hline(yintercept = 0.80, color = "grey99") +
  geom_line(aes(group = variable, color=model)) +
  scale_x_continuous(limits = c(10, 62), breaks = seq(12, 60, by = 8)) +
  scale_y_continuous(limits = c(0,1), breaks = seq(0.2, 0.8, by = 0.2)) +
  ylab("power") +
  theme(panel.grid = element_blank(),
        plot.title = element_text(size = 11, face = "bold"),
        legend.title = element_blank()) +
  facet_grid(trend ~ effect) +
  scale_colour_paletteer_d(palette = "wesanderson::Moonrise3") +
  ggtitle("Scenario 3: fixed and random period effects")

```

Back in the real world, of course, we do not know the true data generation process. It seems pretty clear that if we can make the assumptions that $\gamma_0$ and $\gamma_1 = 0$, then *Model 2* appears to be the way to go. However, if we want to be careful not to introduce  bias, *Model 3* might preferred. But, these findings are based on the specific data generating assumptions of these simulations. More general guidelines will require the formal derivation of design effects of the different models under the various scenarios. That is the next step.

<p><small><font color="darkkhaki">
References:

Hussey, Michael A., and James P. Hughes. "Design and analysis of stepped wedge cluster randomized trials." Contemporary clinical trials 28, no. 2 (2007): 182-191.

Teerenstra, Steven, Sandra Eldridge, Maud Graff, Esther de Hoop, and George F. Borm. "A simple sample size formula for analysis of covariance in cluster randomized trials." Statistics in medicine 31, no. 20 (2012): 2169-2178.

Support:

This work was supported in part by the National Institute on Aging (NIA) of the National Institutes of Health under Award Number U54AG063546, which funds the NIA IMbedded Pragmatic Alzheimerâ€™s Disease and AD-Related Dementias Clinical Trials Collaboratory ([NIA IMPACT Collaboratory](https://impactcollaboratory.org/)). The author, a member of the Design and Statistics Core, was the sole writer of this blog post and has no conflicts. The content is solely the responsibility of the author and does not necessarily represent the official views of the National Institutes of Health.
</font></small></p>


<a name="addendum"></a>  

\ 


## Addendum

### General data generation code: function `base_sw`

```{r, eval = FALSE}
base_sw <- function(effect, trend, nsites, nwaves, nperiods, n, s_c, s_cp, s_s, s_sp) {
  
  # define the cluster and subject level effects
  
  defC <- defData(varname = "c", formula = 0, variance = "..s_c")
  defCP <- defDataAdd(varname = "c.p", formula = 0, variance = "..s_cp")
  defS <- defDataAdd(varname = "s", formula = 0, variance = "..s_s")
  defSP <- defDataAdd(varname = "y",
    formula = "..trend * s_per + ..effect * Z * p_fu + c + c.p + s", 
    variance = "..s_sp"
  )
  
  # generate clusters
  
  dsite <- genData(nsites, defC, id = "site")
  
  # generate cluster-period data
  
  dper <- addPeriods(dsite, nPeriods = nperiods, idvars = "site", 
                     timeid = "s_time", perName = "s_per")
  dper <- addColumns(defCP, dper)
  
  # make treatment assignments
  
  dsw <- trtStepWedge(dper, "site", nWaves = nwaves, lenWaves = 1, 
                      startPer = 1, perName = "s_per",
                      grpName = "Z")
  
  # generate individual level data within each cluster
  
  dpat <- genCluster(dper, cLevelVar = "s_time", 
                     numIndsVar = n, level1ID = "id")
  dpat <- addColumns(defS, dpat)
  
  # generate two observation periods for each subject - baseline and follow-up
  
  dpat <- addPeriods(dpat, nPeriods=2, idvars="id", timeid="p_time", perName="p_fu")

  # merge subjects with stepped wedge assignments and generate outcome
  
  setkey(dpat, "s_time", "site", "s_per", "id")
  dsw[, c("c", "c.p") := NULL]
  setkey(dsw, "s_time", "site", "s_per")
  
  dpat <- merge(dpat, dsw)
  setkey(dpat, "id", "p_fu")
  dpat <- addColumns(defSP, dpat)
}
```


### Model 3 under Scenarios 1 and 3

The modeling examples above were based on Scenario 2 assumptions. Here is *Model 3* under Scenarios 1 and 3. (The other models would be described and implemented in similar fashion.)

#### Scenario 1

In Scenario 1, there are no fixed or random period effects, so there is a constant intercept $\alpha$ for each period, rather than period-specific intercepts $\alpha_t$, and there is no site-period random effect $cp_{jt}$:

$$
Y_{ijkt} = \alpha + \gamma_0 k + \gamma_1 Z_{jk} + \gamma_2 k Z_{jk} + c_j  + s_{ijt} + sp_{ijkt}
$$

```{r , message=FALSE, warning=FALSE}
dd <- base_sw(effect = 1, trend = 0, 20, 4, 5, 30, 6, 0, 44, 20)

fit_1a <- lmer(y ~  p_fu * Z + (1|id:site) + (1|site), data = dd)
```

```{r, echo=FALSE}
tbl_regression(fit_1a, tidy_fun = broom.mixed::tidy) %>%
   modify_footnote(ci ~ NA, abbreviation = TRUE)
```

#### Scenario 3

Scenario 3 includes both fixed and random period effects, so we have $\alpha_t$ and $cp_{jt}$.

$$
Y_{ijkt} = \alpha_t + \gamma_0 k + \gamma_1 Z_{jk} + \gamma_2 k Z_{jk} + c_j  + cp_{jt} + s_{ijt} + sp_{ijkt}
$$

```{r , message=FALSE, warning=FALSE}
dd <- base_sw(effect = 1, trend = 3, 20, 4, 5, 30, 4, 2, 44, 20)
dd[, period := factor(s_per, labels = c(1, 2, 3, 4, 5))]

fit_2a <- lmer(y ~  period + p_fu * Z - 1 + (1|id:site) + (1|s_time:site) + (1|site), 
  data = dd)
```

```{r, echo = FALSE} 
tbl_regression(fit_2a, tidy_fun = broom.mixed::tidy) %>%
   modify_footnote(ci ~ NA, abbreviation = TRUE)
```

<br>