---
title: Accounting for ties in a Bayesian proportional hazards model
author: Package Build
date: '2025-03-18'
slug: []
categories: []
tags:
  - R
  - survival analysis
  - Bayesian analysis
type: ''
subtitle: ''
image: ''
draft: TRUE
---



<p>Over my past few posts, I’ve been progressively building towards a Bayesian model for a stepped-wedge cluster randomized trial with a time-to-event outcome, where time will be modeled using a spline function. I started with a simple Cox proportional hazards model for a traditional RCT, ignoring time as a factor. In the next post, I introduced a nonlinear time effect. For the third post—one I initially thought was ready to publish—I extended the model to a cluster randomized trial without explicitly incorporating time. I was then working on the grand finale, the full model, when I ran into an issue: I couldn’t recover the effect-size parameter used to generate the data.</p>
<p>After an embarrassingly long debugging process, I finally realized the problem—many events shared the same event times, and my model failed to account for ties. This issue hadn’t been apparent in the earlier models, but the final version was particularly sensitive to it. So, I decided to step back and first implement a model that properly handles ties before moving ahead.</p>
<div id="whats-the-issue" class="section level3">
<h3>What’s the issue?</h3>
<p>The fundamental issue is that the likelihood in the original model assumes event times are unique for each individual. This assumption is reasonable when time is measured in hours but becomes problematic when using days and even more so with weeks, especially if the study covers a broad time range.</p>
<p>When multiple individuals experience an event at the same recorded time (ties), the challenge is defining the appropriate “risk set”—the group still at risk at that time. Two commonly used approaches for handling ties are the <em>Breslow</em> and <em>Efron</em> methods.</p>
<ul>
<li><p>The <em>Breslow</em> method takes a simple approach by assuming that all tied events share the same risk set. It treats them as if they happened sequentially but applies the same risk set to each event. This can work well when ties are rare but may introduce bias if they are frequent.</p></li>
<li><p>The <em>Efron</em> method refines this by adjusting the risk set dynamically. Instead of treating all tied events as occurring with full risk sets, it reduces the risk set incrementally as each event happens. This better approximates a scenario where events truly occur in close succession rather than simultaneously.</p></li>
</ul>
<p>In practical terms, the <em>Efron</em> method provides a more accurate correction when ties are common, while <em>Breslow</em> remains a computationally simpler choice. Another option is the Exact method, which calculates the likelihood by considering all possible orderings of tied events. While this approach is the most precise, it is often computationally impractical for large datasets. Much of this is described nicely in
<a href="https://www.jstor.org/stable/2533573" target="_blank">Hertz-Picciotto and Rockhill</a>, though the original methods are detailed by <a href="https://www.tandfonline.com/doi/abs/10.1080/01621459.1977.10480613" target="_blank">Efron</a> and
<a href="https://www.jstor.org/stable/2529620" target="_blank">Breslow</a>.
Finally, these <a href="https://www.google.com/url?sa=t&amp;source=web&amp;rct=j&amp;opi=89978449&amp;url=https://myweb.uiowa.edu/pbreheny/7210/f15/notes/11-5.pdf" target="_blank">lecture notes</a> by Patrick Breheny provide a nice explanation of algorithms for handling tied survival times.</p>
</div>
<div id="implementing-the-efron-method" class="section level3">
<h3>Implementing the Efron method</h3>
<p>Since <em>Efron</em> method generally provides better estimates, I chose to incorporate this into the Bayesian model. The partial likelihood under this approach is</p>
<p><span class="math display">\[
\log L(\beta) = \sum_{j=1}^{J} \left[ \sum_{i \in D_j} x_i^\top \beta - \sum_{r=0}^{d_j-1} \log \left( \sum_{k \in R_j} \exp(x_k^\top \beta) - r \cdot \bar{w} \right) \right] \\
\]</span></p>
<ul>
<li><span class="math inline">\(D_j\)</span> is the set of individuals who experience an event at time <span class="math inline">\(t_j\)</span>.</li>
<li><span class="math inline">\(R_j\)</span> is the risk set at time <span class="math inline">\(t_j\)</span>, including all individuals who are still at risk at that time.</li>
<li><span class="math inline">\(d_j\)</span> is the number of events occurring at time <span class="math inline">\(t_j\)</span>.</li>
<li><span class="math inline">\(r\)</span> ranges from 0 to <span class="math inline">\(d_j - 1\)</span>, iterating over the tied events.</li>
<li><span class="math inline">\(\bar{w}\)</span> represents the average risk weight of individuals experiencing an event at <span class="math inline">\(t_j\)</span>:</li>
</ul>
<p><span class="math display">\[\bar{w} = \frac{1}{d_j} \sum_{i \in D_j} \exp(x_i^\top \beta)\]</span></p>
<p>The key idea here is that instead of treating all tied events as occurring simultaneously with the full risk set (as in Breslow’s method), <em>Efron</em> gradually reduces the risk set as each tied event is considered. This provides a more refined approximation of the true likelihood, particularly when ties are common.</p>
<p>In case a simple numerical example is helpful, here is a toy example of 15 individuals, where three share a common event time of 14 days (show as <span class="math inline">\(Y\)</span>), and two share an event time of 25 days. The critical feature of the <em>Efron</em> correction is that <span class="math inline">\(\bar{w}\)</span> is computed by averaging <span class="math inline">\(\text{exp}(\theta)\)</span> across the group experiencing the event at a given time.</p>
<p>Initially, the cumulative sum of <span class="math inline">\(\text{exp}(\theta)\)</span> for each individual in the group is identical. However, as each event in the tied group is processed sequentially, the risk set is dynamically updated, and the contribution from individuals who have already been accounted for is gradually reduced. This is reflected in the term <span class="math inline">\(Z = U-V\)</span>, where <span class="math inline">\(U\)</span> represents the initial total risk weight and <span class="math inline">\(V\)</span> accounts for the incremental reduction as ties are processed.</p>
<p><img src="figures/efron.png" width="90%" /></p>
<p><br></p>
</div>
<div id="simulating-data" class="section level3">
<h3>Simulating data</h3>
<p>To generate data for testing the Bayesian model, I am simulating an RCT with 1,000 individuals randomized 1:1 to one of two groups (represented by <span class="math inline">\(A\)</span>). The treatment effect is defined by the hazard ratio <span class="math inline">\(\delta_f\)</span>. The data includes censoring and ties. We start with loading the necessary libraries:</p>
<pre class="r"><code>library(simstudy)
library(data.table)
library(survival)
library(survminer)
library(cmdstanr)</code></pre>
<p>Here is the simulation - definitions, parameters, and the data generation. The Kaplan-Meier plot shows the two arms - red is intervention, and green is control, and the black crosses are the censoring times.</p>
<pre class="r"><code>#### Data definitions

def &lt;- defData(varname = &quot;A&quot;, formula = &quot;1;1&quot;, dist = &quot;trtAssign&quot;)

defS &lt;-
  defSurv(
    varname = &quot;timeEvent&quot;,
    formula = &quot;-11.6 + ..delta_f * A&quot;,
    shape = 0.30
  )  |&gt;
  defSurv(varname = &quot;censorTime&quot;, formula = -11.3, shape = .35)

#### Parameters

delta_f &lt;- log(1.5)

#### Generate single data set

set.seed(7398)

dd &lt;- genData(1000, def)
dd &lt;- genSurv(dd, defS, timeName = &quot;Y&quot;, censorName = &quot;censorTime&quot;, digits = 0,
              eventName = &quot;event&quot;, typeName = &quot;eventType&quot;, keepEvents  = TRUE)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/km_plot-1.png" width="576" /></p>
<p>Of the 48 unique event times, 44 are shared by multiple individuals. This histogram shows the number of events at each time point:</p>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/tied_plot-1.png" width="576" /></p>
</div>
<div id="stan-model" class="section level3">
<h3>Stan model</h3>
<p>The Stan code below extends the model to analyze survival data that I presented <a href="https://www.rdatagen.net/post/2025-02-11-estimating-a-bayesian-proportional-hazards-model/" target="_blank">earlier</a>, so much of the model remains the same. I’ve removed the binary search function that appeared in the earlier model, replacing it with an index field that is passed from <code>R</code>. There are additional data requirements that must also be sent calculated in <code>R</code> to provide information about the ties. The model does not have additional parameters. The adjustment of the likelihood for the ties takes place in the “model” block.</p>
<p>This Stan code extends the survival model I described <a href="https://www.rdatagen.net/post/2025-02-11-estimating-a-bayesian-proportional-hazards-model/" target="_blank">earlier</a>, with much of the structure remaining unchanged. The binary search function from the previous model has been removed and replaced with an index field passed from <code>R</code>. New data fields related to ties are also required, which must be calculated in <code>R</code> and provided to the model. There are no additional parameters, and the likelihood adjustment for ties is handled in the “model” block.</p>
<pre class="r"><code>stan_code &lt;-
&quot;
data {

  int&lt;lower=0&gt; N_o;        // Number of uncensored observations
  array[N_o] int i_o;      // Index in data set

  int&lt;lower=0&gt; N;          // Number of total observations
  vector[N] x;             // Covariates for all observations
  
  array[N] int index;
  
  int&lt;lower=0&gt; T;            // Number of records as ties
  int&lt;lower=1&gt; G;            // Number of groups of ties
  array[T] int t_grp;        // Indicating tie group
  array[T] int t_index;      // Index in data set
  vector[T] t_adj;           // Adjustment for ties (efron)
}

parameters {
  
  real beta;          // Fixed effects for covariates

}

model {
  
  // Prior
  
  beta ~ normal(0, 4);
  
  // Calculate theta for each observation to be used in likelihood
  
  vector[N] theta;
  vector[N] log_sum_exp_theta;
  vector[G] exp_theta_grp = rep_vector(0, G);
  
  int first_in_grp;
  
  for (i in 1:N) {
    theta[i] = x[i] * beta;  
  }

  // Computing cumulative sum of log(exp(theta)) from last to first observation
  
  log_sum_exp_theta[N] = theta[N];
  
  for (i in tail(sort_indices_desc(index), N-1)) {
    log_sum_exp_theta[i] = log_sum_exp(theta[i], log_sum_exp_theta[i + 1]);
  }
  
  // Efron algorithm - adjusting cumulative sum for ties
  
  for (i in 1:T) {
    exp_theta_grp[t_grp[i]] += exp(theta[t_index[i]]);
  }

  for (i in 1:T) {
  
    if (t_adj[i] == 0) {
      first_in_grp = t_index[i];
    }

    log_sum_exp_theta[t_index[i]] =
      log( exp(log_sum_exp_theta[first_in_grp]) - t_adj[i] * exp_theta_grp[t_grp[i]]);
  }
  
  // Likelihood for uncensored observations

  for (n_o in 1:N_o) {
    target += theta[i_o[n_o]] - log_sum_exp_theta[i_o[n_o]];
  }
  
}

generated quantities {
  real exp_beta = exp(beta);
}
&quot;</code></pre>
<div id="compiling-the-model" class="section level4">
<h4>Compiling the model</h4>
<pre class="r"><code>stan_model &lt;- cmdstan_model(write_stan_file(stan_code))</code></pre>
</div>
<div id="preparing-the-data-for-stan" class="section level4">
<h4>Preparing the data for Stan</h4>
<pre class="r"><code>dx &lt;- copy(dd)
setorder(dx, Y)
dx[, index := .I]

dx.obs &lt;- dx[event == 1]
N_obs &lt;- dx.obs[, .N]
i_obs &lt;- dx.obs[, index]

N_all &lt;- dx[, .N]
x_all &lt;- dx[, A]

ties &lt;- dx[, .N, keyby = Y][N&gt;1, .(grp = .I, Y)]
ties &lt;- merge(ties, dx, by = &quot;Y&quot;)
ties &lt;- ties[, order := 1:.N, keyby = grp][, .(grp, index)]
ties[, adj := 0:(.N-1)/.N, keyby = grp]

stan_data &lt;- list(
  N_o = N_obs,
  i_o = i_obs,
  N = N_all,
  x = x_all,
  index = dx$index,
  T = nrow(ties),
  G = max(ties$grp),
  t_grp = ties$grp,
  t_index = ties$index,
  t_adj = ties$adj
)</code></pre>
</div>
<div id="fitting-models" class="section level4">
<h4>Fitting models</h4>
<p>First, I am estimating the Bayesian model. To save time, I’m using <code>$optimize()</code> to obtain the MLE for <code>beta</code> from the Stan model rather than using MCMC to sample the full posterior distribution. The estimated hazard ratio is right on target:</p>
<pre class="r"><code>fit_mle &lt;- stan_model$optimize(data=stan_data, jacobian = FALSE)
fit_mle$draws(format=&quot;df&quot;)[1,c(&quot;beta&quot;, &quot;exp_beta&quot;)]</code></pre>
<pre><code>## # A tibble: 1 × 2
##    beta exp_beta
##   &lt;dbl&gt;    &lt;dbl&gt;
## 1 0.401     1.49</code></pre>
<p>For comparison, I fit a Cox proportional hazards model using a frequentist (non-Bayesian) approach. The log hazard ratio estimate matches that of the Bayesian model.</p>
<pre class="r"><code>cox_model &lt;- coxph(Surv(Y, event) ~ A , data = dd, ties = &quot;efron&quot;)</code></pre>
<pre><code>## # A tibble: 1 × 5
##   term  estimate std.error statistic      p.value
##   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;
## 1 A        0.401    0.0715      5.62 0.0000000194</code></pre>
<p>Of course, I really should conduct more extensive simulations to better understand the operating characteristics of the Bayesian model, particularly to assess how estimates behave when the ties are ignored. However, I’m eager to get back to the original program, moving next to a random effects model, and then completing the full model by combining the random effect with the spline. I’ll leave these additional simulations for you to explore.</p>
<p>
<p><small><font color="darkkhaki">
References:</p>
<p>Breslow, N., 1974. Covariance analysis of censored survival data. Biometrics, pp.89-99.</p>
<p>Efron, B., 1977. The efficiency of Cox’s likelihood function for censored data. Journal of the American statistical Association, 72(359), pp.557-565.</p>
<p>Hertz-Picciotto, I. and Rockhill, B., 1997. Validity and efficiency of approximation methods for tied survival times in Cox regression. Biometrics, pp.1151-1156.</p>
</font></small>
</p>
</div>
</div>
