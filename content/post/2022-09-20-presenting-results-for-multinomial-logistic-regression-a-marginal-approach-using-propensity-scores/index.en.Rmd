---
title: 'Presenting results for multinomial logistic regression: a marginal approach using propensity scores'
author: Package Build
date: '2022-09-20'
slug: []
categories: []
tags:
  - R
type: ''
subtitle: ''
image: ''
draft: true
---

<style>

table {
    width: 100%;
}

tr { 
    font-family: "Courier New";
    font-size: .7em;
    color: blue;
    height: 10%; 
}

</style>

```{r, echo=FALSE}
options(digits = 3)
```

Multinomial logistic regression modeling can provide an understanding of the factors influencing an unordered, categorical outcome. For example, if we are interested in identifying individual-level characteristics associated with political parties in the United States (*Democratic*, *Republican*, *Libertarian*, *Green*), a multinomial model would be a reasonable approach to for estimating the strength of the associations. In the case of a randomized trial or epidemiological study, we might be primarily interested in the effect of a specific intervention or exposure while controlling for other covariates. Unfortunately, interpreting results from a multinomial logistic model can be a bit of a challenge, particularly when there is a large number of possible responses and covariates. 

My goal here is to generate a data set to illustrate how difficult it might be to interpret the parameter estimates from a multinomial model. And then I lay out a relatively simple solution that allows us to easily convert from the odds scale to the probability scale so we can more easily see the effect of the exposure on the outcome.

### The multinomial logistic model

This is the formal specification of the model:

$$log\left[\frac{P(P_i = y)}{P(P_i = ref)}\right] = \alpha_y + \gamma_y A_i+ \boldsymbol{X_i \beta_y}$$

where $P_i$ is the political party of individual $i$, $P_i \in \{R, L, G\}$ and *ref* is the reference party - we'll set that to be $D$ in this case. $A_i$ is a treatment/exposure indicator, and is 1 if exposed, 0 otherwise. (Say there is a particular ad campaign that we are assessing.)
$\boldsymbol{X_i}$ is a matrix of covariates. 

The complexity of the model starts to emerge as we consider the parameters, which are all specific to the level of $y$. $\alpha_y$ is the intercept for the log odds of $y$ vs. the reference category (so $\alpha_y$ represents the log odds for the unexposed with average covariate values, assuming all covariates have been centered at zero). $\gamma_y$ is the log odds ratio comparing the odds of $y$ vs. $D$ for the exposed and unexposed. $\boldsymbol{\beta_y}$ is a vector of parameters that reflect the association of the covariates with the party choice.

If we have $K$ possible categories (in this example $K = 4$), there are $K-1$ sets of parameters. That is challenging enough. But each of those parameters is in relation to the reference category, so if you want to compare the odds across two non-reference categories, it can be a little challenging.

### Simulating the data

Before getting started, here are the necessary R packages for everything we are doing here.

```{r}
library(simstudy)
library(data.table)
library(ggplot2)
library(nnet)
library(MatchIt)
```

#### Definitions

In this simulation, I am generating a categorical outcome that has four levels. There is a single exposure ($A$) and two covariates ($x_1$ and $x_2$). Six time periods worth of data are being generated, The probability of *exposure* ($A$) depends on both covariates and time. The outcome is associated with the covariates, and the effect of the intervention *changes* over time, complicating matters.

First, I define the effects of time on both exposure and the outcome (*defT*). Then I define covariates $x_1$ and $x_2$, exposure $A$, and outcome $Y$ (*defY*).

```{r}
trunc_norm <- function(mean, sd, lower, upper) {
  msm::rtnorm(n = 1, mean = mean, sd = sd, lower = lower, upper = upper)
}

defT <- defData(varname = "a_j", 
  formula = "trunc_norm(mean = 0, sd = 0.4, 
  lower = -Inf, upper = 0)", dist = "nonrandom")
defT <- defData(defT, varname = "b_j", 
  formula = "trunc_norm(mean = 0, sd = 0.4, 
  lower = -Inf, upper = 0)", dist = "nonrandom")

defY <- defDataAdd(varname = "x1", formula = 0, variance = 1)
defY <- defDataAdd(defY, varname = "x2", formula = 0.5, dist = "binary")
defY <- defDataAdd(defY, varname = "A", formula = "-1 + a_j - .5*x1 + .6*x2", 
  dist = "binary", link = "logit")
defY <- defDataAdd(defY, varname = "y", 
  formula = "b_j - 1.3 + 0.1*A - 0.3*x1 - 0.5*x2 + .55*A*period;
             b_j - 0.6 + 1.4*A + 0.2*x1 - 0.5*x2;
             -0.3 - 0.3*A - 0.3*x1 - 0.5*x2 ", 
  dist = "categorical", link = "logit")
```

#### Data generation

In the data generation step, we assume six periods and 200 individuals in each period:

```{r}

set.seed(999)

dd <- genData(6, defT, id = "period")
dd <- genCluster(dd, "period", 200, level1ID = "id")

dd <- addColumns(defY, dd)
dd <- genFactor(dd, "y")

dd[, fy := relevel(fy, ref = "4")]
dd[, period := factor(period)]
```

Here are a few rows of the data set:

```{r, echo=FALSE}
dd[, .(period, id, A, x1, x2, fy)]
```

Here are a two figures that allow us to visualize the relationship of the exposure $A$ with the covariates and time:

```{r exp_plots, echo=FALSE, fig.width = 6, fig.height = 2.5}
dsum <- dd[, .(p_rx = mean(A)), keyby = .(period, x2)]

ggplot(data = dsum, aes(x = period, y = p_rx)) +
  geom_line(aes(group = x2, color = factor(x2))) +
  geom_point(aes(color = factor(x2)), size = .6) +
  theme(panel.grid = element_blank())  +
  scale_color_manual(values = c("red", "black"), name = "x2", 
                     guide = guide_legend(reverse = TRUE) ) +
  ylab("P(A = 1)") 
  
dd[, fx1 := cut(x1, 
  breaks = quantile(x1, prob = seq(0.10, 1, by = .1)), 
  include.lowest=T, labels = F)]
dd[is.na(fx1), fx1 := 0]
dsum <- dd[, .(p_rx = mean(A)), keyby = .(fx1, x2)]

ggplot(data = dsum, aes(x = fx1, y = p_rx)) +
  geom_line(aes(group = x2, color = factor(x2))) +
  geom_point(aes(color = factor(x2)), size = .6) +
  theme(panel.grid = element_blank()) +
  scale_color_manual(values = c("red", "black"), name = "x2",
                     guide = guide_legend(reverse = TRUE) ) +
  xlab("x1") +
  ylab("P(A = 1)")
```

Typically, I would like to plot the raw outcome data to get an initial sense of the how the outcome relates to the covariates of interest, but with a categorical measure that has four levels, it is not so obvious how to present the data in an insightful way. At the very least, we can show the distributions of the outcome over time and exposure, without taking into account the covariates (so there might be some confounding inherent in the plots):

```{r out_plot, echo=FALSE, fig.width = 9, fig.height = 2.5}
dsum <- dd[, .N, keyby = .(fy, A, period)]
dsum[, p := N/sum(N), keyby = .(period, A)]
dsum[, response := factor(fy, levels = c(1, 2, 3, 4))]
dsum[, A := factor(A, labels = c("not exposed", "exposed"))]

ggplot(data = dsum, aes(x = period,  group = A)) +
  geom_line(aes(y = p, color = A), size = 0.6) +
  facet_grid(. ~ response, labeller = label_both) +
  theme(panel.grid = element_blank(),
        legend.title = element_blank()) +
  scale_color_manual(values = c("#806cc6", "#b2c66c")) +
  ylab("proportion") 
```

### "Traditional" analysis

If we do suspect there might be confounding due to covariates $x_1$ and $x_2$, and we see that there appears to be different effects of exposure over time, it would not be unreasonable to estimate a multinomial model that adjusts for both covariates and includes an interaction term for exposure and period.

```{r}
fit <- multinom(fy ~ x1 + x2 + A*period, data = dd, trace = FALSE)
```

#### Interpreting the results

The parameter estimates for a multinomial model are shown in the table below. In this case, the fourth response category is the reference, so the table shows the odds ratios for each response relative to the reference (the intercepts $\alpha_y$'s are not shown in the table). Each section of the table (labeled "1", "2", and "3") represent the estimated parameters for each response level. While some readers may be able to get a lot out of this table, I find it a little overwhelming, particularly when it comes to understanding (a) the impact of time on the exposure effect, and (b) how responses other than the reference category compare to each other.

```{r, echo=FALSE, results='asis', warning=FALSE, message=FALSE}

library(magrittr)

multinom_pivot_wider <- function(x) {
  
  # create tibble of results
  df <- tibble::tibble(outcome_level = unique(x$table_body$groupname_col))
  df$tbl <- 
    purrr::map(
      df$outcome_level,
      function(lvl) {
        gtsummary::modify_table_body(
          x, 
          ~dplyr::filter(.x, .data$groupname_col %in% lvl) %>%
            dplyr::ungroup() %>%
            dplyr::select(-.data$groupname_col)
        )
      }
    )
  
  gtsummary::tbl_merge(df$tbl, tab_spanner = paste0("**", df$outcome_level, "**"))
}

gtsummary::tbl_regression(fit, exponentiate = TRUE) %>% multinom_pivot_wider()

```

#### Probability scale

What I really want is to be able to see the estimates on the probability scale. This is always challenging because we can only get predicted probabilities at *specific* levels of the covariates (i.e. the model is *conditional*), and it is not clear what levels of the covariates we should use for this purpose. While we could just use the average value of each covariate to generate an average probability for each exposure group and each time period, there is something arbitrary to doing this. 

Perhaps, a somewhat more palatable approach is get estimates of the *marginal* probabilities. A while ago, I [presented an approach](https://www.rdatagen.net/post/2021-06-15-estimating-a-risk-difference-using-logistic-regression/){target="_blank"} in the context of logistic regression (which is just a special case of multinomial regression, where the categorical outcome has only two levels) that estimated a predicted probability for each individual under each treatment arm, and then calculated an average risk difference by averaging across all the patients. This could presumably work here, but I decided to try another approach that eliminates the covariates from the analysis by using propensity score matching.

### Propensity score matching

I've described propensity score matching in an earlier [post](https://www.rdatagen.net/post/different-models-estimate-different-causal-effects-part-ii/){target="_blank"} (and provided at least one good reference there), so I won't go into much detail here. The general idea is that we can estimate a probability of exposure (i.e., the propensity score), and then match individuals in the two exposure groups based on those scores. If done well, this creates two comparable groups that are balanced with respect to the confounders (assuming all the confounders have been measured and are included in the exposure model for the propensity score). Once the matching is done, we can estimate the multinomial model without any covariates - and convert to predicted probabilities without relaying on any assumptions about the covariates.

In the first step, I am matching individuals within each time period This way, the groups will be balanced at each time point, and I could estimate marginal probabilities for each period.

```{r}
matchby <- function(dx) {
  
  m <- matchit(A ~ x1 + x2, data = dx,
        method = "nearest", distance = "glm", caliper = .25)
  match.data(m)
  
  }

m.out <- rbindlist(lapply(1:6, function(x) matchby(dd[period==x])))
```

#### Analysis of matched data

In the second step, I fit a multinomial model that includes only exposure and time, and then generate predicted probability for each exposure and period combination.

```{r}
mfit <- multinom(fy ~ A*period, data = m.out, trace = FALSE)

dnew <- data.table(expand.grid(A = c(0,1), period = factor(c(1:6))))
dpred <- data.table(predict(mfit, newdata = dnew, "probs"))

dpred <- cbind(dnew, dpred)
dplot <- melt(data = dpred, id.vars = c("A", "period"), 
              value.name = "proportion", variable.name = "response")
dplot[, response := factor(response, levels = c(1, 2, 3, 4))]
dplot[, A := factor(A, labels = c("not exposed", "exposed"))]
```

Here are the predicted probabilities for second time period:

```{r, echo = FALSE}
setkey(dplot, "response")
dplot[period == 2]
```

#### Bootstrap estimates of confidence bands

To go along with our point estimates, we need a measure of uncertainty, which we will estimate by bootstrap. For this analysis, I am bootstrapping the whole process, starting by sampling with replacement within each period and each exposure group, doing the matching, fitting the model, and generating the predictions.

```{r}
bs <- function(x) {
  
  ids <- dd[, .(id = sample(id, .N, replace = T)), keyby = .(period, A)][, id]
  db <- dd[ids]
  db[, id := .I]
  
  mb.out <- rbindlist(lapply(1:6, function(x) matchby(db[period==x])))
  
  mfit <- multinom(fy ~ A*period, data = mb.out, trace = FALSE)
  
  dbnew <- data.table(expand.grid(A = c(0,1), period = factor(c(1:6))))
  dbpred <- data.table(predict(mfit, newdata = dbnew, "probs"))
  
  cbind(iter = x, dnew, dbpred)
}

bspred <- rbindlist(lapply(1:500, function(x) bs(x)))

```

#### Plot point estimates and confidence bands

What follows is the code to generate the figure showing the predicted probabilities for each arm. But before creating the plot, I've extracted 95% confidence intervals for each response level and period from the bootstrap data that will be used to draw the confidence bands.

```{r final_plot, fig.height=2.5, fig.width = 9}
bsplot <- melt(data = bspred, id.vars = c("iter", "A", "period"), 
  value.name = "proportion", variable.name = "response")
bsplot[, response := factor(response, levels = c(1, 2, 3, 4))]
bsplot[, A := factor(A, labels = c("not exposed", "exposed"))]

ci <- bsplot[, 
  .(l95 = quantile(proportion, 0.025), u95 = quantile(proportion, 0.975)), 
  keyby = .(response, A, period)
]

ggplot(data = dplot, aes(x = period,  group = A)) +
  geom_ribbon(data = ci, 
              aes(ymin = l95, ymax = u95, fill = A),
              alpha = .2)  +
  geom_line(aes(y = proportion, color = A), size = .8) +
  facet_grid(. ~ response, labeller = label_both) +
  theme(panel.grid = element_blank(),
        legend.title = element_blank()) +
  scale_color_manual(values = c("#806cc6", "#b2c66c")) +
  scale_fill_manual(values = c("#806cc6", "#b2c66c"))
```

The point estimates mirror the marginal raw data plots quite closely, which is hardly surprising since we treated time as a categorical variable and the model is saturated. The benefit of doing the modeling is that we have generated estimates of uncertainty, and are in a position to make some inferences. For example, it looks like the exposure has an increasing effect on the probability of a level "1" response in the last three periods. Likewise, the effect of the exposure on the probability of a level "2" response was strongest in the first two periods, and then disappears. And there is too much uncertainty to say anything definitive about level "3" and "4" responses.
