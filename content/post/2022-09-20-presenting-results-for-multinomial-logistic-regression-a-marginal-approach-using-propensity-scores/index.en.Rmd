---
title: 'Presenting results for multinomial logistic regression: a marginal approach
  using propensity scores'
author: Package Build
date: '2022-09-20'
slug: []
categories: []
tags:
  - R
type: ''
subtitle: ''
image: ''
draft: true
---

<style>

table {
    width: 100%;
}

tr { 
    font-family: "Courier New";
    font-size: .7em;
    color: blue;
    height: 10%; 
}

</style>

```{r, echo=FALSE}
options(digits = 3)
```

Multinational logistic regression modeling can provide an understanding of the factors influencing a response that is an unordered, categorical outcome measure. For example, if we are interested in identifying individual-level characteristics associated with political parties in the United States (*Democratic*, *Republican*, *Libertarian*, *Green*), a multinomial model would be a reasonable approach to take. In the case of a randomized trial or epidemiological study, we might be particularly interested in the effect of a specific intervention or exposure.

Interpreting results from a multinomial logistic model can be a bit of a challenge, particularly when there is a large number of possible responses and covariates. This is the formal specification of the model:

$$log\left[\frac{P(P_i = y)}{P(P_i = ref)}\right] = \alpha_y + \gamma_y A_i+ \boldsymbol{X_i \beta_y}$$

where $P_i$ is the political party of individual $i$, $P_i \in \{R, L, G\}$ and $ref$ is the reference party - we'll assume that to be $D$ here. $A_i$ is a treatment/exposure indicator, and is 1 if exposed, 0 otherwise. $\boldsymbol{X_i}$ is a matrix of covariates. (For example, say we have some ad campaign that we hope will influence party affiliation - good luck with that.)

We can see the complexity of the model immediately by noticing that the parameters are specific to the level of $y$. $\alpha_y$ is intercept for the log odds of $y$ vs. reference (so $\alpha_y$ represents the log odds for the unexposed with covariates at the average, assuming all covariates have been centered). $\gamma_y$ is the log odds ratio comparing the odds of $y$ vs. $D$ for the exposed and unexposed. $\boldsymbol{\beta_y}$ is a vector of parameters that reflect the association of the covariates with the party choice.

If we have $K$ possible categories (in this example $K = 4$), there are $K-1$ sets of parameters. That is challenging enough. But each of those parameters is in relation to the reference category, so if you want to compare the odds across two non-reference categories, things get a little more difficult.

My goal here is to generate a data set to illustrate how difficult it might be to interpret the parameter estimates from a multinomial model. And then I have a relatively simple solution that allows us to easily convert from the odds scale to the probability scale so we can literally see the effect of the exposure on the outcome.

### Simulating the data

Before getting started, here are the necessary R packages for everything we are doing here.

```{r}
library(simstudy)
library(data.table)
library(ggplot2)
library(nnet)
library(MatchIt)
```

#### Definitions

In this simulation, I am generating a categorical outcome that has four levels. There is a single exposure ($A$) and two covariates ($x_1$ and $x_2$). Six years worth of data are being generated, The probability of *exposure* ($A$) depends on both covariates and time. The outcome is associated with the covariates, and the effect of the intervention *changes* over time, complicating matters.

First, I define the effects of time on both exposure and the outcome. Then I generate covariates $x_1$ and $x_2$, exposure $A$, and outcome $Y$.

```{r}
trunc_norm <- function(mean, sd, lower, upper) {
  msm::rtnorm(n = 1, mean = mean, sd = sd, lower = lower, upper = upper)
}

defT <- defData(varname = "a_j", 
  formula = "trunc_norm(mean = 0, sd = 0.4, 
  lower = -Inf, upper = 0)", dist = "nonrandom")
defT <- defData(defT, varname = "b_j", 
  formula = "trunc_norm(mean = 0, sd = 0.4, 
  lower = -Inf, upper = 0)", dist = "nonrandom")

defY <- defDataAdd(varname = "x1", formula = 0, variance = 1)
defY <- defDataAdd(defY, varname = "x2", formula = 0.5, dist = "binary")
defY <- defDataAdd(defY, varname = "A", formula = "-1 + a_j - .5*x1 + .6*x2", 
  dist = "binary", link = "logit")
defY <- defDataAdd(defY, varname = "y", 
  formula = "b_j - 1.3 + 0.1*A - 0.3*x1 - 0.5*x2 + .55*A*year;
             b_j - 0.6 + 1.4*A + 0.2*x1 - 0.5*x2;
             -0.3 - 0.3*A - 0.3*x1 - 0.5*x2 ", 
  dist = "categorical", link = "logit")
```

#### Data generation

In the data generation step, we assume six years and 200 individuals in each year:

```{r}

set.seed(999)

dd <- genData(6, defT, id = "year")
dd <- genCluster(dd, "year", 200, level1ID = "id")

dd <- addColumns(defY, dd)
dd <- genFactor(dd, "y")

dd[, fy := relevel(fy, ref = "4")]
dd[, year := factor(year)]
```

Here is what the dataset looks like:

```{r, echo=FALSE}
dd[, .(year, id, A, x1, x2, fy)]
```

Here are a couple of figures that allow us to visualize the relationship of the exposure $A$ with the covariates and time:

```{r, echo=FALSE, fig.width = 6, fig.height = 2.5}
dsum <- dd[, .(p_rx = mean(A)), keyby = .(year, x2)]

ggplot(data = dsum, aes(x = year, y = p_rx)) +
  geom_line(aes(group = x2, color = factor(x2))) +
  geom_point(aes(color = factor(x2)), size = .6) +
  theme(panel.grid = element_blank())  +
  scale_color_manual(values = c("red", "black"), name = "x2", 
                     guide = guide_legend(reverse = TRUE) ) +
  ylab("P(A = 1)") 
  
dd[, fx1 := cut(x1, 
  breaks = quantile(x1, prob = seq(0.10, 1, by = .1)), 
  include.lowest=T, labels = F)]
dd[is.na(fx1), fx1 := 0]
dsum <- dd[, .(p_rx = mean(A)), keyby = .(fx1, x2)]

ggplot(data = dsum, aes(x = fx1, y = p_rx)) +
  geom_line(aes(group = x2, color = factor(x2))) +
  geom_point(aes(color = factor(x2)), size = .6) +
  theme(panel.grid = element_blank()) +
  scale_color_manual(values = c("red", "black"), name = "x2",
                     guide = guide_legend(reverse = TRUE) ) +
  xlab("x1") +
  ylab("P(A = 1)")
```

Typically, I would like to plot the raw outcome data to get an initial sense of the how the outcome relates to the covariates of interest, but with a categorical measure that has four levels, it is so obvious how to present the data in an insightful way. At the very least, we can show the distributions of the outcome over time and exposure:

```{r, echo=FALSE, fig.width = 9, fig.height = 2.5}
dsum <- dd[, .N, keyby = .(fy, A, year)]
dsum[, p := N/sum(N), keyby = .(year, A)]
dsum[, response := factor(fy, levels = c(1, 2, 3, 4))]
dsum[, A := factor(A, labels = c("not exposed", "exposed"))]

ggplot(data = dsum, aes(x = year,  group = A)) +
  geom_line(aes(y = p, color = A), size = 0.6) +
  facet_grid(. ~ response, labeller = label_both) +
  theme(panel.grid = element_blank(),
        legend.title = element_blank()) +
  scale_color_manual(values = c("#806cc6", "#b2c66c")) +
  ylab("proportion") 
```

### "Traditional" analysis

Since we suspect that there might be confounding due to covariates $x_1$ and $x_2$, and there appears to be different effects of exposure over time, it would not be unreasonble to estimate a multinomial model that adjusts for both covariates and includes an interaction term for exposure and year.

```{r}
fit <- multinom(fy ~ x1 + x2 + A*year, data = dd, trace = FALSE)
```

#### Interpreting the results

The parameter estimates for a multinomial model are shown in the table below. In this case, the fourth response category is the reference, so the table shows the odds ratios for each response relative to the reference (the intercepts or $\alpha_j$'s are not shown in the table). Each section of the table (labeled "1", "2", and "3") represent the estimated parameters for each response level. While some may be able to get a lot out of this table, I find it a little overwhelming, particularly when it comes to understanding (a) the impact of time on the exposure effect, and (b) how responses other than the reference category compare to each other. 

```{r, echo=FALSE, results='asis', warning=FALSE, message=FALSE}

library(magrittr)

multinom_pivot_wider <- function(x) {
  
  # create tibble of results
  df <- tibble::tibble(outcome_level = unique(x$table_body$groupname_col))
  df$tbl <- 
    purrr::map(
      df$outcome_level,
      function(lvl) {
        gtsummary::modify_table_body(
          x, 
          ~dplyr::filter(.x, .data$groupname_col %in% lvl) %>%
            dplyr::ungroup() %>%
            dplyr::select(-.data$groupname_col)
        )
      }
    )
  
  gtsummary::tbl_merge(df$tbl, tab_spanner = paste0("**", df$outcome_level, "**"))
}

gtsummary::tbl_regression(fit, exponentiate = TRUE) %>% multinom_pivot_wider()

```

#### Probability scale

What I really want is to be able to see the estimates on the probability scale. This is always challenging because we can only get predicted probabilities at *specific* levels of the covariates (i.e. the model is *conditional*), and it is not clear what levels of the covariates we should use for this purpose. While we could just use the average value of each covariate to generate an average probability for each exposure group and each time period, there is something arbitrary to doing this. A maybe somewhat more palatable approach is get estimates of the *marginal* probabilities. A while ago, I [presented an approach](https://www.rdatagen.net/post/2021-06-15-estimating-a-risk-difference-using-logistic-regression/){target="_blank"} in the context of logistic regression (which is just a special case of multinomial regression, where the categorical outcome has only two levels) that estimated a probability for each individual under each treatment arm, and then calculated an average risk difference by averaging across all the patients. This could presumably work here, but I decided to try another approach that eliminates the covariates from the analysis by using propensity score matching.

### Propensity score matching

I've described propensity score matching in an earlier [post](https://www.rdatagen.net/post/different-models-estimate-different-causal-effects-part-ii/){target="_blank"} (and provided at least one good reference), so I won't go into much detail here. The general idea is that we can estimate a probability of exposure (i.e., the propensity score), and then match individuals based on those scores. If done well, this creates two comparable groups that are balanced with respect to the confounders (assuming all the confounders have been measured and are included in the exposure model for the propensity score). And then, we can estimate the multinomial model without any covariates - and convert to probabilities without conditioning on any covariates.

In the first step, I matched within each year. This way, the groups were balanced at each time point, and I could estimate marginal probabilities for each year.

```{r}
matchby <- function(dx) {
  
  m <- matchit(A ~ x1 + x2, data = dx,
        method = "nearest", distance = "glm", caliper = .25)
  match.data(m)
  
  }

m.out <- rbindlist(lapply(1:6, function(x) matchby(dd[year==x])))
```

#### Analysis of matched data

In the second step, I fit a multinomial model that includes only exposure and time, and then generate predicted probability for each exposure and year combination.

```{r}
mfit <- multinom(fy ~ A*year, data = m.out, trace = FALSE)

dnew <- data.table(expand.grid(A = c(0,1), year = factor(c(1:6))))
dpred <- data.table(predict(mfit, newdata = dnew, "probs"))

dpred <- cbind(dnew, dpred)
dplot <- melt(data = dpred, id.vars = c("A", "year"), 
              value.name = "proportion", variable.name = "response")
dplot[, response := factor(response, levels = c(1, 2, 3, 4))]
dplot[, A := factor(A, labels = c("not exposed", "exposed"))]
```

Here are the predicted probabilities for Year 2:

```{r, echo = FALSE}
setkey(dplot, "response")
dplot[year == 2]
```

#### Bootstrap estimates of confidence bands

To go along with our point estimates, we need a measure of uncertainty, which we will estimate by bootstrap. For this analysis, I am bootstrapping the whole process, starting by sampling with replacement within each year and each exposure group, doing the matching, fitting the model, and generating the predictions.

```{r}
bs <- function(x) {
  
  ids <- dd[, .(id = sample(id, .N, replace = T)), keyby = .(year, A)][, id]
  db <- dd[ids]
  db[, id := .I]
  
  mb.out <- rbindlist(lapply(1:6, function(x) matchby(db[year==x])))
  
  mfit <- multinom(fy ~ A*year, data = mb.out, trace = FALSE)
  
  dbnew <- data.table(expand.grid(A = c(0,1), year = factor(c(1:6))))
  dbpred <- data.table(predict(mfit, newdata = dbnew, "probs"))
  
  cbind(iter = x, dnew, dbpred)
}

bspred <- rbindlist(lapply(1:500, function(x) bs(x)))

```

#### Plot point estimates and confidence bands

Before creating the figure, I've extracted 95% confidence intervals for each response level and year from the bootstrap data.

```{r, fig.height=2.5, fig.width = 9}
bsplot <- melt(data = bspred, id.vars = c("iter", "A", "year"), 
  value.name = "proportion", variable.name = "response")
bsplot[, response := factor(response, levels = c(1, 2, 3, 4))]
bsplot[, A := factor(A, labels = c("not exposed", "exposed"))]

ci <- bsplot[, 
  .(l95 = quantile(proportion, 0.025), u95 = quantile(proportion, 0.975)), 
  keyby = .(response, A, year)
]

ggplot(data = dplot, aes(x = year,  group = A)) +
  geom_ribbon(data = ci, 
              aes(ymin = l95, ymax = u95, fill = A),
              alpha = .2)  +
  geom_line(aes(y = proportion, color = A), size = .8) +
  facet_grid(. ~ response, labeller = label_both) +
  theme(panel.grid = element_blank(),
        legend.title = element_blank()) +
  scale_color_manual(values = c("#806cc6", "#b2c66c")) +
  scale_fill_manual(values = c("#806cc6", "#b2c66c"))
```

The point estimates mirror the marginal raw data plots quite closely, which is hardly surprising since we treated time as a categorical variable and the model is saturated. The benefit of doing the modeling is that we have generated estimates of uncertainty, and are in a position to draw some inferences. For example, it looks like the exposure has an increasing effect on the probability of a level "1" response in the last three years. Likewise, the effect of the exposure on the probability of a level "2" response was strongest in the first two years, and then disappears. There is too much uncertainty to say anything definitive about level "3" and "4" responses.


