---
title: When marginal and conditional logistic model estimates diverge
author: Keith Goldfeld
date: '2017-06-09'
slug: marginal-v-conditional
categories: []
tags: [R]
subtitle: ''
header-includes:
  - \usepackage{amsmath}
---




<STYLE TYPE="text/css">
<!--
  td{
    font-family: Arial; 
    font-size: 9pt;
    height: 2px;
    padding:0px;
    cellpadding="0";
    cellspacing="0";
    text-align: center;
  }
  th {
    font-family: Arial; 
    font-size: 9pt;
    height: 20px;
    font-weight: bold;
    text-align: center;
  }
  table { 
    border-spacing: 0px;
    border-collapse: collapse;
  }
--->
</STYLE>
<p>Say we have an intervention that is assigned at a group or cluster level but the outcome is measured at an individual level (e.g. students in different schools, eyes on different individuals). And, say this outcome is binary; that is, something happens, or it doesn’t. (This is important, because none of this is true if the outcome is continuous and close to normally distributed.) If we want to measure the <em>effect</em> of the intervention - perhaps the risk difference, risk ratio, or odds ratio - it can really matter if we are interested in the <em>marginal</em> effect or the <em>conditional</em> effect, because they likely won’t be the same.</p>
<p>My aim is to show this through a couple of data simulations that allow us to see this visually.</p>
<div id="first-example" class="section level3">
<h3>First example</h3>
<p>In the first scenario, I am going to use a <em>causal inference</em> framework that uses the idea that everyone has a potential outcome under one exposure (such as an intervention of some sort), and another potential outcome under a different exposure (such as treatment as usual or control). (I may discuss potential outcomes and causal inference in more detail in the future.) The potential outcome can be written with a superscript, lie <span class="math inline">\(Y^0\)</span> or <span class="math inline">\(Y^1\)</span>.</p>
<p>To generate the data, I will use this simple model for each potential outcome: <span class="math display">\[ log\left[\frac{P(Y^0_{ij})}{1-P(Y^0_{ij})}\right] = \gamma + \alpha_i\]</span></p>
<p>and <span class="math display">\[ log\left[\frac{P(Y^1_{ij})}{1-P(Y^1_{ij})}\right] = \gamma + \alpha_i + \delta.\]</span> <span class="math inline">\(\delta\)</span> is the treatment effect and is constant across the clusters, on the log-odds scale. <span class="math inline">\(\alpha_i\)</span> is the cluster specific effect for cluster <span class="math inline">\(i\)</span>. <span class="math inline">\(Y^a_{ij}\)</span> is the potential outcome for individual <span class="math inline">\(j\)</span> under exposure <span class="math inline">\(a\)</span>.</p>
<p>Now let’s generate some data and look at it:</p>
<pre class="r"><code># Define data

def1 &lt;- defData(varname = &quot;clustEff&quot;, formula = 0, variance = 2, 
                id = &quot;cID&quot;)
def1 &lt;- defData(def1, varname = &quot;nInd&quot;, formula = 10000, 
                dist = &quot;noZeroPoisson&quot;)
  
def2 &lt;- defDataAdd(varname = &quot;Y0&quot;, formula = &quot;-1 + clustEff&quot;, 
                     dist = &quot;binary&quot;, link = &quot;logit&quot;)
def2 &lt;- defDataAdd(def2, varname = &quot;Y1&quot;, 
                     formula = &quot;-1 + clustEff + 2&quot;, 
                     dist = &quot;binary&quot;, link = &quot;logit&quot;)

options(width = 80)
def1</code></pre>
<pre><code>##     varname formula variance          dist     link
## 1: clustEff       0        2        normal identity
## 2:     nInd   10000        0 noZeroPoisson identity</code></pre>
<pre class="r"><code>def2</code></pre>
<pre><code>##    varname           formula variance   dist  link
## 1:      Y0     -1 + clustEff        0 binary logit
## 2:      Y1 -1 + clustEff + 2        0 binary logit</code></pre>
<pre class="r"><code># Generate cluster level data

set.seed(123)
  
dtC &lt;- genData(n = 100, def1)

# Generate individual level data
  
dt &lt;- genCluster(dtClust = dtC, cLevelVar = &quot;cID&quot;, numIndsVar = &quot;nInd&quot;, 
                   level1ID = &quot;id&quot;)

dt &lt;- addColumns(def2, dt)</code></pre>
<p>Since we have repeated measurements for each cluster (the two potential outcomes), we can transform this into a “longitudinal” data set, though the periods are not time but different exposures.</p>
<pre class="r"><code>dtLong &lt;- addPeriods(dtName = dt, idvars = c(&quot;id&quot;,&quot;cID&quot;), 
                     nPeriods = 2,timevars = c(&quot;Y0&quot;,&quot;Y1&quot;), 
                     timevarName = &quot;Y&quot;
)</code></pre>
<p>When we look at the data visually, we get a hint that the marginal (or average) effect might not be the same as the conditional (cluster-specific) effects.</p>
<pre class="r"><code># Calculate average potential outcomes by exposure (which is called period)

dtMean &lt;- dtLong[, .(Y = mean(Y)), keyby = .(period, cID)] # conditional mean
dtMMean &lt;- dtLong[, .(Y = mean(Y)), keyby = .(period)] # marginal mean
dtMMean[, cID := 999]

ggplot(data = dtMean, aes(x=factor(period), y = Y, group= cID)) +
  # geom_jitter(width= .25, color = &quot;grey75&quot;) +
  geom_line(color = &quot;grey75&quot;, position=position_jitter(w=0.02, h=0.02)) +
  geom_point(data=dtMMean) +
  geom_line(data=dtMMean, size = 1, color = &quot;red&quot;) +
  ylab(&quot;Estimated cluster probability&quot;) +
  scale_y_continuous(labels = scales::percent) +
  theme(axis.title.x = element_blank()) +
  my_theme()</code></pre>
<p><img src="/post/2017-06-09-marginal-v-conditional_files/figure-html/unnamed-chunk-2-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Looking at the plot, we see that the slopes of the grey lines - each representing the change in probability as a result of the exposure for each cluster - vary quite a bit. When the probability without exposure (<span class="math inline">\(Y^0\)</span>) is particularly low or high, the absolute effect of the intervention is small (the slope is minimal). The slope or absolute effect increases when the starting probability is closer to 50%. The red line represents the averages of <span class="math inline">\(Y^0\)</span> and <span class="math inline">\(Y^1\)</span> across all individuals in all clusters. There is no reason to believe that the average slope of the grey lines is the same as the slope of the red line, which is slope of the averages. We will see that more clearly with the next data generation scenario.</p>
<p>Finally, if we look at cluster-specific effects of exposure, we see that on the risk difference scale (difference in probabilities), there is much variation, but on the log-odds ratio scale there is almost no variation. This is as it should be, because on the log-odds scale (which is how we generated the data), the difference between exposure and non-exposure is additive. On the probability scale, the difference is multiplicative. Here are some estimated differences for a sample of clusters:</p>
<pre class="r"><code>dtChange &lt;- dt[, .(Y0 = mean(Y0), Y1 = mean(Y1)), keyby = cID]
dtChange[, riskdiff := round(Y1 - Y0, 2)]
dtChange[, loratio := round( log( (Y1 / (1-Y1)) / (Y0 / (1-Y0) )), 2)]

dtChange[sample(1:100, 10, replace = F),
         .(Y0 = round(Y0,2), Y1 = round(Y1,2), riskdiff, loratio), 
         keyby=cID]</code></pre>
<pre><code>##     cID   Y0   Y1 riskdiff loratio
##  1:  18 0.02 0.14     0.12    1.95
##  2:  19 0.50 0.89     0.39    2.06
##  3:  22 0.22 0.67     0.45    1.97
##  4:  24 0.12 0.49     0.37    1.94
##  5:  30 0.69 0.94     0.26    2.00
##  6:  31 0.40 0.83     0.42    1.95
##  7:  34 0.56 0.90     0.35    1.99
##  8:  38 0.26 0.72     0.46    2.01
##  9:  72 0.01 0.09     0.08    1.97
## 10:  99 0.22 0.66     0.44    1.93</code></pre>
</div>
<div id="second-example" class="section level2">
<h2>Second example</h2>
<p>This time around, we will add an additional individual level covariate that will help us visualize the difference a bit more clearly. Let us say that <em>age</em> is positively associated with increased probability in the outcome. (In this case, we measured age and then normalized it so that the mean age in the sample is 0.) And this time around, we are not going to use potential outcomes, but will randomly assign clusters to an intervention or treatment group.</p>
<p>This is the data generating model and the code:</p>
<p><span class="math display">\[ log\left[\frac{P(Y_{ij})}{1-P(Y_{ij})}\right] = \gamma + \alpha_j + \beta_1*Trt_j + \beta_2*Age_i\]</span></p>
<pre class="r"><code>def1 &lt;- defData(varname = &quot;clustEff&quot;, formula = 0, variance = 2, id = &quot;cID&quot;)
def1 &lt;- defData(def1, varname = &quot;nInd&quot;, formula = 100, dist = &quot;noZeroPoisson&quot;)
  
# Each individual now has a measured age

def2 &lt;- defDataAdd(varname = &quot;age&quot;, formula = 0, variance = 2)
def2 &lt;- defDataAdd(def2, varname = &quot;Y&quot;, 
                   formula = &quot;-4 + clustEff + 2*trt + 2*age&quot;, 
                   dist = &quot;binary&quot;, link = &quot;logit&quot;)
  
# Generate cluster level data
  
dtC &lt;- genData(200, def1)
dtC &lt;- trtAssign(dtC, grpName = &quot;trt&quot;) #
  
# Generate individual level data
  
dt &lt;- genCluster(dtClust = dtC, cLevelVar = &quot;cID&quot;, numIndsVar = &quot;nInd&quot;, 
                 level1ID = &quot;id&quot;)
dt &lt;- addColumns(def2, dt)</code></pre>
<p>By fitting a conditional model (generalized linear mixed effects model) and a marginal model (we should fit a generalized estimating equation model to get the proper standard error estimates, but will estimate a generalized linear model, because the GEE model does not have a “predict” option in R; the point estimates for both marginal models should be quite close), we can see that indeed the conditional and marginal averages can be quite different.</p>
<pre class="r"><code>glmerFit1 &lt;- glmer(Y ~ trt + age + (1 | cID), data = dt, family=&quot;binomial&quot;)
glmFit1 &lt;- glm(Y ~ trt + age, family = binomial, data = dt)</code></pre>
<pre><code>##                   Intercept  Trt  Age
## conditional model     -3.82 1.99 2.01
## marginal model        -2.97 1.60 1.54</code></pre>
<p>Now, we’d like to visualize how the conditional and marginal treatment effects diverge. We can use the model estimates from the conditional model to predict probabilities for each cluster, age, and treatment group. (These will appear as grey lines in the plots below). We can also predict marginal probabilities from the marginal model based on age and treatment group while ignoring cluster. (These marginal estimates appear as red lines.) Finally, we can predict probability of outcomes for the conditional model also based on age and treatment group alone, but fixed at a mythical cluster whose random effect is 0. (These “average” conditional estimates appear as black lines.)</p>
<pre class="r"><code>newCond &lt;- expand.grid(cID = unique(dt$cID), age=seq(-4, 4, by =.1))
newCond0 &lt;- data.table(trt = 0, newCond)
newCond1 &lt;- data.table(trt = 1, newCond)

newMarg0 &lt;- data.table(trt = 0, age = seq(-4, 4, by = .1))
newMarg1 &lt;- data.table(trt = 1, age = seq(-4, 4, by = .1))

newCond0[, pCond0 := predict(glmerFit1, newCond0, type = &quot;response&quot;)]
newCond1[, pCond1 := predict(glmerFit1, newCond1, type = &quot;response&quot;)]

newMarg0[, pMarg0 := predict(glmFit1, newMarg0, type = &quot;response&quot;)]
newMarg0[, pCAvg0 := predict(glmerFit1, newMarg0[,c(1,2)], 
                             re.form = NA, type=&quot;response&quot;)]

newMarg1[, pMarg1 := predict(glmFit1, newMarg1, type = &quot;response&quot;)]
newMarg1[, pCAvg1 := predict(glmerFit1, newMarg1[,c(1,2)], 
                             re.form = NA, type=&quot;response&quot;)]

dtAvg &lt;- data.table(age = newMarg1$age, 
           avgMarg = newMarg1$pMarg1 - newMarg0$pMarg0, 
           avgCond = newMarg1$pCAvg1 - newMarg0$pCAvg0
)

p1 &lt;- ggplot(aes(x = age, y = pCond1), data=newCond1) + 
  geom_line(color=&quot;grey&quot;, aes(group = cID)) +
  geom_line(data=newMarg1, aes(x = age, y = pMarg1), color = &quot;red&quot;, size = 1) +
  geom_line(data=newMarg1, aes(x = age, y = pCAvg1), color = &quot;black&quot;, size = 1) +
  ggtitle(&quot;Treatment group&quot;) +
  xlab(&quot;Age&quot;) +
  ylab(&quot;Probability&quot;) +
  my_theme()

p0 &lt;- ggplot(aes(x = age, y = pCond0), data=newCond0) + 
  geom_line(color=&quot;grey&quot;, aes(group = cID)) +
  geom_line(data=newMarg0, aes(x = age, y = pMarg0), color = &quot;red&quot;, size = 1) +
  geom_line(data=newMarg0, aes(x = age, y = pCAvg0), color = &quot;black&quot;, size = 1) +
  ggtitle(&quot;Control group&quot;) +
  xlab(&quot;Age&quot;) +
  ylab(&quot;Probability&quot;) +
  my_theme()

pdiff &lt;- ggplot(data = dtAvg) + 
  geom_line(aes(x = age, y = avgMarg), color = &quot;red&quot;, size = 1) +
  geom_line(aes(x = age, y = avgCond), color = &quot;black&quot;, size = 1) +
  ggtitle(&quot;Risk difference&quot;) +
  xlab(&quot;Age&quot;) +
  ylab(&quot;Probability&quot;) +
  my_theme()

grid.arrange(p1, p0, pdiff)</code></pre>
<p><img src="/post/2017-06-09-marginal-v-conditional_files/figure-html/unnamed-chunk-7-1.png" width="432" style="display: block; margin: auto;" /></p>
<p>We see pretty clearly across all ages that the marginal and conditional estimates of average treatment differences differ quite dramatically.</p>
<p>Below are point estimates and plots for data generated with very little variance across clusters, that is <span class="math inline">\(var(\alpha_i)\)</span> is close to 0. (We change this in the simulation by setting <code>def1 &lt;- defData(varname = &quot;clustEff&quot;, formula = 0, variance = 0.05, id = &quot;cID&quot;)</code>.)</p>
<pre><code>##                   Intercept  Trt  Age
## conditional model     -4.03 2.08 2.01
## marginal model        -4.00 2.07 1.99</code></pre>
<p><img src="/post/2017-06-09-marginal-v-conditional_files/figure-html/unnamed-chunk-8-1.png" width="432" style="display: block; margin: auto;" /></p>
<p>The black lines obscure the red - the marginal model estimate is not much different from the conditional model estimate - because the variance across clusters is negligible.</p>
</div>
