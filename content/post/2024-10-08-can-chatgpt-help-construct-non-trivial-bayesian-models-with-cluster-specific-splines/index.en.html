---
title: Can ChatGPT help construct non-trivial statistical models? An example with Bayesian "random" splines
author: Package Build
date: '2024-10-08'
slug: []
categories: []
tags:
  - R
  - Bayesian model
  - ChatGPT
type: ''
subtitle: ''
image: ''
---



<p>I’ve been curious to see how helpful ChatGPT can be for implementing relatively complicated models in <code>R</code>. About two years ago, I <a href="https://www.rdatagen.net/post/2022-11-01-modeling-secular-trend-in-crt-using-gam/" target="_blank">described</a> a model for estimating a treatment effect in a cluster-randomized stepped wedge trial. We used a generalized additive model (GAM) with site-specific splines to account for general time trends, implemented using the <code>mgcv</code> package. I’ve been interested in exploring a Bayesian version of this model, but hadn’t found the time to try - until I happened to pose this simple question to ChatGPT:</p>
<blockquote>
I want to build an R Stan model with cluster specific random splines - is that possible?
</blockquote>
<p>And as ChatGPT does, it eagerly let me know that this would not be a problem:</p>
<blockquote>
<p>Yes, it’s possible to build a model in R using Stan with cluster-specific random splines. This approach involves using a hierarchical model where each cluster has its own spline, but the splines share a common structure.</p>
<p>Here is a step-by-step outline to guide you through this process:</p>
<ol style="list-style-type: decimal">
<li><strong>Data Preparation</strong>: Organize your data into clusters and ensure it’s in a suitable format for modeling.</li>
<li><strong>Spline Basis Functions</strong>: Generate spline basis functions (e.g., B-splines) for your data. This can be done using packages like <code>splines</code> or <code>mgcv</code> in R.</li>
<li><strong>Stan Model Code</strong>: Write the Stan code for the hierarchical spline model. The model will include cluster-specific random effects for the spline coefficients.</li>
<li><strong>Data List for Stan</strong>: Create a list of data inputs required by the Stan model, including the spline basis functions and the data itself.</li>
<li><strong>Run the Model</strong>: Use the <code>rstan</code> package to compile and fit the model.</li>
</ol>
</blockquote>
<p>Maybe I shouldn’t have been, but I was pretty impressed by the code that followed. What follows are the highlights of my interaction with ChatGPT.</p>
<div id="preliminaries" class="section level3">
<h3>Preliminaries</h3>
<p>Before getting started, here are the libraries needed to run all of the following code:</p>
<pre class="r"><code>library(ggplot2)
library(simstudy)
library(data.table)
library(mgcv)
library(splines)
library(splines2)
library(cmdstanr)
library(posterior)</code></pre>
</div>
<div id="data-generation" class="section level3">
<h3>Data generation</h3>
<p>To explore different modeling options, I wanted a simple data generation process to create the simulated data set. ChatGPT suggested using the following non-linear function for <span class="math inline">\(y_{ik}\)</span>, the outcome for individual <span class="math inline">\(i\)</span> in cluster <span class="math inline">\(k\)</span>, based on predictor <span class="math inline">\(x_{ik}\)</span>:
<span class="math display">\[ y_{ik} = \text{sin}(2\pi x_{ik}), \ \ \{ x \in \mathbb{R} \mid 0 \leq x \leq 1 \} \]</span>
The code was simple enough:</p>
<pre class="r"><code>set.seed(2024)

n &lt;- 1000
k &lt;- 10  # number of clusters
x &lt;- runif(n)
cluster &lt;- sample(1:k, n, replace = TRUE)
y &lt;- sin(2  * pi * x) + rnorm(n, sd = 0.35)
dd &lt;- data.table(y, x, cluster)
dd$cluster &lt;- factor(dd$cluster)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/GPTdata-1.png" width="816" /></p>
<p>Although the data generation process suggested by ChatGPT was helpful, it had a significant shortcoming. I wanted to model <em>cluster-specific</em> spline curves, but the ChatGPT code generated the same curve for all clusters. To address this, I used the general formulation and added a cluster-specific effect <span class="math inline">\(a_k\)</span>, which stretches the <em>sin</em> curve differently for each cluster:
<span class="math display">\[ y_{ik} = \text{sin}(2\pi a_k x_{ik}), \ \ \{ a \in \mathbb{R} \mid 0.6 \leq a \leq 1.4 \} \]</span></p>
<pre class="r"><code>k &lt;- 10  # number of clusters

defc &lt;- defData(varname = &quot;a&quot;, formula = &quot;0.6;1.4&quot;, dist = &quot;uniform&quot;)
defi &lt;- 
  defDataAdd(varname = &quot;x&quot;, formula = &quot;0;1&quot;, dist = &quot;uniform&quot;) |&gt;
  defDataAdd(
    varname = &quot;y&quot;, 
    formula =  &quot;sin(2 * a * ..pi * x)&quot;, 
    variance = 0.35^2
  )

dc &lt;- genData(k, defc, id = &quot;cluster&quot;)
dd &lt;- genCluster(dc, &quot;cluster&quot;, 100, &quot;id&quot;)
dd &lt;- addColumns(defi, dd)
dd[, cluster := factor(cluster)]</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/ksgData-1.png" width="816" /></p>
</div>
<div id="data-modeling" class="section level3">
<h3>Data modeling</h3>
<p>The goal is to estimate cluster-specific curves that capture the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> within each cluster. I am aiming for these curves to reflect the overall trend without overfitting the data; in other words, we want the estimated function to provide a smooth and interpretable representation of the relationship, balancing flexibility and simplicity.</p>
<p>Although the purpose of my conversation with ChatGPT was to get a Bayesian version of this random spline model, I started off by asking for it to generate a generalized additive model (GAM) to provide a basis for comparison. This is what it came up with:
<span class="math display">\[ y_{ik} = \beta_0 + s_k(x_{ik}) + \epsilon_{ik}, \ \ \epsilon \sim N(0, \sigma_y) \]</span></p>
<p>where <span class="math inline">\(s_k(x)\)</span> is a smooth spline function of <span class="math inline">\(x\)</span>. The estimated model can be used to provide predictions that can be plotted to describe the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>:</p>
<pre class="r"><code>gam &lt;- gamm(
  y ~ s(x) + s(x, cluster, bs = &quot;fs&quot;, k = 8),
  data = dd, method=&quot;REML&quot;
)

dd$g &lt;- predict(gam$gam)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/gamplot-1.png" width="816" /></p>
<div id="bayesian-spline-model" class="section level4">
<h4>Bayesian spline model</h4>
<p>The first Bayesian model that ChatGPT generated can be described using this notation:</p>
<p><strong>Data</strong></p>
<ul>
<li><span class="math inline">\(N\)</span>: number of individuals</li>
<li><span class="math inline">\(K\)</span>: number of clusters</li>
<li><span class="math inline">\(M\)</span>: number of spline basis functions</li>
<li><span class="math inline">\(y_{ik}\)</span>: outcome for individual <span class="math inline">\(i\)</span> in cluster <span class="math inline">\(k\)</span>, <span class="math inline">\(i \in 1,\dots,N\)</span>, <span class="math inline">\(k \in 1,\dots ,K\)</span></li>
<li><span class="math inline">\(\boldsymbol{X} \in \mathbb{R}^{N \times M}\)</span>: matrix of spline basis function values</li>
</ul>
<p><strong>Parameters</strong></p>
<ul>
<li><span class="math inline">\(\boldsymbol{\beta_{k}} \in \mathbb{R}^M\)</span>: spline coefficients for cluster <span class="math inline">\(k\)</span> (a vector of length <span class="math inline">\(M\)</span> for each cluster)</li>
<li><span class="math inline">\(\sigma_y\)</span>: standard deviation of the observation noise</li>
<li><span class="math inline">\(\sigma_\beta\)</span>: prior standard deviation for the spline coefficients</li>
</ul>
<p><strong>Model</strong></p>
<p><em>Likelihood</em>:</p>
<p><span class="math display">\[ y_{ik} \sim N\left( \sum_{m=1}^M X_{im} \beta_{km}, \sigma_y  \right), \ i \in 1,\dots, N, \ k \in 1, \dots, K\]</span></p>
<p><em>Priors</em>:</p>
<p><span class="math display">\[
\boldsymbol{\beta_{k}} \sim N(0, \sigma_{\beta} \boldsymbol{I_M}), \ \ k \in 1,...,K \\
\sigma_{y} \sim N(0, 1), \ \ \sigma_y \gt 0 \\
\sigma_{\beta} \sim N(0, 1), \ \ \sigma_{\beta} \gt 0
\]</span></p>
<p>The <code>Stan</code> code provided by ChatGPT aligns with this description. As part of the model, I also requested code to generate outcome predictions for each observation, which is implemented in the <em>generated quantities</em> block. My goal was to plot the median of those predictions for each individual <span class="math inline">\(i\)</span> as a comparison to the GAM plot above.”</p>
<pre class="stan"><code>data {
  int&lt;lower=1&gt; N;                          // number of observations
  int&lt;lower=1&gt; K;                          // number of clusters
  int&lt;lower=1&gt; M;                          // number of basis functions
  array[N] int&lt;lower=1, upper=K&gt; cluster;  // cluster ids
  matrix[N, M] X_spline;                   // basis function values
  vector[N] y;                             // response variable
}

parameters {
  matrix[K, M] beta;          // cluster-specific spline coefficients
  real&lt;lower=0&gt; sigma_y;      // observation noise
  real&lt;lower=0&gt; sigma_beta;   // prior standard deviation for beta
}

model {
  sigma_y ~ normal(0, 1);
  sigma_beta ~ normal(0, 1);
  
  // Priors for beta
  
  for (k in 1:K) {
    beta[k] ~ normal(0, sigma_beta);
  }
  
  // Likelihood
  
  for (n in 1:N) {
    y[n] ~ normal(X_spline[n] * beta[cluster[n]]&#39;, sigma_y);
  }
}

generated quantities {
  
  vector[N] y_pred;                    // Vector of observations.
  
  for (n in 1:N) {
    y_pred[n] = normal_rng(X_spline[n] * beta[cluster[n]]&#39;, sigma_y);
  }
}</code></pre>
</div>
<div id="spline-basis-functions" class="section level4">
<h4>Spline basis functions</h4>
<p>In the likelihood, <span class="math inline">\(y_i\)</span> is modeled as a function of the vector <span class="math inline">\(\boldsymbol{X_i}\)</span> rather than the single measurement <span class="math inline">\(x_i\)</span>. While I won’t delve deeply into spline estimation, I want to conceptually outline how this vector is constructed in the context of cubic splines.</p>
<p>We control the flexibility of the curve by specifying the number of knots. A unique curve is fitted between each pair of knots (as well as at the ends), with constraints ensuring smooth transitions between these curves. The estimation of these curves is performed using basis functions, specifically B-spline basis functions of <span class="math inline">\(x\)</span>.</p>
<p>The number of basis functions is determined by the number of knots. For instance, the plot below illustrates the <span class="math inline">\(M=9\)</span> basis functions required for <span class="math inline">\(K=5\)</span> knots. Each basis function contributes an element to the vector <span class="math inline">\(\boldsymbol{X}\)</span> for each value of <span class="math inline">\(x\)</span>. In the case of cubic splines, at most four basis functions can be non-zero between any two knots, as indicated by the intervals on the x-axis. Consequently, the vector <span class="math inline">\(\boldsymbol{X}\)</span> consists of the values of each basis function at a given point <span class="math inline">\(x\)</span>, with at most four non-zero entries corresponding to the active basis functions. (As an example, in the plot below there is a vertical line at a single point <span class="math inline">\(x\)</span> that passes through four basis functions.)</p>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/basis-1.png" width="816" /></p>
<p>This example uses <span class="math inline">\(M = 5\)</span> knots to introduce a slight overfitting of the data, which will allow me to apply another model in the next step that will further smooth the curves. (In a real-world setting, it may have made more sense to start out with fewer knots.) The <code>bs</code> function (in the <code>splines</code> package) computes the B-spline basis function values for each observed <span class="math inline">\(x\)</span>.</p>
<pre class="r"><code>n_knots &lt;- 5
knot_dist &lt;- 1/(n_knots + 1)
probs &lt;- seq(knot_dist, 1 - knot_dist, by = knot_dist)
knots &lt;- quantile(dd$x, probs = probs)
spline_basis &lt;- bs(dd$x, knots = knots, degree = 3, intercept = TRUE)
X_spline &lt;- as.matrix(spline_basis)</code></pre>
</div>
<div id="data-list-for-stan" class="section level4">
<h4>Data list for stan</h4>
<p>To fit the model, we need to create the data set that <code>Stan</code> will use to estimate the parameters.</p>
<pre class="r"><code>stan_data &lt;- list(
  N = nrow(dd),           # number of observations
  K = k,                  # number of clusters
  M = ncol(X_spline),     # number of basis functions
  cluster = dd$cluster,   # vector of cluster ids
  X_spline = X_spline,    # basis function values
  y = dd$y                # response variable
)</code></pre>
</div>
<div id="run-stan-model" class="section level4">
<h4>Run stan model</h4>
<p>ChatGPT provided code to estimate the model using the <code>rstan</code> package. However, I prefer using the <code>cmdstanr</code> package, which I find more stable and generally less finicky. From the plot, you can see that the estimation was quite good. However, the curves are a bit too wiggly, indicating the data may have been slightly overfit, particularly for clusters 1, 3, and 7.</p>
<pre class="r"><code>mod &lt;- cmdstan_model(&quot;code/spline.stan&quot;)

fit &lt;- mod$sample(
  data = stan_data,
  chains = 4,
  iter_warmup = 500,
  iter_sampling = 2000,
  parallel_chains = 4,
  refresh = 0 # print update every 500 iters
)</code></pre>
<pre><code>## Running MCMC with 4 parallel chains...</code></pre>
<pre><code>## Chain 2 finished in 5.4 seconds.
## Chain 1 finished in 5.6 seconds.
## Chain 3 finished in 5.6 seconds.
## Chain 4 finished in 5.8 seconds.
## 
## All 4 chains finished successfully.
## Mean chain execution time: 5.6 seconds.
## Total execution time: 5.9 seconds.</code></pre>
<pre class="r"><code>draws &lt;- as_draws_df(fit$draws())

ds &lt;- summarize_draws(draws, .fun = median) |&gt; data.table()
dd$np &lt;- ds[substr(variable, 1, 3) == &quot;y_p&quot;, 2]</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/bayes-1.png" width="816" /></p>
</div>
</div>
<div id="penalized-spline" class="section level3">
<h3>Penalized spline</h3>
<p>When I made my initial inquiry to ChatGPT, it provided only a single model and didn’t indicate that there might be alternatives. To elicit another option, I had to specifically ask. To smooth the estimate provided by the initial model (which admittedly I made too wiggly on purpose), I asked ChatGPT to provide a <em>penalized</em> Bayesian spline model, and it obliged.</p>
<p>The model is just an extension of the spline model, with an added penalization term that is based on the second derivative of the B-spline basis functions. We can strengthen or weaken the penalization term using a tuning parameter <span class="math inline">\(\lambda\)</span>, that is provided to the model. The <code>Stan</code> model code is unchanged from the original model, except for the added penalization term.</p>
<pre class="stan"><code>model {

  sigma_y ~ normal(0, 1);
  sigma_beta ~ normal(0, 1);

  // Priors for beta
  
  for (k in 1:K) {
    beta[k] ~ normal(0, sigma_beta);
  }
  
  //Penalization &lt;---------------------------------------
  
  for (k in 1:K) {
    target += -lambda * sum(square(D2_spline * beta[k]&#39;));
  }
  
  // Likelihood
  
  for (n in 1:N) {
    y[n] ~ normal(X_spline[n] * beta[cluster[n]]&#39;, sigma_y);
  }
}</code></pre>
<p>The second derivatives of the B-spline basis functions are estimated using the <code>dbs</code> function in the <code>splines2</code> package. Like the matrix <span class="math inline">\(\boldsymbol{X}\)</span>, <span class="math inline">\(\boldsymbol{D_2}\)</span> has dimensions <span class="math inline">\(N \times\ M\)</span>. Both <span class="math inline">\(\boldsymbol{D_2}\)</span> and <span class="math inline">\(\lambda\)</span> are added to the data passed to <code>Stan</code>:</p>
<pre class="r"><code>D2 &lt;- dbs(dd$x, knots = knots, degree = 3, derivs = 2, intercept = TRUE)
D2_spline &lt;- as.matrix(D2)

stan_data &lt;- list(
  N = nrow(dd),
  K = k,
  M = ncol(X_spline),
  cluster = dd$cluster,
  X_spline = X_spline,
  D2_spline = D2_spline,
  y = dd$y,
  lambda = 0.00005
)</code></pre>
<pre><code>## Running MCMC with 4 parallel chains...</code></pre>
<pre><code>## Chain 2 finished in 16.5 seconds.
## Chain 1 finished in 16.6 seconds.
## Chain 3 finished in 16.7 seconds.
## Chain 4 finished in 16.7 seconds.
## 
## All 4 chains finished successfully.
## Mean chain execution time: 16.6 seconds.
## Total execution time: 16.8 seconds.</code></pre>
<p>The plot directly comparing the penalized Bayesian model with the initial Bayesian model (initial Bayesian model in blue) shows the impact of further smoothing.</p>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/pen-1.png" width="816" /></p>
<p>A direct comparison between the GAM and penalized Bayesian models (GAM in green) suggests that there might be some differences in the estimation for at least several clusters, particularly those that change direction twice. The penalized Bayesian model appears to be smoothing more than the GAM:</p>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/compare%20-1.png" width="816" /></p>
<p>I was also aware of a third version of the Bayesian spline model that uses a <em>random-walk</em> prior on the <span class="math inline">\(\beta\text{&#39;s}\)</span> to induce smoothing. Unprompted, ChatGPT did not mention this. But, upon request it did give me code that I was able to implement successfully. I’ll leave it to you to explore this further on your own—or perhaps ask ChatGPT for assistance.</p>
<p>
<p><small><font color="darkkhaki">
Reference:</p>
<p>OpenAI. (2024). ChatGPT (September 30, Version) [Large language model]. <a href="https://chat.openai.com/" class="uri">https://chat.openai.com/</a></p>
<p>Support:</p>
<p>This work is supported within the National Institutes of Health (NIH) Health Care Systems Research Collaboratory by cooperative agreement UG3/UH3AT009844 from the National Institute on Aging. This work also received logistical and technical support from the NIH Collaboratory Coordinating Center through cooperative agreement U24AT009676. Support was also provided by the NIH National Center for Complementary and Integrative Health Administrative Supplement for Complementary Health Practitioner Research Experience through cooperative agreement UH3AT009844 and by the National Center for Complementary and Integrative Health of the National Institutes of Health under award number UH3AT009844. Work also supported by Memorial Sloan Kettering Cancer Center Support Grant/Core Grant P30CA008748. The author was the sole writer of this blog post and has no conflicts. The content is solely the responsibility of the author and does not necessarily represent the official views of the National Institutes of Health.</p>
</font></small>
</p>
</div>
