---
title: 
  'Drawing the wrong conclusion about subgroups: a comparison of Bayes and frequentist methods'
author:
date: '2021-09-14'
slug: []
categories: []
tags:
  - Bayesian model
  - Stan
  - R
type: ''
subtitle: ''
image: ''
---

<script src="{{< blogdown/postref >}}index.en_files/header-attrs/header-attrs.js"></script>


<p>In the previous <a href="https://www.rdatagen.net/post/2021-08-31-subgroup-analysis-using-a-bayesian-hierarchical-model/" target="_blank">post</a>, I simulated data from a hypothetical RCT that had heterogeneous treatment effects across subgroups defined by three covariates. I presented two Bayesian models, a strongly <em>pooled</em> model and an <em>unpooled</em> version, that could be used to estimate all the subgroup effects in a single model. I compared the estimates to a set of linear regression models that were estimated for each subgroup separately.</p>
<p>My goal in doing these comparisons is to see how often we might draw the wrong conclusion about subgroup effects when we conduct these types of analyses. In a typical frequentist framework, the probability of making a mistake is usually considerably greater than the 5% error rate that we allow ourselves, because conducting multiple tests gives us more chances to make a mistake. By using Bayesian hierarchical models that share information across subgroups and more reasonably measure uncertainty, I wanted to see if we can reduce the chances of drawing the wrong conclusions.</p>
<div id="simulation-framework" class="section level3">
<h3>Simulation framework</h3>
<p>The simulations used here are based on the same general process I used to generate a single data set the <a href="https://www.rdatagen.net/post/2021-08-31-subgroup-analysis-using-a-bayesian-hierarchical-model/" target="_blank">last time around</a>. The key difference is that I now want to understand the operating characteristics of the models, and this requires many data sets (and their model fits). Much of the modeling is similar to last time, so I’m primarily showing new code.</p>
<p>This is a pretty computing intensive exercise. While the models don’t take too long to fit, especially with only 150 observations per data set, fitting 2500 sets of models can take some time. As I do for all the simulations that require repeated Bayesian estimation, I executed all of this on a high-performance computer. I used a framework similar to what I’ve described for conducting <a href="https://www.rdatagen.net/post/2021-03-16-framework-for-power-analysis-using-simulation/" target="_blank">power analyses</a> and <a href="https://www.rdatagen.net/post/a-frequentist-bayesian-exploring-frequentist-properties-of-bayesian-models/" target="_blank">exploring the operating characteristics</a> of Bayesian models.</p>
<div id="definitions" class="section level4">
<h4>Definitions</h4>
<p>The definitions of the data generation process are the same as in the previous post, except I’ve made the generation of <code>theta</code> more flexible. Last time, I fixed the coefficients (<span class="math inline">\(\tau\)</span>’s) at specific values. Here, the <span class="math inline">\(\tau\)</span>’s can vary from iteration to iteration. Even though I am generating data with no treatment effect, I am taking a Bayesian point of view on this - so that the treatment effect parameters will have a distribution that is centered around 0 with very low variance.</p>
<pre class="r"><code>library(cmdstanr)
library(simstudy)
library(posterior)
library(data.table)
library(slurmR)

setgrp &lt;- function(a, b, c) {
  
  if (a==0 &amp; b==0 &amp; c==0) return(1)
  if (a==1 &amp; b==0 &amp; c==0) return(2)
  if (a==0 &amp; b==1 &amp; c==0) return(3)
  if (a==0 &amp; b==0 &amp; c==1) return(4)
  if (a==1 &amp; b==1 &amp; c==0) return(5)
  if (a==1 &amp; b==0 &amp; c==1) return(6)
  if (a==0 &amp; b==1 &amp; c==1) return(7)
  if (a==1 &amp; b==1 &amp; c==1) return(8)
  
}

s_define &lt;- function() {
  
  d &lt;- defData(varname = &quot;a&quot;, formula = 0.6, dist=&quot;binary&quot;)
  d &lt;- defData(d, varname = &quot;b&quot;, formula = 0.4, dist=&quot;binary&quot;)
  d &lt;- defData(d, varname = &quot;c&quot;, formula = 0.3, dist=&quot;binary&quot;)
  d &lt;- defData(d, varname = &quot;theta&quot;,
    formula = &quot;..tau[1] + ..tau[2]*a  + ..tau[3]*b + ..tau[4]*c +
               ..tau[5]*a*b + ..tau[6]*a*c + ..tau[7]*b*c + ..tau[8]*a*b*c&quot;,
    dist = &quot;nonrandom&quot;
  )
  
  drx &lt;- defDataAdd(
    varname = &quot;y&quot;, formula = &quot;0 + theta*rx&quot;, 
    variance = 16, 
    dist = &quot;normal&quot;
  )

  return(list(d = d, drx = drx))
  
}</code></pre>
</div>
<div id="data-generation" class="section level4">
<h4>Data generation</h4>
<p>We are generating the eight values of <code>tau</code> for each iteration from a <span class="math inline">\(N(\mu = 0, \sigma = 0.5)\)</span> distribution before generating <code>theta</code> and the outcome <code>y</code>:</p>
<pre class="r"><code>s_generate &lt;- function(n, list_of_defs) {
  
  list2env(list_of_defs, envir = environment())
  
  tau &lt;- rnorm(8, 0, .5)
  
  dd &lt;- genData(n, d)
  dd &lt;- trtAssign(dd, grpName = &quot;rx&quot;)
  dd &lt;- addColumns(drx, dd)
  
  dd[, grp := setgrp(a, b, c), keyby = id]
  
  dd[]
  
}</code></pre>
<p>Looking at a single data set, we can see that <code>theta</code> is close to, but is not exactly 0, as we would typically do in simulation using a frequentist framework (where the parameters are presumed known).</p>
<pre class="r"><code>set.seed(298372)

defs &lt;- s_define()
s_generate(10, defs)</code></pre>
<pre><code>##     id a b c theta rx     y grp
##  1:  1 0 0 1  0.34  0 -3.45   4
##  2:  2 1 0 1  0.78  1  3.20   6
##  3:  3 0 0 0 -0.28  1  7.29   1
##  4:  4 0 1 0 -0.37  1  2.76   3
##  5:  5 0 0 0 -0.28  0 -0.48   1
##  6:  6 1 0 0 -0.25  0  1.09   2
##  7:  7 0 0 0 -0.28  0 -1.45   1
##  8:  8 0 0 1  0.34  1 -5.78   4
##  9:  9 1 0 0 -0.25  1  2.97   2
## 10: 10 1 0 0 -0.25  0 -1.25   2</code></pre>
</div>
<div id="model-fitting" class="section level4">
<h4>Model fitting</h4>
<p>The models here are precisely how I defined it in the last post. The code is a bit involved, so I’m not including it - let me know if you’d like to see it. For each data set, I fit a set of subgroup-specific linear regression models (as well as an overall model that ignored the subgroups), in addition to the two Bayesian models described in the previous post. Each replication defines the data, generates a new data set, and estimates the three different models before returning the results.</p>
<pre class="r"><code>s_model &lt;- function(dd, mod_pool, mod_nopool) {
  ...
}

s_replicate &lt;- function(x, n, mod_pool, mod_nopool) {
  
  set_cmdstan_path(path = &quot;/.../cmdstan/2.25.0&quot;)
  
  defs &lt;- s_define()
  generated_data &lt;- s_generate(n, defs)
  estimates &lt;- s_model(generated_data, mod_pool, mod_nopool)

  estimates[]
}</code></pre>
<p>The computation is split up so that 50 multi-core computing nodes run 50 replications. There’s actually parallelization in parallel, as each of the nodes has multiple processors so the Bayesian models can be estimated with parallel chains:</p>
<pre class="r"><code>set_cmdstan_path(path = &quot;/gpfs/share/apps/cmdstan/2.25.0&quot;)

model_pool &lt;- cmdstan_model(&quot;/.../subs_pool_hpc.stan&quot;)
model_nopool &lt;- cmdstan_model(&quot;/.../subs_nopool_hpc.stan&quot;)

job &lt;- Slurm_lapply(
  X = 1:2500, 
  FUN = s_replicate, 
  n = 150,
  mod_pool = model_pool,
  mod_nopool = model_nopool,
  njobs = 50, 
  mc.cores = 4L,
  job_name = &quot;i_subs&quot;,
  tmp_path = &quot;/.../scratch&quot;,
  plan = &quot;wait&quot;,
  sbatch_opt = list(time = &quot;12:00:00&quot;, partition = &quot;cpu_short&quot;, `mem-per-cpu` = &quot;5G&quot;),
  export = c(&quot;s_define&quot;, &quot;s_generate&quot;, &quot;s_model&quot;),
  overwrite = TRUE
)

job
res &lt;- Slurm_collect(job)

save(res, file = &quot;/.../sub_0.rda&quot;)</code></pre>
</div>
</div>
<div id="results" class="section level3">
<h3>Results</h3>
<p>The figure shows the results from 80 models. Each column is a different subgroup (and the last is the overall treatment effect estimate). The intervals are the 95% credible intervals from the Bayesian models, and the 95% confidence interval from the linear regression model. The intervals are color coded based on whether the interval includes 0 (grey) or not (red). The red intervals are cases where we might incorrectly conclude that there is indeed some sort of effect. There are many more red lines for the linear regression estimates compared to either of the Bayesian models:</p>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>For the full set of 2500 replications, about 5% of the intervals from the <em>pooled</em> Bayes did not include 0, lower than the <em>unpooled</em> model, and far below the approach using individual subgroup regression models:</p>
<pre><code>##    pooled unpooled   lm
## 1:  0.051     0.11 0.37</code></pre>
<p>I started off the last post by motivating this set of simulations with an experience I recently had with journal reviewers who were skeptical of an analysis of a subgroup effect size. I’m not sure that the journal reviewers would buy the approach suggested here, but it seems that pooling estimates across subgroups provides a viable way to guard against making overly strong statements about effect sizes when they are not really justified.</p>
</div>
