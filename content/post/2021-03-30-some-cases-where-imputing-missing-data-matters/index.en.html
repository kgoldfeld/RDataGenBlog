---
title: 
  "The case of three MAR mechanisms: when is multiple imputation mandatory?"
author: 
date: '2021-03-30'
slug: []
categories: []
tags:
  - R
  - Missing data
type: ''
subtitle: ''
image: ''
output:
  blogdown::html_page:
    anchor_sections: no
draft: TRUE
---

<script src="{{< blogdown/postref >}}index.en_files/header-attrs/header-attrs.js"></script>


<p>I thought I’d written about this before, but I searched through my posts and I couldn’t find what I was looking for. If I am repeating myself, my apologies. I <a href="https://www.rdatagen.net/post/musings-on-missing-data/" target="_blank">explored</a> missing data two years ago, using directed acyclic graphs (DAGs) to help understand the various missing data mechanisms (MAR, MCAR, and MNAR). The DAGs provide insight into when it is appropriate to use observed data to get unbiased estimates of population quantities even though some of the observations are missing information.</p>
<p>In that original post, I mentioned I might have more to say at some point in the future. Well, two years later I am again thinking about missing data, this time in the context of an ongoing randomized controlled trial. The research team has been discussing various ways to address potential biases that missing information might be introducing into the analysis. The group has decided that we need to use imputation to fill in the missing data, but I wanted to be clear why this added step is called for. After all, it is quite well known that imputation may not be necessary in light of missing data (see this <a href="https://statisticalhorizons.com/ml-is-better-than-mi" target="_blank">post</a>, for example.)</p>
<p>I’ve created three scenarios with data missing at random (MAR), where the probability of missingness is a function of observed data. In the first scenario, the treatment effect can surprisingly be estimated simply by comparing the means, no adjustment or imputation needed. In the second case, comparing the means directly is not appropriate, but adjustment for the predictor of missingness is sufficient; again, no imputation needed. And in the third case, neither a simple comparison nor a modeling adjustment do the trick; imputation is mandatory.</p>
<div id="a-little-background-for-context" class="section level3">
<h3>A little background for context</h3>
<p>The actual RCT is considerably more complicated than I am describing here, but this is the general idea. Individuals are randomized to one of two study arms <span class="math inline">\(A\)</span>, where <span class="math inline">\(A_i=1\)</span> if patient <span class="math inline">\(i\)</span> is in the treatment arm, and <span class="math inline">\(A_i = 0\)</span> if the the patient is in the control arm. We measure the outcome <span class="math inline">\(Y\)</span> at two time points, so we have <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span>; our primary interest, however, is <span class="math inline">\(Y_2\)</span>. We measure a key covariate <span class="math inline">\(X\)</span> that influences both <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span>. This is the true underlying DAG:</p>
<p><img src="img/study_DAG.png" style="width:40.0%" /></p>
<p>The challenge is that, for some patients, the second measurement <span class="math inline">\(Y_2\)</span> is missing, and we believe that <span class="math inline">\(Y_1\)</span> is a good predictor of the missingness pattern. But before getting into this (which is Case #3), I’ll start with a simpler scenario.</p>
</div>
<div id="case-1" class="section level3">
<h3>Case #1</h3>
<p>In the first scenario, there is only a single outcome measurement <span class="math inline">\(Y\)</span>, and we have measured <span class="math inline">\(X\)</span>. The simplified DAG looks like this:</p>
<p><img src="img/study_simple_DAG.png" style="width:30.0%" /></p>
<p>Unfortunately, we’ve only been able to collect the outcome measurement <span class="math inline">\(Y\)</span> for a subset of the sample, so that the observed <span class="math inline">\(Y^*\)</span> includes missing values for some subjects. The missing data mechanism is MAR, because the level of the observed baseline covariate <span class="math inline">\(X\)</span> determines the probability of observing <span class="math inline">\(Y\)</span>. The indicator <span class="math inline">\(R_y = 1\)</span> when we do observe <span class="math inline">\(Y\)</span> and <span class="math inline">\(R_y = 0\)</span> when we do not.</p>
<p><img src="img/MAR_1_DAG.png" style="width:35.0%" /></p>
<p>I’ll go ahead and simulate data based on this first DAG. In case you’d like to replicate, here are the libraries necessary for the simulations:</p>
<pre class="r"><code>library(simstudy)
library(ggplot2)
library(broom)
library(data.table)
library(mice)</code></pre>
<p>The data definitions establish the relationship between <span class="math inline">\(A\)</span>, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> (the treatment effect of <span class="math inline">\(A\)</span> on <span class="math inline">\(Y\)</span> is 2.5) as well as create a missingness mechanism for <span class="math inline">\(Y\)</span> that is a function <span class="math inline">\(X\)</span>; subjects with higher values of <span class="math inline">\(X\)</span> are more likely to have missing outcome measurements.</p>
<pre class="r"><code>def1 &lt;- defData(varname = &quot;x&quot;, formula=0.5, dist = &quot;binary&quot;)
def2 &lt;- defDataAdd(varname = &quot;y&quot;, formula = &quot;5 + 5*x + 2.5*a&quot;, variance = 2)
defm &lt;- defMiss(varname = &quot;y&quot;, formula = &quot;-3.5 + 2.3*x&quot;, logit.link = TRUE)</code></pre>
<p>To generate the observed data with missing data, we first generate a complete data set (based on the data definitions), and then we generate a missing data matrix, which finally gives us the observed data set which includes <span class="math inline">\(\text{NA}\)</span>’s for about 13% of the <span class="math inline">\(Y\)</span>’s.</p>
<pre class="r"><code>set.seed(17236)

dd &lt;- genData(500, def1)
dd &lt;- trtAssign(dd, grpName = &quot;a&quot;)
dd &lt;- addColumns(def2, dd)

ddmiss &lt;- genMiss(dd, defm, id = &quot;id&quot;)
ddobs &lt;- genObs(dd, ddmiss, id = &quot;id&quot;)

ddobs</code></pre>
<pre><code>##       id x a    y
##   1:   1 0 0  6.1
##   2:   2 1 0  9.2
##   3:   3 1 1 11.6
##   4:   4 0 0  4.5
##   5:   5 1 1   NA
##  ---             
## 496: 496 0 0  5.8
## 497: 497 0 1  7.3
## 498: 498 0 1  6.9
## 499: 499 1 1 11.1
## 500: 500 1 0 10.0</code></pre>
<p>Using the full data set <code>dd</code> (without any missing data), we can get a point estimate of the treatment effect <span class="math inline">\(\delta\)</span> merely by calculating</p>
<p><span class="math display">\[\hat{\delta} = \bar{Y}_{a=1} - \bar{Y}_{a=0}\]</span></p>
<pre class="r"><code>dd[, .(avg = mean(y)), keyby = a][ , avg - shift(avg)][2]</code></pre>
<pre><code>## [1] 2.5</code></pre>
<p>There is no reason to believe that the observed data means are the same as the complete data set means. That is, it is not likely that <span class="math inline">\(\bar{Y^*}_{a=1}\)</span> = <span class="math inline">\(\bar{Y}_{a=1}\)</span> or <span class="math inline">\(\bar{Y^*}_{a=0}\)</span> = <span class="math inline">\(\bar{Y}_{a=0}\)</span>. Observations with higher values of <span class="math inline">\(X\)</span> (and thus higher values of <span class="math inline">\(Y\)</span>) are more likely to have missing <span class="math inline">\(Y\)</span>’s, so the average observed values in both treatment groups should be lower. This seems to be the case here:</p>
<pre class="r"><code>dd[, .(avg = mean(y)), keyby = a]</code></pre>
<pre><code>##    a  avg
## 1: 0  7.5
## 2: 1 10.0</code></pre>
<pre class="r"><code>ddobs[, (avg = mean(y, na.rm = TRUE)), keyby = a]</code></pre>
<pre><code>##    a  V1
## 1: 0 7.2
## 2: 1 9.7</code></pre>
<p>In the real world, we can only estimate the treatment effect <span class="math inline">\(\delta^*\)</span> with the data that we have:
<span class="math display">\[\hat{\delta}^* = \bar{Y}_{a=1}^* - \bar{Y}_{a=0}^*\]</span></p>
<p>It looks like, in this case at least, the bias in estimates of the means are in the same direction, so that the estimate of the treatment effect based on the <em>difference</em> of means in the observed data is unbiased:</p>
<pre class="r"><code>ddobs[!is.na(y), .(avg = mean(y)), keyby = a][ , avg - shift(avg)][2]    </code></pre>
<pre><code>## [1] 2.5</code></pre>
<p>If this is the case more generally for data sets generated using this mechanism, we may not need to worry at all about the missing data mechanism; even though we know it is MAR, we might be able to treat it as MCAR, and just use the observed measurements only, without any adjustment or imputation.</p>
<p>Simulating 2500 data sets using steps outlined above provides insight into the nature of the bias. (I’ve provided generic code for generating repeated data sets in the <a href="#addendum">addendum</a>.) The estimates based on the complete data set are shown on the <span class="math inline">\(x\)</span> axis, and the observed data estimates are on the <span class="math inline">\(y\)</span> axis. The dotted lines show the average of the estimates for the complete and observed data sets, respectively.</p>
<p>For both treatment arms, the average estimate from the complete data sets is centered around the true value (used in the data generation process). As expected (since higher values of Y are likely to be missing), the average estimate for each arm is biased downwards when we do not take into consideration the missingness.</p>
<p><img src="img/MAR_1_y.png" style="width:70.0%" /></p>
<p>However, the bias is removed when we consider the treatment effect, which is our primary interest. In this (perhaps overly) simplistic scenario, there is no price to pay when ignoring the missing data. Both estimates are centered around 2.5, the true value.</p>
<p><img src="img/MAR_1_diff.png" style="width:40.0%" /></p>
</div>
<div id="case-2" class="section level3">
<h3>Case #2</h3>
<p>The second example differs from the first only in one respect: the size of the intervention effect depends on the baseline covariate <span class="math inline">\(X\)</span> (the line drawn from <span class="math inline">\(X\)</span> to the arrow connecting <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span> represents this effect modification).</p>
<p><img src="img/MAR_2_DAG.png" style="width:35.0%" /></p>
<p>In the example, <span class="math inline">\(\delta_0 = 1\)</span> for the sub-population with <span class="math inline">\(X = 0\)</span>, and <span class="math inline">\(\delta_1 = 4\)</span> for the sub-population with <span class="math inline">\(X = 1\)</span>. If the population were evenly distributed between <span class="math inline">\(X=0\)</span> and <span class="math inline">\(X=1\)</span>, then we would observe an overall effect <span class="math inline">\(\delta = 2.5\)</span>.</p>
<pre class="r"><code>d1 &lt;- defData(varname = &quot;x&quot;, formula=0.5, dist = &quot;binary&quot;)
d2 &lt;- defDataAdd(varname = &quot;y&quot;, formula = &quot;6 + 1*a + 2*x + 3*a*x&quot;, variance = 2)
dm &lt;- defMiss(varname = &quot;y&quot;, formula = &quot;-3.5 + 2.3*x&quot;, logit.link = TRUE)</code></pre>
<p>But this time around, if we go ahead and naïvely estimate <span class="math inline">\(\delta^* = \bar{Y}_{a=1}^* - \bar{Y}_{a=0}^*\)</span>, the estimate will be biased.</p>
<p><img src="img/MAR_2_diff.png" style="width:40.0%" /></p>
<p>The reason for this bias is that the mix of <span class="math inline">\(X\)</span> in the observed sample is different from the complete sample (and population); since <span class="math inline">\(X\)</span> influences the effect size this change impacts the overall unadjusted estimate. In the complete data set <span class="math inline">\(P(X=1) = 0.50\)</span>, but in an observed data set with missing values <span class="math inline">\(P^*(X=1) = 0.44\)</span> (this can be confirmed using the assumptions from the data generation process, but I’ll let you do that as an exercise if you’d like.) The population average treatment effect is <span class="math inline">\(P(X=0) \times 1 + P(X=1) \times 4 = 2.5\)</span>. And in the data set with missing data <span class="math inline">\(P^*(X=0) \times 1 + P^*(X=1) \times 4 = 0.56 \times 1 + 0.44 \times 4 = 2.3\)</span>.</p>
<p>We can still estimate the treatment effect if we adjust for <span class="math inline">\(X\)</span> in a regression model, or just take the difference in means within each level of <span class="math inline">\(X\)</span>. These estimates are unbiased:</p>
<p><img src="img/MAR_2_adj.png" style="width:70.0%" /></p>
<p>If we want to recover the population average treatment effect, we can reweight the group-level treatment effects by the distribution of <span class="math inline">\(X\)</span> in complete sample (since <span class="math inline">\(X\)</span> is fully observed). No imputation is needed.</p>
</div>
<div id="case-3" class="section level3">
<h3>Case #3</h3>
<p>Now we are back to the original motivating scenario. The missing data mechanism is depicted in the next DAG. Those with higher scores in the first period are more likely to have missing values in the second time period, perhaps because they have improved sufficiently and no longer feel like participating in the study.</p>
<p><img src="img/MAR_3_DAG.png" style="width:40.0%" /></p>
<p>The DAG is implemented with these definitions:</p>
<pre class="r"><code>def1 &lt;- defData(varname = &quot;x&quot;, formula=0.5, dist = &quot;binary&quot;)

def2 &lt;- defDataAdd(varname = &quot;y1&quot;, formula = &quot;5 + a*2.5 + 5*x&quot;, variance = 2)
def2 &lt;- defDataAdd(def2, &quot;y2&quot;, formula = &quot;1 + y1 + 5*x&quot;, variance = 2)

defm &lt;- defMiss(varname = &quot;y2&quot;, formula = &quot;-4.5 + 0.3*y1&quot;, logit.link = TRUE)</code></pre>
<p>In this case, simply comparing the means in the data sets with missing data provides a biased estimate - we can see this on the left; the argument is similar to the one I made in the previous scenario. If we opt to control for <span class="math inline">\(Y_1\)</span>, we introduce all sorts of biases, as <span class="math inline">\(Y_1\)</span> is a mediator between <span class="math inline">\(A\)</span> and <span class="math inline">\(Y_2\)</span>, as well as a collider. (I’ve written about the dangers of controlling for post-intervention variables <a href="https://www.rdatagen.net/post/be-careful/" target="_blank">here</a> and the need to be careful with colliders <a href="https://www.rdatagen.net/post/another-reason-to-be-careful-about-what-you-control-for/" target="_blank">here</a>.)</p>
<p><img src="img/MAR_3_trt.png" style="width:70.0%" /></p>
<p>Since neither a simple comparison of means nor an adjusted model will suffice here, our only option is to use multiple imputation, which in <code>R</code> can be can be implemented with the package <a href="https://amices.org/mice/" target="_blank">mice</a>. Below, I am showing code that generates 20 imputed data sets, fits models for each of them, and pools the results to provide a single estimate and measure of uncertainty.</p>
<pre class="r"><code>library(mice)

imp &lt;- mice(ddobs[,-&quot;id&quot;], m=20, maxit=5, print=FALSE)
fit &lt;- with(imp, lm(y2 ~ a))
results &lt;- summary(pool(fit))</code></pre>
<p>Multiple imputation has been applied to the same 2500 data sets with missing data that are represented in the biased estimate plots. The plot below shows a pretty strong correlation with the estimates from the full data mode, and both are centered at the true population effect of 2.5.</p>
<p><img src="img/MAR_3_imp.png" style="width:40.0%" /></p>
<p>The takeaway from all this is that while multiple imputation is not always necessary, if you think there are potentially unmeasured confounders or post-intervention measures that are conceivably in the mix, a multiple imputation approach might be wiser than trying to adjust your way out of the problem.</p>
<p>I plan on implementing a Bayesian model that treats the missing data as parameters. If I can get that working, I will share it here, of course.</p>
<p><br /></p>
<p><a name="addendum"></a></p>
</div>
<div id="addendum" class="section level3">
<h3>Addendum</h3>
<p>In case you’d like to play around with other scenarios, I’m including the code that will allow you to repeatedly sample data sets. Just provide you our data definitions.</p>
<pre class="r"><code>s_generate &lt;- function(n) {
  
  dd &lt;- genData(n, d1)
  dd &lt;- trtAssign(dd, grpName = &quot;a&quot;)
  dd &lt;- addColumns(d2, dd)
  
  dmiss &lt;- genMiss(dd, dm, id = &quot;id&quot;)
  dobs &lt;- genObs(dd, dmiss, id = &quot;id&quot;)
  
  return(list(dd, dobs))
  
}

s_replicate &lt;- function(n) {
  
  dsets &lt;- s_generate(n)
  
  diff.complete &lt;- dsets[[1]][, .(avg = mean(y2)), keyby = a][ , avg - shift(avg)][2]    
  diff.obs&lt;- dsets[[2]][!is.na(y2), .(avg = mean(y2)), keyby = a][ , avg - shift(avg)][2] 
  
  est.complete &lt;- coef(lm(y2 ~ a, data = dsets[[1]]))[&quot;a&quot;]
  est.obs &lt;- coef(lm(y2 ~ y1 + a, data = dsets[[2]]))[&quot;a&quot;]
  
  imp &lt;- mice(dsets[[2]][,-&quot;id&quot;], m=20, maxit=5, print=FALSE)
  fit &lt;- with(imp, lm(y2 ~ a))
  pooled.ests &lt;- summary(pool(fit))
  est.impute &lt;- pooled.ests$estimate[2]
  
  return(data.table(diff.complete, est.complete, diff.obs, est.obs, est.impute))
}

results &lt;- rbindlist(mclapply(1:2500, function(x) s_replicate(300), mc.cores = 4))</code></pre>
</div>
