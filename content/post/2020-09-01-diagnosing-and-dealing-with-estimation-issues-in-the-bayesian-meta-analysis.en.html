---
title: Diagnosing and dealing with degenerate estimation in a Bayesian meta-analysis
author: Keith Goldfeld
date: '2020-09-01'
slug: diagnosing-and-dealing-with-estimation-issues-in-the-bayesian-meta-analysis
categories: []
tags:
  - R
  - Bayesian model
  - Stan
type: ''
subtitle: ''
image: ''
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>The federal government recently granted emergency approval for the use of antibody rich blood plasma when treating hospitalized COVID-19 patients. This announcement is <a href="https://www.statnews.com/2020/08/24/trump-opened-floodgates-convalescent-plasma-too-soon/" target="_blank">unfortunate</a>, because we really don’t know if this promising treatment works. The best way to determine this, of course, is to conduct an experiment, though this approval makes this more challenging to do; with the general availability of convalescent plasma (CP), there may be resistance from patients and providers against participating in a randomized trial. The emergency approval sends the incorrect message that the treatment is definitively effective. Why would a patient take the risk of receiving a placebo when they have almost guaranteed access to the therapy?</p>
<p>This doesn’t obviate the fact that we still need to figure out if CP is effective. Last month, I described an <a href="https://www.rdatagen.net/post/simulating-mutliple-studies-to-simulate-a-meta-analysis/" target="_blank">approach</a> to pooling data across multiple different, but similar, studies as way to leverage information that is being generated around the country and world from ongoing trials. I was particularly cryptic about the nature of the research, because the paper describing the details of the proposed research had not yet been <a href="https://jamanetwork.com/journals/jama/fullarticle/2768851" target="_blank">published</a>. Now, the project has a name (<strong>COMPILE</strong>), a <a href="https://med.nyu.edu/departments-institutes/population-health/divisions-sections-centers/biostatistics/research/continuous-monitoring-pooled-international-trials-convalescent-plasma-covid19-hospitalized-patients" target="_blank">website</a>, and most importantly, participating studies committed to sharing data.</p>
<p>In preparation for the analyses, we have been developing a statistical plan, which is based on a pooled Bayesian model similar to what I <a href="https://www.rdatagen.net/post/a-bayesian-model-for-a-simulated-meta-analysis/" target="_blank">described</a>) earlier. The Bayesian approach offers much needed flexibility in this context when we must make a principled decision as quickly as possible. Indeed, now that the <a href="https://www.fda.gov/news-events/press-announcements/fda-issues-emergency-use-authorization-convalescent-plasma-potential-promising-covid-19-treatment" target="_blank">emergency approval</a> has been granted, there is even more urgency. The study’s Data Safety and Monitoring Board will be evaluating the data frequently, which a Bayesian approach accommodates quite well. (I imagine I will have much to write about over the next few months as we try to better understand the implications and challenges of taking this path.)</p>
<p>In this post, I am describing a nitty-gritty issue related to Markov chain Monte Carlo (MCMC) estimation: stability. It may not sound super-exciting, but stable estimation is key to drawing correct inferences from the estimated posterior distribution. As a model becomes more complex, the MCMC estimation in <code>stan</code> can be plagued by degenerate sampling caused by divergent transitions. MCMC works well when the algorithm takes the sampler across the full posterior distribution without getting stuck, but all bets are off when the process breaks down.</p>
<p>Using a slightly simplified version of the data and model we are proposing for COMPILE, I want to show how to see if things have gotten stuck at some point, and then present a possible solution to getting things unstuck. (I highly encourage you to look <a href="http://mc-stan.org/bayesplot/articles/visual-mcmc-diagnostics.html" target="_blank">here</a> and <a href="https://cran.r-project.org/web/packages/bayesplot/vignettes/visual-mcmc-diagnostics.html" target="_blank">here</a> for many more details if this is indeed exciting to you.)</p>
<div id="the-model" class="section level3">
<h3>The model</h3>
<p>The studies included in this meta-analysis will be similar in that they are all providing 1- or 2-units of plasma to the patients randomized to therapy. However, there will be differences with respect to the control arm: studies will use a saline solution, non-convalescent plasma, or usual care (in the last case, the study is not blinded). We need to account for the possibility that the treatment effect will vary slightly depending on the type of control. In this case, I am assuming a binary outcome (though in the actual study we are using an ordinal outcome and a proportional odds model). Here is the logistic model:
<span class="math display">\[\text{logodds}(P(y_{ik}=1)) = \alpha_0 + b_k + \delta_k x_{ik}\]</span>
where the observed data are <span class="math inline">\(y_{ik}\)</span>, the indicator for the outcome (say death) of patient <span class="math inline">\(i\)</span> in study <span class="math inline">\(k\)</span>, and <span class="math inline">\(x_{ik}\)</span>, an indicator set to <span class="math inline">\(1\)</span> if the patient is in the control arm, 0 otherwise. The parameters are <span class="math inline">\(\alpha_0\)</span>, the global logodds of death (note here the intercept represents the treatment condition, not the control condition as is usually the case); <span class="math inline">\(b_k\)</span>, the study-specific logodds of death given treatment; and <span class="math inline">\(\delta_k\)</span>, the “control” effect (a logodds ratio) specific to study <span class="math inline">\(k\)</span>.</p>
<p>The really interesting aspect of this model is <span class="math inline">\(\delta_k\)</span>. This effect is a function of three components - the study site, the control-type, and the the overall/global treatment effect. We are assuming that there is a tendency for studies to vary around a control-type effect average. So, with three controls <span class="math inline">\(c\in {1,2,3}\)</span>:
<span class="math display">\[\delta_k \sim N(\delta_c, \eta_0),\]</span></p>
<p>determined by the control-type of study <span class="math inline">\(k\)</span>. Furthermore, we assume that the control-type effects, the <span class="math inline">\(\delta_c\)</span>’s, vary around a global effect <span class="math inline">\(\Delta\)</span>:
<span class="math display">\[\delta_c \sim N(\Delta, \eta).\]</span>
We assume that <span class="math inline">\(\eta\)</span> is quite small; that is, the control effects will likely be very similar. We are not actually interested in <span class="math inline">\(\eta\)</span> so we do not attempt to estimate it. However <span class="math inline">\(\eta_0\)</span>, the variability across studies, is important, so we <em>will</em> estimate that.</p>
</div>
<div id="generating-data" class="section level3">
<h3>Generating data</h3>
<p>To start, here are the <code>R</code> packages I am using to generate the data.</p>
<pre class="r"><code>library(simstudy)
library(data.table)</code></pre>
<p>Next, I define the study level parameters: the study-specific intercept <span class="math inline">\(b_k\)</span> and the study-specific “control” effect <span class="math inline">\(\delta_k\)</span>, which is a function of the control-type. Note I do not specify a study-level “control” effect, there will just be natural variation across studies. The individual-level outcome <span class="math inline">\(y_{ik}\)</span> is defined as a function of study parameters. The overall treatment effect is <span class="math inline">\(\Delta = 0.5\)</span> on the logodds ratio scale.</p>
<pre class="r"><code>def_s &lt;- defDataAdd(varname = &quot;b_k&quot;, formula = 0, variance = 0.025)
def_s &lt;- defDataAdd(
    def_s, varname = &quot;delta_k&quot;, 
    formula = &quot;(c_type==1) * 0.4 + (c_type==2) * 0.5 + (c_type==3) * 0.6&quot;, 
    dist = &quot;nonrandom&quot;
  )
  
def_i &lt;- defDataAdd(
    varname = &quot;y&quot;, formula = &quot;-1 + b_k + rx * delta_k&quot;, 
    dist = &quot;binary&quot;, link = &quot;logit&quot;)</code></pre>
<p>I am generating 7 studies with under each control type, for a total of 21 studies. Each study has 50 patients, 25 in each arm, for a total of 1050 patients.</p>
<pre class="r"><code>dc &lt;- genData(3, id = &quot;c_type&quot;)
    
ds &lt;- genCluster(dc, &quot;c_type&quot;, numIndsVar = 7, level1ID = &quot;site&quot;)
ds &lt;- addColumns(def_s, ds)

di &lt;- genCluster(ds, &quot;site&quot;, 50, &quot;id&quot;)
di &lt;- trtAssign(di, 2, strata = &quot;site&quot;, grp = &quot;rx&quot;)
di &lt;- addColumns(def_i, di)</code></pre>
</div>
<div id="estimation-using-stan" class="section level3">
<h3>Estimation using stan</h3>
<p>I am using <code>rstan</code> directly to sample from the posterior distribution, as opposed to using a more user-friendly package like <code>brms</code> or <code>rstanarm</code>. I’ve actually been warned against taking this approach by folks at stan, because it can be more time consuming and could lead to problems of the sort that I am showing you how to fix. However, I find building the model using stan code very satisfying and illuminating; this process has really given me a much better appreciation of the Bayesian modeling. And besides, this model is odd enough that trying to shoehorn it into a standard <code>brms</code> model might be more trouble than it is worth.</p>
<pre class="r"><code>library(rstan)
library(ggplot)
library(bayesplot)</code></pre>
<p>The stan code, which can reside in its own file, contains a number of <em>blocks</em> that define the model. The <code>data</code> block specifies the data will be provided to the model; this can include summary data as well as raw data. The <code>parameters</code> block is where you specify the parameters of the model that need to be estimated. The <code>transformed parameters</code> block includes another set of parameters that will be estimated, but are a function of parameters defined in the previous block. And in this case, the last block is the <code>model</code> where prior distributions are specified as well as the likelihood (outcome) model. Rather than walk you through the details here, I will let you study a bit and see how this relates to the model I specified above.</p>
<pre class="stan"><code>data {
  
  int&lt;lower=0&gt; N;                // number of observations
  int&lt;lower=0&gt; C;                // number of control types
  int&lt;lower=1&gt; K;                // number of studies
  int y[N];                      // vector of categorical outcomes
  int&lt;lower=1,upper=K&gt; kk[N];    // site for individual
  int&lt;lower=0,upper=1&gt; ctrl[N];  // treatment or control
  int&lt;lower=1,upper=C&gt; cc[K];    // specific control for site
  
}

parameters {

  real alpha;               // overall intercept for treatment
  vector[K] beta_k;         // site specific intercept
  real&lt;lower=0&gt; sigma_b;    // sd of site intercepts

  vector[K] delta_k;        // site specific treatment effect
  real&lt;lower=0&gt;  eta_0;     // sd of delta_k (around delta_c)

  vector[C] delta_c;        // control-specific effect
  real Delta;               // overall control effect
  
}

transformed parameters{ 
  
  vector[N] yhat;

  for (i in 1:N)  
      yhat[i] = alpha +  beta_k[kk[i]] + (ctrl[i] * (delta_k[kk[i]]));

}

model {
  
  // priors
  
  alpha ~ student_t(3, 0, 2.5);
  beta_k ~ normal(0, sigma_b);
  sigma_b ~ cauchy(0, 1);
  eta_0 ~ cauchy(0, 1);

  for (k in 1:K)
      delta_k[k] ~ normal(delta_c[cc[k]], eta_0);

  delta_c ~ normal(Delta, 0.5);
  Delta ~ normal(0, 10);
  
  // likelihood/outcome model
  
  y ~ bernoulli_logit(yhat);
}</code></pre>
<p>We need to compile the stan code so that it can be called from the <code>R</code> script:</p>
<pre class="r"><code>rt_c &lt;- stanc(&quot;binary_outcome.stan&quot;)
sm_c &lt;- stan_model(stanc_ret = rt_c, verbose=FALSE)</code></pre>
<p>And here is the <code>R</code> code that prepares the data for the stan program and then samples from the posterior distribution. In this case, I will be using 4 different Monte Carlo chains of 2500 draws each (after allowing for 500 warm-up draws), so we will have a actual sample size of 10,000.</p>
<pre class="r"><code>N &lt;- nrow(di) ;                       
C &lt;- di[, length(unique(c_type))]     
K &lt;- di[, length(unique(site))]       
y &lt;- as.numeric(di$y)                 
kk &lt;- di$site                         
ctrl &lt;- di$rx                         
cc &lt;- di[, .N, keyby = .(site, c_type)]$c_type  
    
sampdat &lt;- list(N=N, C=C, K=K, y=y, kk=kk, ctrl=ctrl, cc=cc)
    
fit_c &lt;-  sampling(
  sm_c, data = sampdat, iter = 3000, warmup = 500, 
  show_messages = FALSE, cores = 4, refresh = 0,
  control = list(adapt_delta = 0.8)
)</code></pre>
</div>
<div id="inspecting-the-posterior-distribution" class="section level3">
<h3>Inspecting the posterior distribution</h3>
<p>Assuming that everything has run smoothly, the first thing to do is to make sure that the MCMC algorithm adequately explored the full posterior distribution in the sampling process. We are typically interested in understanding the properties of the distribution, like the mean or median, or 95% credible intervals. To have confidence that these properties reflect the true posterior probabilities, we need to be sure that the sample they are drawn from is a truly representative one.</p>
<p>A quick and effective way to assess the “representativeness” of the sample is to take a look at the trace plot for a particular parameter, which literally tracks the path of the MCMC algorithm through the posterior distribution. Below, I’ve included two trace plots, one for <span class="math inline">\(\Delta\)</span>, the overall effect, and the other for <span class="math inline">\(\eta_0\)</span>, the variability of a study’s control effect <span class="math inline">\(\delta_k\)</span> around <span class="math inline">\(\delta_c\)</span>. On the left, the plots appear as they should, with lines jumping up and down. However, these particular plots include red indicators where the algorithm got stuck, where there were <em>divergent transitions</em>. We really don’t want to see any of this indicators, because that is a sign that our sample is not representative of the posterior distribution.</p>
<pre class="r"><code>posterior_c &lt;- as.array(fit_c) 
lp_c &lt;- log_posterior(fit_c)
np_c &lt;- nuts_params(fit_c)

color_scheme_set(&quot;mix-brightblue-gray&quot;)

mcmc_trace(posterior_c, pars = &quot;Delta&quot;, np = np_c) + 
  xlab(&quot;Post-warmup iteration&quot;)

mcmc_trace(
  posterior_c, pars = &quot;Delta&quot;, np = np_c, window = c(1500, 1700)
) + 
  xlab(&quot;Post-warmup iteration&quot;)</code></pre>
<p><img src="/img/post-bayesdiag/trace_c.png" /></p>
<p>On the right in the figure above, I’ve zoomed in on steps 700 to 900 to see if we can see any patterns. And sure enough, we can. In the <span class="math inline">\(\Delta\)</span> plot, straight lines appear in the middle, evidence that the sampling for some of the chains did indeed get stuck. Likewise, the plot for <span class="math inline">\(\eta_0\)</span> shows flat lines near <span class="math inline">\(0\)</span>.</p>
<p>There’s an additional plot that shows the same thing but in a slightly more dramatic and comprehensive way. This plot (shown below) has a line for each step connecting the parameter estimates of that step. The red lines represent divergent transitions. The important thing to note here is that in all cases with divergent transitions, <span class="math inline">\(\eta_0\)</span> found itself close to <span class="math inline">\(0\)</span>. In other words, the sampling was getting stuck at this point, and this is the likely culprit for the sampling issues.</p>
<pre class="r"><code>color_scheme_set(&quot;darkgray&quot;)

parcoord_c &lt;-mcmc_parcoord(
  posterior_c, np = np_c, 
  pars = c(&quot;eta_0&quot;, &quot;sigma_b&quot;, &quot;alpha&quot;,
           &quot;delta_c[1]&quot;, &quot;delta_c[2]&quot;,&quot;delta_c[3]&quot;, &quot;Delta&quot;)
  )

parcoord_c +
  scale_x_discrete(expand = c(0.01, 0.01)) +
  theme(panel.background = element_rect(fill = &quot;grey90&quot;)) +
  ylim(-3, 3) +
  ggtitle(&quot;Original model specification&quot;)</code></pre>
<p><img src="/img/post-bayesdiag/parc_c.png" id="id" class="class" style="width:75.0%;height:75.0%" /></p>
</div>
<div id="a-remedy-for-divergent-transitions" class="section level3">
<h3>A remedy for divergent transitions</h3>
<p>In a moment, I will provide a brief illustration of the perils of divergent transitions, but before that I want to describe “non-centered parameterization,” an approach that can be used to mitigate divergence. The idea is that since sampling from a standard Gaussian or normal distribution is less likely to lead to problematic transitions, we should try to do this as much as possible. “In a non-centered parameterization we do not try to fit the group-level parameters directly, rather we fit a latent Gaussian variable from which we can recover the group-level parameters with a scaling and a translation.” (See <a href="https://mc-stan.org/users/documentation/case-studies/divergences_and_bias.html" target="_blank">here</a> for the source of this quote and much more.)</p>
<p>For example, in the original model specification, we parameterized <span class="math inline">\(\delta_k\)</span> and <span class="math inline">\(\delta_c\)</span> as</p>
<p><span class="math display">\[
\delta_k \sim N(\delta_c, \eta_0) \\
\delta_c \sim N(\Delta, 0.5)
\]</span></p>
<p><span class="math display">\[
\eta_0 \sim Cauchy(0, 1) \\
\Delta \sim N(0, 10),
\]</span></p>
<p>whereas using “non-centered” parameterization, we would incorporate two latent standard normal variables <span class="math inline">\(\theta_{rx}\)</span> and <span class="math inline">\(\theta_c\)</span> into the model. <span class="math inline">\(\delta_k\)</span> and <span class="math inline">\(\delta_c\)</span> have the same prior distribution as the original model, but we are now sampling from standard normal prior distribution:
<span class="math display">\[
\delta_k = \delta_c + \eta_0 \theta_{rx} \\
\theta_{rx} \sim N(0, 1) 
\]</span></p>
<p><span class="math display">\[
\delta_c = \Delta + 0.5\theta_c\\
\theta_c \sim N(0, 1)
\]</span></p>
<p><span class="math display">\[
\eta_0 \sim Cauchy(0, 1) \\
\Delta \sim N(0, 10).
\]</span></p>
<p>This transformation makes the path through the posterior distribution much smoother and, at least in this case, eliminates the divergent transitions.</p>
<p>Here is the stan code using non-centered parameterization (again, feel free to linger and study):</p>
<pre class="stan"><code>data {
  
  int&lt;lower=0&gt; N;                 // number of observations
  int&lt;lower=0&gt; C;                 // number of control types
  int&lt;lower=1&gt; K;                 // number of studies
  int y[N];                       // vector of categorical outcomes
  int&lt;lower=1,upper=K&gt; kk[N];     // site for individual
  int&lt;lower=0,upper=1&gt; ctrl[N];   // treatment or control
  int&lt;lower=1,upper=C&gt; cc[K];     // specific control for site

}

parameters {

  real alpha;               // overall intercept for treatment
   
  real&lt;lower=0&gt; sigma_b;
  real&lt;lower=0&gt;  eta_0;     // sd of delta_k (around delta)

  real Delta;               // overall control effect
  
   // non-centered parameterization
  
  vector[K] z_ran_rx;   // site level random effects (by period)
  vector[K] z_ran_int;  // individual level random effects 
  vector[C] z_ran_c;  // individual level random effects 

}

transformed parameters{ 
  
  vector[N] yhat;
  vector[K] beta_k;
  vector[K] delta_k;        // site specific treatment effect
  vector[C] delta_c;

  beta_k = sigma_b * z_ran_int + alpha;
  
  for (i in 1:C)
    delta_c[i] = 0.5 * z_ran_c[i] + Delta; 
  
  for (i in 1:K)
    delta_k[i] = eta_0 * z_ran_rx[i] + delta_c[cc[i]]; 
  
  for (i in 1:N)  
      yhat[i] = beta_k[kk[i]] + ctrl[i] * delta_k[kk[i]];

}

model {
  
  // priors
  
  alpha ~ student_t(3, 0, 2.5);

  z_ran_c ~ std_normal();
  z_ran_int ~ std_normal();  
  z_ran_rx ~ std_normal();  

  sigma_b ~ cauchy(0, 1);
  eta_0 ~ cauchy(0, 1);
  
  Delta ~ normal(0, 10);
  
  // outcome model
  
  y ~ bernoulli_logit(yhat);
}</code></pre>
<p>Looking at the trace plots from the non-centered model makes it clear that divergent transitions are no longer a problem. There are no red indicators, and the patterns of straight lines have been eliminated:</p>
<p><img src="/img/post-bayesdiag/trace_nc.png" /></p>
</div>
<div id="proceed-with-caution-if-you-ignore-the-divergence-warnings" class="section level3">
<h3>Proceed with caution if you ignore the divergence warnings</h3>
<p>If we don’t heed the warnings, how bad can things be? Well, it will probably depend on the situation, but after exploring with multiple data sets, I have convinced myself that it is probably a good idea to reduce the number of divergent transitions as close to 0 as possible.</p>
<p>I conducted an experiment by generating 100 data sets and fitting a model using both the original and non-centered parameterizations. I collected the posterior distribution for each data set and estimation method, and plotted the density curves. (In case you are interested, I used a parallel process running on a high-performance computing core to do this; running on my laptop, this would have taken about 5 hours, but on the HPC it ran in under 15 minutes.) The purpose of this was to explore the shapes of the densities across the different data sets. I know it is a little odd to use this frequentist notion of repeatedly sampling datasets to evaluate the performance of these two approaches, but I find it to be illuminating. (If you’re interested in the code for that, let me know.)</p>
<p>Below on the right, the plot of the posteriors from the non-centered parameterization shows variability in location, but is otherwise remarkably consistent in shape and scale. On the left, posterior densities show much more variation; some are quite peaked and others are even bi-modal. (While I am not showing this here, the densities from samples with more divergent transitions tend to diverge the most from the well-behaved densities on the right.)</p>
<p>Although the mean or median estimates from a divergent sample may not be too far off from its non-divergent counterpart, the more general description of the distribution may be quite far off the mark, making it likely that inferences too will be off the mark.</p>
<p><img src="/img/post-bayesdiag/post_plot.png" /></p>
</div>
