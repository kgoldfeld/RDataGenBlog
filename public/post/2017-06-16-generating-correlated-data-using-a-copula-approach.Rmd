---
title: 'Copulas and correlated data generation: getting beyond the normal distribution'
author: Keith Goldfeld
date: '2017-06-19'
slug: correlated-data-copula
categories: []
tags:
  - R
subtitle: ''
---

Using the `simstudy` package, it's possible to generate correlated data from a normal distribution using the function *genCorData*. I've wanted to extend the functionality so that we can generate correlated data from other sorts of distributions; I thought it would be a good idea to begin with binary and Poisson distributed data, since those come up so frequently in my work. `simstudy` can already accommodate more general correlated data, but only in the context of a random effects data generation process. This might not be what we want, particularly if we are interested in explicitly generating data to explore marginal models (such as a GEE model) rather than a conditional random effects model (a topic I explored in my [previous](https://www.rdatagen.net/post/marginal-v-conditional/) discussion). The extension can quite easily be done using *copulas*.  

Based on [this](https://en.wikipedia.org/wiki/Copula_%28probability_theory%29) definition, a copula is a "multivariate probability distribution for which the marginal probability distribution of each variable is uniform." It can be shown that $U$ is uniformly distributed if $U=F(X)$, where $F$ is the CDF of a continuous random variable $X$. Furthermore, if we can generate a multivariate $\mathbf{X}$, say $(X_1, X_2, ..., X_k)$ with a known covariance or correlation structure (e.g. exchangeable, auto-regressive, unstructured), it turns that the corresponding multivariate $\mathbf{U}, (U_1, U_2, ..., U_k)$ will maintain that structure. And in a final step, we can transform $\mathbf{U}$ to another random variable $\mathbf{Y}$ that has a target distribution by applying the inverse CDF $F_i^{-1}(U_i)$ of that target distribution to each $U_i$. Since we can generate a multivariate normal $\mathbf{X}$, it is relatively short leap to implement this copula algorithm in order to generate correlated data from other distributions. 

## Implementing the copula algorithm in R

While this hasn't been implemented just yet in `simstudy`, this is along the lines of what I am thinking:

```{r, echo = TRUE}
library(simstudy)
library(data.table)

set.seed(555)

# Generate 1000 observations of 4 RVs from a multivariate normal 
# dist - each N(0,1) - with a correlation matrix where rho = 0.4 

dt <- genCorData(1000, mu = c(0, 0, 0, 0), sigma = 1, 
                 rho = 0.4, corstr = "cs" )
dt
round(cor(dt[,-1]), 2)

### create a long version of the data set

dtM <- melt(dt, id.vars = "id", variable.factor = TRUE, 
            value.name = "X", variable.name = "seq")
setkey(dtM, "id")   # sort data by id
dtM[, seqid := .I]  # add index for each record

### apply CDF to X to get uniform distribution

dtM[, U := pnorm(X)]

### Generate correlated Poisson data with mean and variance 8
### apply inverse CDF to U

dtM[, Y_pois := qpois(U, 8), keyby = seqid]
dtM

### Check mean and variance of Y_pois

dtM[, .(mean = round(mean(Y_pois), 1), 
        var = round(var(Y_pois), 1)), keyby = seq]

### Check correlation matrix of Y_pois's - I know this code is a bit ugly
### but I just wanted to get the correlation matrix quickly.

round(cor(as.matrix(dcast(data = dtM, id~seq, 
                          value.var = "Y_pois")[,-1])), 2)
```

The correlation matrices for $\mathbf{X}$ and $\mathbf{Y_{Pois}}$ aren't too far off.

Here are the results for an auto-regressive (AR-1) correlation structure. (I am omitting some of the code for brevity's sake):

```{r, echo = TRUE}

# Generate 1000 observations of 4 RVs from a multivariate normal 
# dist - each N(0,1) - with a correlation matrix where rho = 0.4 

dt <- genCorData(1000, mu = c(0, 0, 0, 0), sigma = 1, 
                 rho = 0.4, corstr = "ar1" )

round(cor(dt[,-1]), 2)
```

```{r, echo = FALSE}
### create a long version of the data set

dtM <- melt(dt, id.vars = "id", variable.factor = TRUE, 
            value.name = "X", variable.name = "seq")
setkey(dtM, "id")   # sort data by id
dtM[, seqid := .I]  # add index for each record

### apply CDF to X

dtM[, U := pnorm(X)]

### Generate correlated Poisson data with mean and variance 8
### apply inverse CDF to U

dtM[, Y_pois := qpois(U, 8), keyby = seqid]
```

```{r, echo = TRUE}
### Check mean and variance of Y_pois

dtM[, .(mean = round(mean(Y_pois), 1), 
        var = round(var(Y_pois), 1)), keyby = seq]

### Check correlation matrix of Y_pois's

round(cor(as.matrix(dcast(data = dtM, id~seq, 
                          value.var = "Y_pois")[,-1])), 2)
```

Again - comparing the two correlation matrices - the original normal data, and the derivative Poisson data - suggests that this can work pretty well.

Using the last data set, I fit a GEE model to see how well the data generating process is recovered:

```{r, echo = TRUE}
library(geepack)

geefit <- geepack::geeglm(Y_pois ~ 1, data = dtM, family = poisson,
                          id = id, corstr = "ar1")

summary(geefit)

```

In the GEE output, alpha is an estimate of $\rho$. The estimated alpha is 0.399, quite close to 0.40, the original value used to generate the normally distributed data.

## Binary outcomes

We can also generate binary data:

``` {r, echo=TRUE}

### Generate binary data with p=0.5 (var = 0.25)

dtM[, Y_bin := qbinom(U, 1, .5), keyby = seqid]
dtM

### Check mean and variance of Y_bin

dtM[, .(mean = round(mean(Y_bin), 2), 
        var = round(var(Y_bin), 2)), keyby = seq]

### Check correlation matrix of Y_bin's

round(cor(as.matrix(dcast(data = dtM, id~seq, 
                          value.var = "Y_bin")[,-1])), 2)

```

The binary data are correlated, but the correlation coefficient doesn't replicate as well as the Poisson distribution. While both the Poisson and binary CDF's are  discontinuous, the extreme jump in the binary CDF leads to this discrepancy. Values that are relatively close to each other on the normal scale, and in particular on the uniform scale, can be 'sent' to opposite ends of the binary scale (that is to 0 and to 1) if they straddle the cutoff point $p$ (the probability of the outcome in the binary distribution); values similar in the original data are very different in the target data. This bias is partially attenuated by values far apart on the uniform scale yet falling on the same side of $p$ (both driven to 0 or both to 1); in this case values different in the original data are similar (actually identical) in the target data.

The series of plots below show bivariate data for the original multivariate normal data, and the corresponding uniform, Poisson, and binary data. We can see the effect of extreme discontinuity of the binary data. (R code available [here](https://github.com/kgoldfeld/RDataGenBlog/tree/master/static/img/post-copula).)

```{r, echo=FALSE, warning = FALSE}
set.seed(58)

dt <- genCorData(250, mu = c(0, 0), sigma = 1, 
                 rho = 0.8, corstr = "cs", 
                 cnames = c("X1","X2"))

dt[, U1 := pnorm(X1)]
dt[, U2 := pnorm(X2)]

dt[, Y1 := qpois(U1, 3)]
dt[, Y2 := qpois(U2, 3)]

dt[, B1 := qbinom(U1, 1, .5)]
dt[, B2 := qbinom(U2, 1, .5)]

dtCor <- dt[, .(X = cor(X1, X2), U = cor(U1, U2),
                Y = cor(Y1, Y2), B = cor(B1, B2))]

dtS <- dtCor[,.(X = paste("Estimated \u03C1:", sprintf("%1.2f",X)),
         U = paste("Estimated \u03C1:", sprintf("%1.2f",U)),
         Y = paste("Estimated \u03C1:", sprintf("%1.2f",Y)),
         B = paste("Estimated \u03C1:", sprintf("%1.2f",B)))]

p1 <- ggplot(data = dt, aes(x=X1, y=X2)) +
  geom_point(color="#6285BA", size = 1) +
  theme_ksg("grey95") +
  scale_x_continuous(limits = c(-2.5,2.5)) +
  scale_y_continuous(limits = c(-2.5,2.5)) +
  ggtitle("Normal") +
  annotate(geom = "text", label =  dtS$X, x = -1.5, y = 2, fontface = 2, size = 3)

p2 <- ggplot(data = dt, aes(x=U1, y=U2)) +
  geom_point(color="#6285BA", size = 1) +
  theme_ksg("grey95") +
  scale_x_continuous(limits = c(0,1), breaks = c(0,1)) +
  scale_y_continuous(limits = c(0,1), breaks = c(0,1)) +
  theme(legend.position = "none") +
  ggtitle("Uniform")  +
  annotate(geom = "text", label =  dtS$U, x = .20, y = 0.92, 
           fontface = 2, size = 3)


p3 <- ggplot(data = dt, aes(x=Y1, y=Y2)) +
  geom_jitter(color="#6285BA", height = .15, width = .15, size = 1) +
  theme_ksg("grey95") +
  scale_x_continuous(limits = c(-.2, 7.2), breaks = c(0:7)) +
  scale_y_continuous(limits = c(-.2, 7.2), breaks = c(0:7)) +
  theme(legend.position = "none") +
  ggtitle("Poisson") +
  annotate(geom = "text", label =  dtS$Y, x = 1.5, y = 6.6, fontface = 2, size = 3)

p4 <- ggplot(data = dt, aes(x=B1, y=B2), size = 1) +
  geom_jitter(color="#6285BA", height = .1, width = .1) +
  theme_ksg("grey95") +
  scale_x_continuous(limits = c(-.2, 1.2), breaks = c(0:1)) +
  scale_y_continuous(limits = c(-.2, 1.2), breaks = c(0:1)) +
  theme(legend.position = "none") +
  ggtitle("Binary")+
  annotate(geom = "text", label =  dtS$B, x = .5, y = 0.5, fontface = 2, size = 3)

gridExtra::grid.arrange(p1, p2, p3, p4, nrow = 2)

```

## Some simulation results

A series of simulations shows how well the estimates of $\rho$ compare across a set of different assumptions. In each of the plots below, we see how $\rho$ for the non-normal data changes as a function of $\rho$ from the original normally distributed data. For each value of $\rho$, I varied the parameter of the non-normal distribution (in the case of the binary data, I varied the probability of the outcome; in the case of the Poisson data, I varied the parameter $\lambda$ which defines the mean and variance). I also considered both covariance structures, exchangeable and ar-1. (R code available [here](https://github.com/kgoldfeld/RDataGenBlog/tree/master/static/img/post-copula).)

![](/img/post-copula/dists.png)
  


These simulations confirm what we saw earlier. The Poisson data generating process recovers the original $\rho$ under both covariance structures reasonably well. The binary data generating process is less successful, with the exchangeable structure doing slightly better than then auto-regressive structure.

Hopefully soon, this will be implemented in `simstudy` so that we can generate data from more general distributions with a single function call.